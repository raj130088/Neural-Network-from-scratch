{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Coding a single neuron</b></span></h1>\n"
      ],
      "metadata": {
        "id": "iGLTLHiV6ji2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A neuron with four inputs\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0\n",
        "\n",
        "# Calculate the neuron's output\n",
        "output = (inputs[0] * weights[0] +\n",
        "          inputs[1] * weights[1] +\n",
        "          inputs[2] * weights[2] +\n",
        "          inputs[3] * weights[3] + bias)\n",
        "\n",
        "print(output)\n",
        "# Expected output: 4.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW2ypT1V6kIY",
        "outputId": "b8e4be04-de8b-4ead-8b7d-21378f4810fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>A layer of neuron manually</b></span></h1>\n"
      ],
      "metadata": {
        "id": "Nx_9-BFo7lVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A layer of 3 neurons, each with 4 inputs\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "# Weights for each of the 3 neurons\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "# Biases for each of the 3 neurons\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "# Manually calculating the output for each neuron\n",
        "output = [\n",
        "    # Neuron 1\n",
        "    (inputs[0] * weights[0][0] +\n",
        "     inputs[1] * weights[0][1] +\n",
        "     inputs[2] * weights[0][2] +\n",
        "     inputs[3] * weights[0][3] + biases[0]),\n",
        "\n",
        "    # Neuron 2\n",
        "    (inputs[0] * weights[1][0] +\n",
        "     inputs[1] * weights[1][1] +\n",
        "     inputs[2] * weights[1][2] +\n",
        "     inputs[3] * weights[1][3] + biases[1]),\n",
        "\n",
        "    # Neuron 3\n",
        "    (inputs[0] * weights[2][0] +\n",
        "     inputs[1] * weights[2][1] +\n",
        "     inputs[2] * weights[2][2] +\n",
        "     inputs[3] * weights[2][3] + biases[2])\n",
        "]\n",
        "\n",
        "print(output)\n",
        "# Expected output: [4.8, 1.21, 2.385]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTRodU2a7fXN",
        "outputId": "8fd8457f-4806-4fc1-a55f-b5e9f8b0bea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>A layer of neuron using Loop</b></span></h1>\n"
      ],
      "metadata": {
        "id": "dgHPLRFe-Nje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A layer of 3 neurons, each with 4 inputs (scalable version)\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "# Final list of outputs from the layer\n",
        "layer_outputs = []\n",
        "\n",
        "# Iterate through each neuron's weights and bias\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\n",
        "    # Start with the neuron's bias\n",
        "    neuron_output = 0\n",
        "    # Iterate through the inputs and the neuron's weights\n",
        "    for n_input, weight in zip(inputs, neuron_weights):\n",
        "        # Calculate the weighted sum of inputs\n",
        "        neuron_output += n_input * weight\n",
        "    # Add the bias to the weighted sum\n",
        "    neuron_output += neuron_bias\n",
        "    # Add the neuron's final output to the layer's output list\n",
        "    layer_outputs.append(neuron_output)\n",
        "\n",
        "print(layer_outputs)\n",
        "# Expected output: [4.8, 1.21, 2.385]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6L8g1qP-N6D",
        "outputId": "3793b5b7-d36e-43b1-e396-55598eccb996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>A layer of neuron using NumPy</b></span></h1>\n"
      ],
      "metadata": {
        "id": "3yciNNDKBrn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# A batch of 3 input samples\n",
        "# inputs: batch size * number of input features\n",
        "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
        "          [2.0, 5.0, -1.0, 2.0],\n",
        "          [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "# Weights for 3 neurons\n",
        "# weights: number of neurons * number of input features\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "# biases: 1 * bias for each neuron\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "# Convert inputs and weights to NumPy arrays\n",
        "inputs_array = np.array(inputs)\n",
        "weights_array = np.array(weights)\n",
        "\n",
        "# The inputs are multiplied by the TRANSPOSE of the weights\n",
        "# Note: np.array() is used to enable the .T (transpose) method\n",
        "layer_outputs = np.dot(inputs_array, weights_array.T) + biases\n",
        "# when we add biases to np.dot(inputs, np.array(weights).T)\n",
        "# broadcasting occurs and biases is added to each row\n",
        "\n",
        "print(layer_outputs)\n",
        "# Expected output:\n",
        "# [[ 4.8    1.21   2.385]\n",
        "#  [ 8.9   -1.81   0.285]\n",
        "#  [ 1.485  2.065  1.425]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U_zJOxI-SfD",
        "outputId": "4b148822-c62f-46fb-b015-9f4c09fd31d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.79999995  1.21000004  2.38499999]\n",
            " [ 8.9000001  -1.80999994  0.19999999]\n",
            " [ 1.41000003  1.051       0.02599999]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Coding a two layer network</b></span></h1>\n"
      ],
      "metadata": {
        "id": "OEVKbOf8DJJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# A batch of 3 input samples (same as before)\n",
        "inputs = np.array([[1.0, 2.0, 3.0, 2.5],\n",
        "                   [2.0, 5.0, -1.0, 2.0],\n",
        "                   [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# --- Layer 1 ---\n",
        "# 3 neurons, 4 weights each\n",
        "weights1 = np.array([[0.2, 0.8, -0.5, 1.0],\n",
        "                     [0.5, -0.91, 0.26, -0.5],\n",
        "                     [-0.26, -0.27, 0.17, 0.87]])\n",
        "biases1 = np.array([2.0, 3.0, 0.5])\n",
        "\n",
        "# --- Layer 2 ---\n",
        "# 3 neurons, but now with 3 weights each (because Layer 1 has 3 outputs)\n",
        "weights2 = np.array([[0.1, -0.14, 0.5],\n",
        "                     [-0.5, 0.12, -0.33],\n",
        "                     [-0.44, 0.73, -0.13]])\n",
        "biases2 = np.array([-1.0, 2.0, -0.5])"
      ],
      "metadata": {
        "id": "39gg2BiDCh55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate the output of Layer 1\n",
        "layer1_outputs = np.dot(inputs, weights1.T) + biases1\n",
        "\n",
        "# Step 2: Use Layer 1's output as the input for Layer 2\n",
        "layer2_outputs = np.dot(layer1_outputs, weights2.T) + biases2\n",
        "\n",
        "# Print the final output from the second layer\n",
        "print(layer2_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAYwlDqLDFg0",
        "outputId": "2b4b6940-1c69-467b-cc24-3d93aba69ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.50310004 -1.04184985 -2.03874993]\n",
            " [ 0.24339998 -2.73320007 -5.76329994]\n",
            " [-0.99314     1.41254002 -0.35655001]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Using a class for dense layer</b></span></h1>\n"
      ],
      "metadata": {
        "id": "bbR3BVcgGK3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# This is the blueprint for any dense layer we want to create.\n",
        "class Layer_Dense:\n",
        "\n",
        "    # The __init__ method is called automatically when we create a new layer.\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Create a weights matrix of shape (inputs x neurons) with small random values.\n",
        "        # This shape avoids the need for transposing later.\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        # Create a biases vector of zeros.\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    # The forward method performs the calculation for the layer.\n",
        "    def forward(self, inputs):\n",
        "        # Calculate the dot product and add biases.\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases"
      ],
      "metadata": {
        "id": "5OOJ_FHPDTaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume X is our spiral data with shape (100, 2)\n",
        "# (100 samples, 2 features each)\n",
        "!pip install nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "import numpy as np\n",
        "import nnfs\n",
        "nnfs.init()  # Sets NumPy seed and default dtype for reproducibility\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Plot\n",
        "# plt.scatter(X[:, 0], X[:, 1])\n",
        "# plt.show()\n",
        "\n",
        "# 1. Create a layer instance from our class blueprint.\n",
        "# This layer will take 2 inputs (from our data) and have 3 neurons.\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "\n",
        "# 2. Perform the forward pass by calling the layer's forward method.\n",
        "dense1.forward(X)\n",
        "\n",
        "# The results are now stored inside the object itself.\n",
        "print(dense1.output[:5]) # Print the output for the first 5 samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3azoZdMdF7hy",
        "outputId": "d4d3c4fa-1aa8-4684-b4ab-4b77d490bad8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnfs) (2.0.2)\n",
            "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
            " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
            " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
            " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Activation function RELU</b></span></h1>\n"
      ],
      "metadata": {
        "id": "2WTm9LHRfiF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Activation_ReLU:\n",
        "    # A forward pass takes the output from a dense layer\n",
        "    def forward(self, inputs):\n",
        "        # Apply the ReLU function: max(0, input)\n",
        "        self.output = np.maximum(0, inputs)"
      ],
      "metadata": {
        "id": "h7aWmso9fpF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Activation function Softmax</b></span></h1>\n"
      ],
      "metadata": {
        "id": "VBBWzvZlfiMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        # Step 1: Subtract the max value from each row for numerical stability.\n",
        "        # This prevents the exponentiated values from becoming infinitely large.\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "\n",
        "        # Step 2: Normalize the values to get probabilities.\n",
        "        # Divide each value by the sum of its row.\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "        self.output = probabilities"
      ],
      "metadata": {
        "id": "ekr3x6saf5Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>First forward pass</b></span></h1>\n"
      ],
      "metadata": {
        "id": "XlXEs09hfiRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume X is our spiral data\n",
        "\n",
        "# Create the first dense layer (2 inputs, 3 neurons)\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "# Create the ReLU activation for the first layer\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create the second dense layer (3 inputs from layer1, 3 neurons for 3 classes)\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "# Create the Softmax activation for the final output\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# --- Perform the full forward pass ---\n",
        "\n",
        "# Pass data through the first dense layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Pass the output of the dense layer through the ReLU activation\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Pass the output of the ReLU activation through the second dense layer\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Pass the output of the second dense layer through the Softmax activation\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "# The final probabilities are now in activation2.output\n",
        "print(activation2.output[:5]) # Print probabilities for the first 5 samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyiGHUDkgn1h",
        "outputId": "9e00dd8c-9755-4d34-f6a0-f36583f84540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333343 0.33333343 0.33333316]\n",
            " [0.33333352 0.3333335  0.33333296]\n",
            " [0.33333352 0.3333335  0.33333296]\n",
            " [0.3333336  0.33333358 0.33333278]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Loss function</b></span></h1>\n"
      ],
      "metadata": {
        "id": "4h7U3Og_fiYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# A parent class for any loss function\n",
        "class Loss:\n",
        "    def calculate(self, output, y):\n",
        "        sample_losses = self.forward(output, y)\n",
        "        data_loss = np.mean(sample_losses)\n",
        "        return data_loss\n",
        "\n",
        "# The specific class for Cross-Entropy Loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Clip data to prevent division by 0\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Handle integer labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(len(y_pred_clipped)), y_true]\n",
        "        # Handle one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "        # Calculate losses for each sample\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods"
      ],
      "metadata": {
        "id": "Nc7RDCBIokdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient: The gradient of the loss function is a vector that points in the direction of the steepest ascent (where the loss increases the fastest). By moving in the opposite direction of the gradient, we can descend the loss landscape as efficiently as possible. This is the core idea behind gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "2MzcQa51figl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Chain rule decomposition</b></span></h1>\n",
        "∂Loss / ∂w₀ = (∂Loss / ∂ReLU) * (∂ReLU / ∂Sum) * (∂Sum / ∂(x₀w₀)) * (∂(x₀w₀) / ∂w₀)"
      ],
      "metadata": {
        "id": "6kzUAwNkC-aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X (Inputs): (batch_size, num_input_features)\n",
        "\n",
        "W (Weights): (num_input_features, num_neurons)\n",
        "\n",
        "Z (Output): The result of the dot product is (batch_size, num_neurons).\n",
        "\n",
        "Since the output Z has the shape (batch_size, num_neurons),\n",
        "its gradient ∂L/∂Z must also have the shape (batch_size, num_neurons)."
      ],
      "metadata": {
        "id": "vjnFF5gnSItG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight Gradients: ∂L/∂W = Xᵀ ⋅ (∂L/∂Z)**\n",
        "\n",
        "**Bias Gradients: ∂L/∂b = sum(∂L/∂Z, axis=0)**\n",
        "\n",
        "**Input Gradients (to pass backwards): ∂L/∂X = (∂L/∂Z) ⋅ Wᵀ**"
      ],
      "metadata": {
        "id": "-kagpeb-8bg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h1><b>Coding the backpropagation building blocks</b></span></h1>"
      ],
      "metadata": {
        "id": "i10fdseS9Q1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer_Dense:\n",
        "    # The __init__ and forward methods are the same as before\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Store inputs for use in the backward pass\n",
        "        self.inputs = inputs\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # --- NEW BACKWARD METHOD ---\n",
        "    def backward(self, dvalues):\n",
        "        # dvalues is the upstream gradient, ∂L/∂Z\n",
        "\n",
        "        # 1. Gradient on weights (∂L/∂W = Xᵀ ⋅ ∂L/∂Z)\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "\n",
        "        # 2. Gradient on biases (∂L/∂b = sum(∂L/∂Z))\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # 3. Gradient on inputs (∂L/∂X = (∂L/∂Z) ⋅ Wᵀ)\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "Mi33IcbW9RoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h1><b>Backpropagation through activation function</b></span></h1>"
      ],
      "metadata": {
        "id": "DAE-NAWBAT4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        # Store the input values for use in the backward pass\n",
        "        self.inputs = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # --- NEW BACKWARD METHOD ---\n",
        "    def backward(self, dvalues):\n",
        "        # dvalues is the upstream gradient, ∂L/∂A\n",
        "\n",
        "        # 1. Start by making a copy of the upstream gradient.\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # 2. Zero out the gradient where the original inputs were negative.\n",
        "        # This is the implementation of the chain rule: ∂L/∂A * (1 or 0).\n",
        "        self.dinputs[self.inputs <= 0] = 0"
      ],
      "metadata": {
        "id": "hiBvXVSn_odT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h1><b>Categorical cross-entropy loss</b></span></h1>\n",
        "Loss = -log(predicted_probability_of_the_correct_class)"
      ],
      "metadata": {
        "id": "3K1JgyXv4_z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values –\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "        # Mask values – only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "        # --- NEW BACKWARD METHOD ---\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # dvalues are the predicted probabilities from Softmax\n",
        "        # y_true are the true labels\n",
        "\n",
        "        # Number of samples and labels\n",
        "        samples = len(dvalues)\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse (integers), turn them into a one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # --- Calculate the gradient using the formula ---\n",
        "        # gradient = -y_true / y_pred\n",
        "        self.dinputs = -y_true / dvalues\n",
        "\n",
        "        # --- Normalize the gradient ---\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "Huqdy3t85SsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main purpose of normalizing the gradient by the number of samples is to make the learning rate independent of the batch size. This stabilizes the training process and makes hyperparameter tuning more consistent.\n",
        "\n",
        "The Problem Without Normalization\n",
        "During backpropagation, the total gradient for a batch is the sum of the individual gradients from each sample. This means that if you use a larger batch, the magnitude of the total gradient will naturally be larger."
      ],
      "metadata": {
        "id": "9Z5wL_kDB3is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How np.eye()[] converts sparse labels into one-hot vector\n",
        "# np.eye(labels) creates an identity matrix of size [labels, labels]\n",
        "\n",
        "# np.eye(3)\n",
        "# array([[1., 0., 0.],\n",
        "#        [0., 1., 0.],\n",
        "#        [0., 0., 1.]])\n",
        "\n",
        "# [] chooses a different row of the identity matrix, depending on the integer label.\n",
        "#[[0. 0. 1.]   # from row 2\n",
        "#[1. 0. 0.]   # from row 0\n",
        "#[0. 1. 0.]]  # from row 1"
      ],
      "metadata": {
        "id": "ix8FFTm9-64M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the derivative for the Softmax activation function is mathematically complicated and involves Jacobian matrices. So we can't find the gradient.\n",
        "\n",
        "Since the Softmax activation is always immediately followed by the Categorical Cross-Entropy loss function, we can mathematically combine them. The derivative of this combined Softmax and Loss function turns out to be an incredibly simple and elegant formula: Predicted Probabilities - True Labels"
      ],
      "metadata": {
        "id": "WKiLTwjsEaR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax classifier – combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # dvalues are the predicted probabilities from Softmax\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "4wSJmI3sEYzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Assembling the whole Neural Network</b></span></h1>"
      ],
      "metadata": {
        "id": "j8xwwoUfPC6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume X is the spiral data and y are the true labels\n",
        "\n",
        "# --- Create the network components ---\n",
        "\n",
        "# Layer 1: 2 input features, 3 neurons\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "# ReLU activation for Layer 1\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Layer 2: 3 input features (from layer 1), 3 output neurons (for 3 classes)\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "\n",
        "# Combined Softmax and Cross-Entropy Loss for the output layer\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "1uW4cRu5PD0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Perform the full forward pass ---\n",
        "\n",
        "# Layer 1\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Layer 2\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Calculate the loss from the output of the final dense layer\n",
        "loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "# Print initial loss and accuracy\n",
        "print(f'Loss: {loss}')\n",
        "# ... (code to calculate and print accuracy)\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis=1)\n",
        "\n",
        "if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis=1)\n",
        "\n",
        "accuracy = np.mean(predictions == y)\n",
        "\n",
        "# Print accuracy\n",
        "print('acc:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQkhYMTTPUZI",
        "outputId": "fa93d728-e450-439c-8e07-75c77ef02330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0986053943634033\n",
            "acc: 0.31666666666666665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Perform the full backward pass ---\n",
        "\n",
        "# Start the backward pass from the combined loss and activation function\n",
        "loss_activation.backward(loss_activation.output, y)\n",
        "\n",
        "# Pass the gradient back through Dense Layer 2\n",
        "dense2.backward(loss_activation.dinputs)\n",
        "\n",
        "# Pass the gradient back through the ReLU Activation\n",
        "activation1.backward(dense2.dinputs)\n",
        "\n",
        "# Pass the gradient back through Dense Layer 1\n",
        "dense1.backward(activation1.dinputs)\n",
        "\n",
        "# --- Gradients are now ready ---\n",
        "# The optimizer can now access these gradients to update the weights\n",
        "print(\"Gradients for Layer 1 weights:\\n\", dense1.dweights)\n",
        "print(\"Gradients for Layer 1 biases:\\n\", dense1.dbiases)\n",
        "print(\"Gradients for Layer 2 weights:\\n\", dense2.dweights)\n",
        "print(\"Gradients for Layer 2 biases:\\n\", dense2.dbiases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCFVAsZcPYsS",
        "outputId": "dfc6cde2-f274-4b2a-e4ea-4d867258ff23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for Layer 1 weights:\n",
            " [[-1.8124637e-04  1.2043593e-04  8.6177075e-05]\n",
            " [ 5.3396635e-04  3.1610730e-04  4.8854109e-04]]\n",
            "Gradients for Layer 1 biases:\n",
            " [[-0.00018286 -0.00084047  0.00025687]]\n",
            "Gradients for Layer 2 weights:\n",
            " [[-8.4585387e-05  2.1817083e-04 -1.3358545e-04]\n",
            " [-4.0713396e-05  3.4001368e-05  6.7120254e-06]\n",
            " [ 7.2265793e-05  7.4821928e-06 -7.9747988e-05]]\n",
            "Gradients for Layer 2 biases:\n",
            " [[ 1.2627570e-05 -2.4172943e-05  1.1234079e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Optimizer SGD</b></span></h1>"
      ],
      "metadata": {
        "id": "dzIug-jwDiSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "    # Initialize the optimizer with a learning rate\n",
        "    def __init__(self, learning_rate=1.0):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Update a layer's parameters\n",
        "    def update_params(self, layer):\n",
        "        # Update weights using the gradient descent rule\n",
        "        layer.weights += -self.learning_rate * layer.dweights\n",
        "        # Update biases using the gradient descent rule\n",
        "        layer.biases += -self.learning_rate * layer.dbiases"
      ],
      "metadata": {
        "id": "Pghp0ll_Pknb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (output of previous layer)\n",
        "# and 3 output values (for 3 classes)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(learning_rate=0.9) # Using a high learning rate, the loss won't decrease\n",
        "\n",
        "# --- Training Loop ---\n",
        "for epoch in range(30001):\n",
        "    # 1. Forward Pass\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Predictions (class indices from Softmax outputs)\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "\n",
        "    # If labels are one-hot encoded, convert them to integers\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "\n",
        "    # 2. Backward Pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # --- 3. Update Parameters ---\n",
        "    # Use the optimizer to update the weights and biases of each layer\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "\n",
        "    # Print progress every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'Epoch: {epoch}, Loss: {loss:.3f}, Accuracy: {accuracy:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TUJlcaLtRKJC",
        "outputId": "1f85baa8-caa5-4dec-baf1-883b6e4f68de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.099, Accuracy: 0.357\n",
            "Epoch: 100, Loss: 1.082, Accuracy: 0.400\n",
            "Epoch: 200, Loss: 1.063, Accuracy: 0.420\n",
            "Epoch: 300, Loss: 1.061, Accuracy: 0.420\n",
            "Epoch: 400, Loss: 1.059, Accuracy: 0.430\n",
            "Epoch: 500, Loss: 1.055, Accuracy: 0.430\n",
            "Epoch: 600, Loss: 1.051, Accuracy: 0.433\n",
            "Epoch: 700, Loss: 1.046, Accuracy: 0.457\n",
            "Epoch: 800, Loss: 1.040, Accuracy: 0.467\n",
            "Epoch: 900, Loss: 1.047, Accuracy: 0.423\n",
            "Epoch: 1000, Loss: 1.036, Accuracy: 0.420\n",
            "Epoch: 1100, Loss: 1.028, Accuracy: 0.383\n",
            "Epoch: 1200, Loss: 1.015, Accuracy: 0.427\n",
            "Epoch: 1300, Loss: 1.003, Accuracy: 0.427\n",
            "Epoch: 1400, Loss: 0.990, Accuracy: 0.440\n",
            "Epoch: 1500, Loss: 0.974, Accuracy: 0.450\n",
            "Epoch: 1600, Loss: 0.960, Accuracy: 0.450\n",
            "Epoch: 1700, Loss: 0.944, Accuracy: 0.453\n",
            "Epoch: 1800, Loss: 0.929, Accuracy: 0.460\n",
            "Epoch: 1900, Loss: 0.929, Accuracy: 0.540\n",
            "Epoch: 2000, Loss: 0.904, Accuracy: 0.513\n",
            "Epoch: 2100, Loss: 0.911, Accuracy: 0.580\n",
            "Epoch: 2200, Loss: 0.866, Accuracy: 0.557\n",
            "Epoch: 2300, Loss: 0.858, Accuracy: 0.517\n",
            "Epoch: 2400, Loss: 0.838, Accuracy: 0.517\n",
            "Epoch: 2500, Loss: 0.877, Accuracy: 0.590\n",
            "Epoch: 2600, Loss: 0.799, Accuracy: 0.640\n",
            "Epoch: 2700, Loss: 0.799, Accuracy: 0.570\n",
            "Epoch: 2800, Loss: 0.795, Accuracy: 0.647\n",
            "Epoch: 2900, Loss: 0.801, Accuracy: 0.620\n",
            "Epoch: 3000, Loss: 0.757, Accuracy: 0.623\n",
            "Epoch: 3100, Loss: 0.769, Accuracy: 0.660\n",
            "Epoch: 3200, Loss: 0.748, Accuracy: 0.653\n",
            "Epoch: 3300, Loss: 0.752, Accuracy: 0.657\n",
            "Epoch: 3400, Loss: 0.742, Accuracy: 0.653\n",
            "Epoch: 3500, Loss: 0.760, Accuracy: 0.660\n",
            "Epoch: 3600, Loss: 0.737, Accuracy: 0.607\n",
            "Epoch: 3700, Loss: 0.812, Accuracy: 0.607\n",
            "Epoch: 3800, Loss: 0.688, Accuracy: 0.633\n",
            "Epoch: 3900, Loss: 0.708, Accuracy: 0.623\n",
            "Epoch: 4000, Loss: 0.764, Accuracy: 0.633\n",
            "Epoch: 4100, Loss: 0.725, Accuracy: 0.673\n",
            "Epoch: 4200, Loss: 0.767, Accuracy: 0.643\n",
            "Epoch: 4300, Loss: 0.699, Accuracy: 0.667\n",
            "Epoch: 4400, Loss: 0.719, Accuracy: 0.667\n",
            "Epoch: 4500, Loss: 0.673, Accuracy: 0.673\n",
            "Epoch: 4600, Loss: 0.676, Accuracy: 0.680\n",
            "Epoch: 4700, Loss: 0.654, Accuracy: 0.680\n",
            "Epoch: 4800, Loss: 0.621, Accuracy: 0.673\n",
            "Epoch: 4900, Loss: 0.630, Accuracy: 0.650\n",
            "Epoch: 5000, Loss: 0.611, Accuracy: 0.720\n",
            "Epoch: 5100, Loss: 0.593, Accuracy: 0.717\n",
            "Epoch: 5200, Loss: 0.614, Accuracy: 0.733\n",
            "Epoch: 5300, Loss: 0.723, Accuracy: 0.693\n",
            "Epoch: 5400, Loss: 0.535, Accuracy: 0.763\n",
            "Epoch: 5500, Loss: 0.565, Accuracy: 0.740\n",
            "Epoch: 5600, Loss: 0.581, Accuracy: 0.747\n",
            "Epoch: 5700, Loss: 0.542, Accuracy: 0.757\n",
            "Epoch: 5800, Loss: 0.609, Accuracy: 0.703\n",
            "Epoch: 5900, Loss: 0.525, Accuracy: 0.760\n",
            "Epoch: 6000, Loss: 0.704, Accuracy: 0.650\n",
            "Epoch: 6100, Loss: 0.483, Accuracy: 0.793\n",
            "Epoch: 6200, Loss: 0.489, Accuracy: 0.807\n",
            "Epoch: 6300, Loss: 0.788, Accuracy: 0.647\n",
            "Epoch: 6400, Loss: 0.517, Accuracy: 0.783\n",
            "Epoch: 6500, Loss: 0.508, Accuracy: 0.777\n",
            "Epoch: 6600, Loss: 0.504, Accuracy: 0.773\n",
            "Epoch: 6700, Loss: 0.700, Accuracy: 0.657\n",
            "Epoch: 6800, Loss: 0.503, Accuracy: 0.780\n",
            "Epoch: 6900, Loss: 0.490, Accuracy: 0.780\n",
            "Epoch: 7000, Loss: 0.483, Accuracy: 0.787\n",
            "Epoch: 7100, Loss: 0.498, Accuracy: 0.780\n",
            "Epoch: 7200, Loss: 0.489, Accuracy: 0.783\n",
            "Epoch: 7300, Loss: 0.481, Accuracy: 0.790\n",
            "Epoch: 7400, Loss: 0.488, Accuracy: 0.790\n",
            "Epoch: 7500, Loss: 0.483, Accuracy: 0.780\n",
            "Epoch: 7600, Loss: 0.515, Accuracy: 0.783\n",
            "Epoch: 7700, Loss: 0.482, Accuracy: 0.790\n",
            "Epoch: 7800, Loss: 0.481, Accuracy: 0.783\n",
            "Epoch: 7900, Loss: 0.484, Accuracy: 0.790\n",
            "Epoch: 8000, Loss: 0.476, Accuracy: 0.790\n",
            "Epoch: 8100, Loss: 0.466, Accuracy: 0.797\n",
            "Epoch: 8200, Loss: 0.474, Accuracy: 0.793\n",
            "Epoch: 8300, Loss: 0.526, Accuracy: 0.787\n",
            "Epoch: 8400, Loss: 0.466, Accuracy: 0.793\n",
            "Epoch: 8500, Loss: 0.467, Accuracy: 0.790\n",
            "Epoch: 8600, Loss: 0.459, Accuracy: 0.797\n",
            "Epoch: 8700, Loss: 0.468, Accuracy: 0.793\n",
            "Epoch: 8800, Loss: 0.460, Accuracy: 0.797\n",
            "Epoch: 8900, Loss: 0.439, Accuracy: 0.807\n",
            "Epoch: 9000, Loss: 0.455, Accuracy: 0.803\n",
            "Epoch: 9100, Loss: 0.460, Accuracy: 0.800\n",
            "Epoch: 9200, Loss: 0.440, Accuracy: 0.803\n",
            "Epoch: 9300, Loss: 0.460, Accuracy: 0.800\n",
            "Epoch: 9400, Loss: 0.455, Accuracy: 0.797\n",
            "Epoch: 9500, Loss: 0.458, Accuracy: 0.800\n",
            "Epoch: 9600, Loss: 0.456, Accuracy: 0.797\n",
            "Epoch: 9700, Loss: 0.457, Accuracy: 0.803\n",
            "Epoch: 9800, Loss: 0.671, Accuracy: 0.740\n",
            "Epoch: 9900, Loss: 0.455, Accuracy: 0.803\n",
            "Epoch: 10000, Loss: 0.449, Accuracy: 0.800\n",
            "Epoch: 10100, Loss: 0.446, Accuracy: 0.807\n",
            "Epoch: 10200, Loss: 0.416, Accuracy: 0.817\n",
            "Epoch: 10300, Loss: 0.442, Accuracy: 0.807\n",
            "Epoch: 10400, Loss: 0.447, Accuracy: 0.797\n",
            "Epoch: 10500, Loss: 0.445, Accuracy: 0.800\n",
            "Epoch: 10600, Loss: 0.872, Accuracy: 0.723\n",
            "Epoch: 10700, Loss: 0.438, Accuracy: 0.810\n",
            "Epoch: 10800, Loss: 0.404, Accuracy: 0.827\n",
            "Epoch: 10900, Loss: 0.430, Accuracy: 0.813\n",
            "Epoch: 11000, Loss: 0.444, Accuracy: 0.810\n",
            "Epoch: 11100, Loss: 0.425, Accuracy: 0.820\n",
            "Epoch: 11200, Loss: 0.434, Accuracy: 0.813\n",
            "Epoch: 11300, Loss: 0.431, Accuracy: 0.820\n",
            "Epoch: 11400, Loss: 0.432, Accuracy: 0.810\n",
            "Epoch: 11500, Loss: 0.423, Accuracy: 0.823\n",
            "Epoch: 11600, Loss: 0.427, Accuracy: 0.817\n",
            "Epoch: 11700, Loss: 0.428, Accuracy: 0.820\n",
            "Epoch: 11800, Loss: 0.420, Accuracy: 0.827\n",
            "Epoch: 11900, Loss: 0.423, Accuracy: 0.823\n",
            "Epoch: 12000, Loss: 0.420, Accuracy: 0.823\n",
            "Epoch: 12100, Loss: 0.426, Accuracy: 0.817\n",
            "Epoch: 12200, Loss: 0.418, Accuracy: 0.820\n",
            "Epoch: 12300, Loss: 0.421, Accuracy: 0.827\n",
            "Epoch: 12400, Loss: 0.423, Accuracy: 0.827\n",
            "Epoch: 12500, Loss: 0.429, Accuracy: 0.810\n",
            "Epoch: 12600, Loss: 0.410, Accuracy: 0.827\n",
            "Epoch: 12700, Loss: 0.411, Accuracy: 0.827\n",
            "Epoch: 12800, Loss: 0.411, Accuracy: 0.830\n",
            "Epoch: 12900, Loss: 0.402, Accuracy: 0.833\n",
            "Epoch: 13000, Loss: 0.407, Accuracy: 0.830\n",
            "Epoch: 13100, Loss: 0.414, Accuracy: 0.827\n",
            "Epoch: 13200, Loss: 0.409, Accuracy: 0.830\n",
            "Epoch: 13300, Loss: 0.427, Accuracy: 0.820\n",
            "Epoch: 13400, Loss: 0.403, Accuracy: 0.837\n",
            "Epoch: 13500, Loss: 0.411, Accuracy: 0.833\n",
            "Epoch: 13600, Loss: 0.403, Accuracy: 0.837\n",
            "Epoch: 13700, Loss: 0.402, Accuracy: 0.833\n",
            "Epoch: 13800, Loss: 0.394, Accuracy: 0.840\n",
            "Epoch: 13900, Loss: 0.395, Accuracy: 0.840\n",
            "Epoch: 14000, Loss: 0.390, Accuracy: 0.840\n",
            "Epoch: 14100, Loss: 0.396, Accuracy: 0.837\n",
            "Epoch: 14200, Loss: 0.394, Accuracy: 0.840\n",
            "Epoch: 14300, Loss: 0.390, Accuracy: 0.843\n",
            "Epoch: 14400, Loss: 0.401, Accuracy: 0.843\n",
            "Epoch: 14500, Loss: 0.395, Accuracy: 0.843\n",
            "Epoch: 14600, Loss: 0.392, Accuracy: 0.830\n",
            "Epoch: 14700, Loss: 0.375, Accuracy: 0.843\n",
            "Epoch: 14800, Loss: 0.374, Accuracy: 0.850\n",
            "Epoch: 14900, Loss: 0.380, Accuracy: 0.850\n",
            "Epoch: 15000, Loss: 0.388, Accuracy: 0.857\n",
            "Epoch: 15100, Loss: 0.374, Accuracy: 0.850\n",
            "Epoch: 15200, Loss: 0.368, Accuracy: 0.857\n",
            "Epoch: 15300, Loss: 0.380, Accuracy: 0.853\n",
            "Epoch: 15400, Loss: 0.363, Accuracy: 0.860\n",
            "Epoch: 15500, Loss: 0.409, Accuracy: 0.840\n",
            "Epoch: 15600, Loss: 0.367, Accuracy: 0.860\n",
            "Epoch: 15700, Loss: 0.372, Accuracy: 0.860\n",
            "Epoch: 15800, Loss: 0.377, Accuracy: 0.857\n",
            "Epoch: 15900, Loss: 0.375, Accuracy: 0.857\n",
            "Epoch: 16000, Loss: 0.371, Accuracy: 0.857\n",
            "Epoch: 16100, Loss: 0.365, Accuracy: 0.860\n",
            "Epoch: 16200, Loss: 0.421, Accuracy: 0.830\n",
            "Epoch: 16300, Loss: 0.358, Accuracy: 0.860\n",
            "Epoch: 16400, Loss: 0.354, Accuracy: 0.867\n",
            "Epoch: 16500, Loss: 0.362, Accuracy: 0.860\n",
            "Epoch: 16600, Loss: 0.352, Accuracy: 0.863\n",
            "Epoch: 16700, Loss: 0.342, Accuracy: 0.870\n",
            "Epoch: 16800, Loss: 0.341, Accuracy: 0.867\n",
            "Epoch: 16900, Loss: 0.362, Accuracy: 0.860\n",
            "Epoch: 17000, Loss: 0.359, Accuracy: 0.867\n",
            "Epoch: 17100, Loss: 0.315, Accuracy: 0.877\n",
            "Epoch: 17200, Loss: 0.373, Accuracy: 0.863\n",
            "Epoch: 17300, Loss: 0.333, Accuracy: 0.877\n",
            "Epoch: 17400, Loss: 0.307, Accuracy: 0.880\n",
            "Epoch: 17500, Loss: 0.337, Accuracy: 0.877\n",
            "Epoch: 17600, Loss: 0.361, Accuracy: 0.870\n",
            "Epoch: 17700, Loss: 0.361, Accuracy: 0.870\n",
            "Epoch: 17800, Loss: 0.359, Accuracy: 0.870\n",
            "Epoch: 17900, Loss: 0.358, Accuracy: 0.870\n",
            "Epoch: 18000, Loss: 0.363, Accuracy: 0.867\n",
            "Epoch: 18100, Loss: 0.336, Accuracy: 0.880\n",
            "Epoch: 18200, Loss: 0.375, Accuracy: 0.863\n",
            "Epoch: 18300, Loss: 0.362, Accuracy: 0.863\n",
            "Epoch: 18400, Loss: 0.355, Accuracy: 0.873\n",
            "Epoch: 18500, Loss: 0.367, Accuracy: 0.867\n",
            "Epoch: 18600, Loss: 0.349, Accuracy: 0.867\n",
            "Epoch: 18700, Loss: 0.367, Accuracy: 0.863\n",
            "Epoch: 18800, Loss: 0.373, Accuracy: 0.867\n",
            "Epoch: 18900, Loss: 0.362, Accuracy: 0.870\n",
            "Epoch: 19000, Loss: 0.374, Accuracy: 0.867\n",
            "Epoch: 19100, Loss: 0.336, Accuracy: 0.867\n",
            "Epoch: 19200, Loss: 0.338, Accuracy: 0.867\n",
            "Epoch: 19300, Loss: 0.374, Accuracy: 0.863\n",
            "Epoch: 19400, Loss: 0.391, Accuracy: 0.853\n",
            "Epoch: 19500, Loss: 0.318, Accuracy: 0.877\n",
            "Epoch: 19600, Loss: 0.359, Accuracy: 0.863\n",
            "Epoch: 19700, Loss: 0.399, Accuracy: 0.847\n",
            "Epoch: 19800, Loss: 0.380, Accuracy: 0.857\n",
            "Epoch: 19900, Loss: 0.337, Accuracy: 0.880\n",
            "Epoch: 20000, Loss: 0.344, Accuracy: 0.873\n",
            "Epoch: 20100, Loss: 0.369, Accuracy: 0.863\n",
            "Epoch: 20200, Loss: 0.332, Accuracy: 0.877\n",
            "Epoch: 20300, Loss: 0.322, Accuracy: 0.877\n",
            "Epoch: 20400, Loss: 0.310, Accuracy: 0.877\n",
            "Epoch: 20500, Loss: 1.240, Accuracy: 0.657\n",
            "Epoch: 20600, Loss: 0.306, Accuracy: 0.887\n",
            "Epoch: 20700, Loss: 0.281, Accuracy: 0.887\n",
            "Epoch: 20800, Loss: 0.323, Accuracy: 0.887\n",
            "Epoch: 20900, Loss: 0.339, Accuracy: 0.877\n",
            "Epoch: 21000, Loss: 0.280, Accuracy: 0.890\n",
            "Epoch: 21100, Loss: 0.282, Accuracy: 0.890\n",
            "Epoch: 21200, Loss: 0.306, Accuracy: 0.893\n",
            "Epoch: 21300, Loss: 0.347, Accuracy: 0.877\n",
            "Epoch: 21400, Loss: 0.339, Accuracy: 0.880\n",
            "Epoch: 21500, Loss: 0.318, Accuracy: 0.873\n",
            "Epoch: 21600, Loss: 0.301, Accuracy: 0.897\n",
            "Epoch: 21700, Loss: 0.285, Accuracy: 0.890\n",
            "Epoch: 21800, Loss: 0.309, Accuracy: 0.897\n",
            "Epoch: 21900, Loss: 0.356, Accuracy: 0.867\n",
            "Epoch: 22000, Loss: 0.320, Accuracy: 0.883\n",
            "Epoch: 22100, Loss: 0.331, Accuracy: 0.880\n",
            "Epoch: 22200, Loss: 0.327, Accuracy: 0.877\n",
            "Epoch: 22300, Loss: 0.358, Accuracy: 0.867\n",
            "Epoch: 22400, Loss: 0.294, Accuracy: 0.890\n",
            "Epoch: 22500, Loss: 0.368, Accuracy: 0.863\n",
            "Epoch: 22600, Loss: 0.335, Accuracy: 0.880\n",
            "Epoch: 22700, Loss: 0.346, Accuracy: 0.870\n",
            "Epoch: 22800, Loss: 0.368, Accuracy: 0.870\n",
            "Epoch: 22900, Loss: 0.283, Accuracy: 0.890\n",
            "Epoch: 23000, Loss: 1.206, Accuracy: 0.690\n",
            "Epoch: 23100, Loss: 0.324, Accuracy: 0.887\n",
            "Epoch: 23200, Loss: 0.356, Accuracy: 0.867\n",
            "Epoch: 23300, Loss: 0.284, Accuracy: 0.897\n",
            "Epoch: 23400, Loss: 0.339, Accuracy: 0.880\n",
            "Epoch: 23500, Loss: 0.350, Accuracy: 0.870\n",
            "Epoch: 23600, Loss: 0.301, Accuracy: 0.883\n",
            "Epoch: 23700, Loss: 0.290, Accuracy: 0.903\n",
            "Epoch: 23800, Loss: 0.269, Accuracy: 0.900\n",
            "Epoch: 23900, Loss: 0.292, Accuracy: 0.903\n",
            "Epoch: 24000, Loss: 0.298, Accuracy: 0.893\n",
            "Epoch: 24100, Loss: 0.267, Accuracy: 0.903\n",
            "Epoch: 24200, Loss: 0.322, Accuracy: 0.883\n",
            "Epoch: 24300, Loss: 0.312, Accuracy: 0.883\n",
            "Epoch: 24400, Loss: 0.284, Accuracy: 0.893\n",
            "Epoch: 24500, Loss: 0.329, Accuracy: 0.893\n",
            "Epoch: 24600, Loss: 0.329, Accuracy: 0.887\n",
            "Epoch: 24700, Loss: 0.378, Accuracy: 0.850\n",
            "Epoch: 24800, Loss: 0.332, Accuracy: 0.880\n",
            "Epoch: 24900, Loss: 0.315, Accuracy: 0.877\n",
            "Epoch: 25000, Loss: 0.289, Accuracy: 0.890\n",
            "Epoch: 25100, Loss: 0.278, Accuracy: 0.890\n",
            "Epoch: 25200, Loss: 0.295, Accuracy: 0.893\n",
            "Epoch: 25300, Loss: 0.297, Accuracy: 0.897\n",
            "Epoch: 25400, Loss: 0.328, Accuracy: 0.887\n",
            "Epoch: 25500, Loss: 0.297, Accuracy: 0.900\n",
            "Epoch: 25600, Loss: 0.326, Accuracy: 0.887\n",
            "Epoch: 25700, Loss: 0.270, Accuracy: 0.903\n",
            "Epoch: 25800, Loss: 0.287, Accuracy: 0.897\n",
            "Epoch: 25900, Loss: 0.394, Accuracy: 0.870\n",
            "Epoch: 26000, Loss: 0.294, Accuracy: 0.900\n",
            "Epoch: 26100, Loss: 0.299, Accuracy: 0.900\n",
            "Epoch: 26200, Loss: 0.283, Accuracy: 0.900\n",
            "Epoch: 26300, Loss: 0.291, Accuracy: 0.903\n",
            "Epoch: 26400, Loss: 0.290, Accuracy: 0.900\n",
            "Epoch: 26500, Loss: 0.283, Accuracy: 0.900\n",
            "Epoch: 26600, Loss: 0.327, Accuracy: 0.890\n",
            "Epoch: 26700, Loss: 0.332, Accuracy: 0.890\n",
            "Epoch: 26800, Loss: 0.280, Accuracy: 0.907\n",
            "Epoch: 26900, Loss: 0.257, Accuracy: 0.907\n",
            "Epoch: 27000, Loss: 0.282, Accuracy: 0.907\n",
            "Epoch: 27100, Loss: 0.314, Accuracy: 0.893\n",
            "Epoch: 27200, Loss: 0.303, Accuracy: 0.897\n",
            "Epoch: 27300, Loss: 0.338, Accuracy: 0.890\n",
            "Epoch: 27400, Loss: 0.299, Accuracy: 0.897\n",
            "Epoch: 27500, Loss: 0.285, Accuracy: 0.907\n",
            "Epoch: 27600, Loss: 0.289, Accuracy: 0.903\n",
            "Epoch: 27700, Loss: 0.244, Accuracy: 0.900\n",
            "Epoch: 27800, Loss: 0.314, Accuracy: 0.887\n",
            "Epoch: 27900, Loss: 0.312, Accuracy: 0.890\n",
            "Epoch: 28000, Loss: 0.304, Accuracy: 0.890\n",
            "Epoch: 28100, Loss: 0.348, Accuracy: 0.873\n",
            "Epoch: 28200, Loss: 0.289, Accuracy: 0.890\n",
            "Epoch: 28300, Loss: 0.303, Accuracy: 0.890\n",
            "Epoch: 28400, Loss: 0.357, Accuracy: 0.873\n",
            "Epoch: 28500, Loss: 0.314, Accuracy: 0.883\n",
            "Epoch: 28600, Loss: 0.321, Accuracy: 0.880\n",
            "Epoch: 28700, Loss: 1.531, Accuracy: 0.690\n",
            "Epoch: 28800, Loss: 0.309, Accuracy: 0.883\n",
            "Epoch: 28900, Loss: 0.325, Accuracy: 0.877\n",
            "Epoch: 29000, Loss: 0.336, Accuracy: 0.883\n",
            "Epoch: 29100, Loss: 0.299, Accuracy: 0.883\n",
            "Epoch: 29200, Loss: 0.287, Accuracy: 0.887\n",
            "Epoch: 29300, Loss: 0.313, Accuracy: 0.877\n",
            "Epoch: 29400, Loss: 0.317, Accuracy: 0.887\n",
            "Epoch: 29500, Loss: 0.310, Accuracy: 0.883\n",
            "Epoch: 29600, Loss: 0.325, Accuracy: 0.873\n",
            "Epoch: 29700, Loss: 0.278, Accuracy: 0.887\n",
            "Epoch: 29800, Loss: 0.327, Accuracy: 0.873\n",
            "Epoch: 29900, Loss: 0.304, Accuracy: 0.887\n",
            "Epoch: 30000, Loss: 0.315, Accuracy: 0.873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (output of previous layer)\n",
        "# and 3 output values (for 3 classes)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD()\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(30001):\n",
        "    # Forward pass through first dense layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Forward pass through ReLU activation\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Forward pass through second dense layer\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Forward pass through activation/loss function\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy and loss every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6fsmx0qfRMZU",
        "outputId": "27a897b0-4000-49c6-87c8-506ae359856c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.333, loss: 1.099\n",
            "epoch: 100, acc: 0.470, loss: 1.073\n",
            "epoch: 200, acc: 0.463, loss: 1.057\n",
            "epoch: 300, acc: 0.460, loss: 1.056\n",
            "epoch: 400, acc: 0.463, loss: 1.055\n",
            "epoch: 500, acc: 0.470, loss: 1.053\n",
            "epoch: 600, acc: 0.437, loss: 1.050\n",
            "epoch: 700, acc: 0.457, loss: 1.049\n",
            "epoch: 800, acc: 0.467, loss: 1.046\n",
            "epoch: 900, acc: 0.463, loss: 1.042\n",
            "epoch: 1000, acc: 0.467, loss: 1.033\n",
            "epoch: 1100, acc: 0.487, loss: 1.017\n",
            "epoch: 1200, acc: 0.457, loss: 1.008\n",
            "epoch: 1300, acc: 0.460, loss: 0.992\n",
            "epoch: 1400, acc: 0.487, loss: 0.976\n",
            "epoch: 1500, acc: 0.437, loss: 1.000\n",
            "epoch: 1600, acc: 0.513, loss: 0.979\n",
            "epoch: 1700, acc: 0.487, loss: 0.946\n",
            "epoch: 1800, acc: 0.530, loss: 0.938\n",
            "epoch: 1900, acc: 0.527, loss: 0.926\n",
            "epoch: 2000, acc: 0.573, loss: 0.886\n",
            "epoch: 2100, acc: 0.610, loss: 0.866\n",
            "epoch: 2200, acc: 0.543, loss: 0.887\n",
            "epoch: 2300, acc: 0.567, loss: 0.867\n",
            "epoch: 2400, acc: 0.617, loss: 0.828\n",
            "epoch: 2500, acc: 0.590, loss: 0.806\n",
            "epoch: 2600, acc: 0.567, loss: 0.847\n",
            "epoch: 2700, acc: 0.633, loss: 0.793\n",
            "epoch: 2800, acc: 0.593, loss: 0.796\n",
            "epoch: 2900, acc: 0.603, loss: 0.742\n",
            "epoch: 3000, acc: 0.667, loss: 0.728\n",
            "epoch: 3100, acc: 0.550, loss: 0.873\n",
            "epoch: 3200, acc: 0.690, loss: 0.702\n",
            "epoch: 3300, acc: 0.663, loss: 0.674\n",
            "epoch: 3400, acc: 0.627, loss: 0.696\n",
            "epoch: 3500, acc: 0.677, loss: 0.686\n",
            "epoch: 3600, acc: 0.597, loss: 0.845\n",
            "epoch: 3700, acc: 0.673, loss: 0.630\n",
            "epoch: 3800, acc: 0.680, loss: 0.693\n",
            "epoch: 3900, acc: 0.667, loss: 0.629\n",
            "epoch: 4000, acc: 0.727, loss: 0.622\n",
            "epoch: 4100, acc: 0.677, loss: 0.628\n",
            "epoch: 4200, acc: 0.747, loss: 0.603\n",
            "epoch: 4300, acc: 0.653, loss: 0.630\n",
            "epoch: 4400, acc: 0.647, loss: 0.771\n",
            "epoch: 4500, acc: 0.673, loss: 0.591\n",
            "epoch: 4600, acc: 0.600, loss: 0.994\n",
            "epoch: 4700, acc: 0.740, loss: 0.532\n",
            "epoch: 4800, acc: 0.683, loss: 0.575\n",
            "epoch: 4900, acc: 0.720, loss: 0.541\n",
            "epoch: 5000, acc: 0.713, loss: 0.540\n",
            "epoch: 5100, acc: 0.717, loss: 0.537\n",
            "epoch: 5200, acc: 0.717, loss: 0.534\n",
            "epoch: 5300, acc: 0.720, loss: 0.528\n",
            "epoch: 5400, acc: 0.717, loss: 0.524\n",
            "epoch: 5500, acc: 0.727, loss: 0.520\n",
            "epoch: 5600, acc: 0.730, loss: 0.513\n",
            "epoch: 5700, acc: 0.717, loss: 0.532\n",
            "epoch: 5800, acc: 0.770, loss: 0.501\n",
            "epoch: 5900, acc: 0.710, loss: 0.591\n",
            "epoch: 6000, acc: 0.750, loss: 0.498\n",
            "epoch: 6100, acc: 0.747, loss: 0.498\n",
            "epoch: 6200, acc: 0.797, loss: 0.475\n",
            "epoch: 6300, acc: 0.760, loss: 0.490\n",
            "epoch: 6400, acc: 0.750, loss: 0.485\n",
            "epoch: 6500, acc: 0.750, loss: 0.499\n",
            "epoch: 6600, acc: 0.760, loss: 0.478\n",
            "epoch: 6700, acc: 0.753, loss: 0.487\n",
            "epoch: 6800, acc: 0.763, loss: 0.472\n",
            "epoch: 6900, acc: 0.763, loss: 0.472\n",
            "epoch: 7000, acc: 0.767, loss: 0.464\n",
            "epoch: 7100, acc: 0.767, loss: 0.469\n",
            "epoch: 7200, acc: 0.763, loss: 0.469\n",
            "epoch: 7300, acc: 0.780, loss: 0.455\n",
            "epoch: 7400, acc: 0.783, loss: 0.455\n",
            "epoch: 7500, acc: 0.813, loss: 0.457\n",
            "epoch: 7600, acc: 0.807, loss: 0.451\n",
            "epoch: 7700, acc: 0.797, loss: 0.463\n",
            "epoch: 7800, acc: 0.817, loss: 0.438\n",
            "epoch: 7900, acc: 0.810, loss: 0.452\n",
            "epoch: 8000, acc: 0.823, loss: 0.435\n",
            "epoch: 8100, acc: 0.840, loss: 0.485\n",
            "epoch: 8200, acc: 0.820, loss: 0.434\n",
            "epoch: 8300, acc: 0.827, loss: 0.446\n",
            "epoch: 8400, acc: 0.837, loss: 0.431\n",
            "epoch: 8500, acc: 0.827, loss: 0.426\n",
            "epoch: 8600, acc: 0.827, loss: 0.446\n",
            "epoch: 8700, acc: 0.820, loss: 0.443\n",
            "epoch: 8800, acc: 0.830, loss: 0.446\n",
            "epoch: 8900, acc: 0.830, loss: 0.426\n",
            "epoch: 9000, acc: 0.823, loss: 0.466\n",
            "epoch: 9100, acc: 0.867, loss: 0.409\n",
            "epoch: 9200, acc: 0.840, loss: 0.427\n",
            "epoch: 9300, acc: 0.843, loss: 0.409\n",
            "epoch: 9400, acc: 0.833, loss: 0.463\n",
            "epoch: 9500, acc: 0.860, loss: 0.391\n",
            "epoch: 9600, acc: 0.827, loss: 0.435\n",
            "epoch: 9700, acc: 0.863, loss: 0.385\n",
            "epoch: 9800, acc: 0.777, loss: 0.499\n",
            "epoch: 9900, acc: 0.820, loss: 0.474\n",
            "epoch: 10000, acc: 0.843, loss: 0.421\n",
            "epoch: 10100, acc: 0.707, loss: 0.927\n",
            "epoch: 10200, acc: 0.830, loss: 0.455\n",
            "epoch: 10300, acc: 0.830, loss: 0.452\n",
            "epoch: 10400, acc: 0.857, loss: 0.401\n",
            "epoch: 10500, acc: 0.827, loss: 0.441\n",
            "epoch: 10600, acc: 0.867, loss: 0.371\n",
            "epoch: 10700, acc: 0.843, loss: 0.436\n",
            "epoch: 10800, acc: 0.840, loss: 0.425\n",
            "epoch: 10900, acc: 0.867, loss: 0.381\n",
            "epoch: 11000, acc: 0.880, loss: 0.356\n",
            "epoch: 11100, acc: 0.827, loss: 0.447\n",
            "epoch: 11200, acc: 0.863, loss: 0.365\n",
            "epoch: 11300, acc: 0.857, loss: 0.384\n",
            "epoch: 11400, acc: 0.837, loss: 0.418\n",
            "epoch: 11500, acc: 0.777, loss: 0.476\n",
            "epoch: 11600, acc: 0.813, loss: 0.490\n",
            "epoch: 11700, acc: 0.833, loss: 0.457\n",
            "epoch: 11800, acc: 0.633, loss: 1.151\n",
            "epoch: 11900, acc: 0.853, loss: 0.405\n",
            "epoch: 12000, acc: 0.887, loss: 0.351\n",
            "epoch: 12100, acc: 0.850, loss: 0.418\n",
            "epoch: 12200, acc: 0.850, loss: 0.393\n",
            "epoch: 12300, acc: 0.853, loss: 0.398\n",
            "epoch: 12400, acc: 0.893, loss: 0.347\n",
            "epoch: 12500, acc: 0.863, loss: 0.380\n",
            "epoch: 12600, acc: 0.830, loss: 0.471\n",
            "epoch: 12700, acc: 0.840, loss: 0.458\n",
            "epoch: 12800, acc: 0.860, loss: 0.411\n",
            "epoch: 12900, acc: 0.817, loss: 0.483\n",
            "epoch: 13000, acc: 0.853, loss: 0.398\n",
            "epoch: 13100, acc: 0.893, loss: 0.363\n",
            "epoch: 13200, acc: 0.873, loss: 0.391\n",
            "epoch: 13300, acc: 0.870, loss: 0.412\n",
            "epoch: 13400, acc: 0.913, loss: 0.308\n",
            "epoch: 13500, acc: 0.900, loss: 0.330\n",
            "epoch: 13600, acc: 0.863, loss: 0.412\n",
            "epoch: 13700, acc: 0.880, loss: 0.369\n",
            "epoch: 13800, acc: 0.900, loss: 0.331\n",
            "epoch: 13900, acc: 0.880, loss: 0.378\n",
            "epoch: 14000, acc: 0.873, loss: 0.390\n",
            "epoch: 14100, acc: 0.867, loss: 0.408\n",
            "epoch: 14200, acc: 0.903, loss: 0.341\n",
            "epoch: 14300, acc: 0.903, loss: 0.331\n",
            "epoch: 14400, acc: 0.897, loss: 0.344\n",
            "epoch: 14500, acc: 0.860, loss: 0.428\n",
            "epoch: 14600, acc: 0.893, loss: 0.358\n",
            "epoch: 14700, acc: 0.880, loss: 0.391\n",
            "epoch: 14800, acc: 0.880, loss: 0.387\n",
            "epoch: 14900, acc: 0.897, loss: 0.338\n",
            "epoch: 15000, acc: 0.887, loss: 0.362\n",
            "epoch: 15100, acc: 0.887, loss: 0.357\n",
            "epoch: 15200, acc: 0.913, loss: 0.305\n",
            "epoch: 15300, acc: 0.907, loss: 0.301\n",
            "epoch: 15400, acc: 0.907, loss: 0.329\n",
            "epoch: 15500, acc: 0.900, loss: 0.335\n",
            "epoch: 15600, acc: 0.907, loss: 0.301\n",
            "epoch: 15700, acc: 0.890, loss: 0.359\n",
            "epoch: 15800, acc: 0.907, loss: 0.339\n",
            "epoch: 15900, acc: 0.907, loss: 0.318\n",
            "epoch: 16000, acc: 0.910, loss: 0.304\n",
            "epoch: 16100, acc: 0.900, loss: 0.338\n",
            "epoch: 16200, acc: 0.903, loss: 0.318\n",
            "epoch: 16300, acc: 0.910, loss: 0.298\n",
            "epoch: 16400, acc: 0.887, loss: 0.356\n",
            "epoch: 16500, acc: 0.917, loss: 0.287\n",
            "epoch: 16600, acc: 0.917, loss: 0.289\n",
            "epoch: 16700, acc: 0.890, loss: 0.346\n",
            "epoch: 16800, acc: 0.900, loss: 0.336\n",
            "epoch: 16900, acc: 0.910, loss: 0.303\n",
            "epoch: 17000, acc: 0.903, loss: 0.308\n",
            "epoch: 17100, acc: 0.887, loss: 0.359\n",
            "epoch: 17200, acc: 0.880, loss: 0.412\n",
            "epoch: 17300, acc: 0.893, loss: 0.350\n",
            "epoch: 17400, acc: 0.913, loss: 0.292\n",
            "epoch: 17500, acc: 0.883, loss: 0.363\n",
            "epoch: 17600, acc: 0.917, loss: 0.281\n",
            "epoch: 17700, acc: 0.903, loss: 0.312\n",
            "epoch: 17800, acc: 0.900, loss: 0.337\n",
            "epoch: 17900, acc: 0.910, loss: 0.299\n",
            "epoch: 18000, acc: 0.860, loss: 0.459\n",
            "epoch: 18100, acc: 0.910, loss: 0.309\n",
            "epoch: 18200, acc: 0.873, loss: 0.419\n",
            "epoch: 18300, acc: 0.873, loss: 0.439\n",
            "epoch: 18400, acc: 0.907, loss: 0.291\n",
            "epoch: 18500, acc: 0.893, loss: 0.345\n",
            "epoch: 18600, acc: 0.897, loss: 0.337\n",
            "epoch: 18700, acc: 0.883, loss: 0.369\n",
            "epoch: 18800, acc: 0.913, loss: 0.282\n",
            "epoch: 18900, acc: 0.873, loss: 0.379\n",
            "epoch: 19000, acc: 0.907, loss: 0.325\n",
            "epoch: 19100, acc: 0.913, loss: 0.291\n",
            "epoch: 19200, acc: 0.917, loss: 0.280\n",
            "epoch: 19300, acc: 0.890, loss: 0.361\n",
            "epoch: 19400, acc: 0.907, loss: 0.296\n",
            "epoch: 19500, acc: 0.850, loss: 0.509\n",
            "epoch: 19600, acc: 0.917, loss: 0.267\n",
            "epoch: 19700, acc: 0.917, loss: 0.274\n",
            "epoch: 19800, acc: 0.920, loss: 0.257\n",
            "epoch: 19900, acc: 0.907, loss: 0.312\n",
            "epoch: 20000, acc: 0.903, loss: 0.307\n",
            "epoch: 20100, acc: 0.917, loss: 0.272\n",
            "epoch: 20200, acc: 0.910, loss: 0.316\n",
            "epoch: 20300, acc: 0.887, loss: 0.374\n",
            "epoch: 20400, acc: 0.863, loss: 0.463\n",
            "epoch: 20500, acc: 0.907, loss: 0.290\n",
            "epoch: 20600, acc: 0.890, loss: 0.355\n",
            "epoch: 20700, acc: 0.900, loss: 0.327\n",
            "epoch: 20800, acc: 0.877, loss: 0.401\n",
            "epoch: 20900, acc: 0.903, loss: 0.308\n",
            "epoch: 21000, acc: 0.827, loss: 0.681\n",
            "epoch: 21100, acc: 0.907, loss: 0.288\n",
            "epoch: 21200, acc: 0.913, loss: 0.292\n",
            "epoch: 21300, acc: 0.893, loss: 0.358\n",
            "epoch: 21400, acc: 0.920, loss: 0.293\n",
            "epoch: 21500, acc: 0.893, loss: 0.391\n",
            "epoch: 21600, acc: 0.917, loss: 0.277\n",
            "epoch: 21700, acc: 0.913, loss: 0.283\n",
            "epoch: 21800, acc: 0.880, loss: 0.405\n",
            "epoch: 21900, acc: 0.900, loss: 0.362\n",
            "epoch: 22000, acc: 0.913, loss: 0.292\n",
            "epoch: 22100, acc: 0.917, loss: 0.274\n",
            "epoch: 22200, acc: 0.897, loss: 0.337\n",
            "epoch: 22300, acc: 0.910, loss: 0.258\n",
            "epoch: 22400, acc: 0.917, loss: 0.246\n",
            "epoch: 22500, acc: 0.903, loss: 0.306\n",
            "epoch: 22600, acc: 0.920, loss: 0.231\n",
            "epoch: 22700, acc: 0.920, loss: 0.236\n",
            "epoch: 22800, acc: 0.843, loss: 0.551\n",
            "epoch: 22900, acc: 0.900, loss: 0.321\n",
            "epoch: 23000, acc: 0.913, loss: 0.284\n",
            "epoch: 23100, acc: 0.910, loss: 0.323\n",
            "epoch: 23200, acc: 0.723, loss: 1.401\n",
            "epoch: 23300, acc: 0.893, loss: 0.352\n",
            "epoch: 23400, acc: 0.923, loss: 0.277\n",
            "epoch: 23500, acc: 0.920, loss: 0.294\n",
            "epoch: 23600, acc: 0.877, loss: 0.390\n",
            "epoch: 23700, acc: 0.927, loss: 0.277\n",
            "epoch: 23800, acc: 0.897, loss: 0.337\n",
            "epoch: 23900, acc: 0.920, loss: 0.247\n",
            "epoch: 24000, acc: 0.900, loss: 0.351\n",
            "epoch: 24100, acc: 0.900, loss: 0.343\n",
            "epoch: 24200, acc: 0.923, loss: 0.236\n",
            "epoch: 24300, acc: 0.917, loss: 0.243\n",
            "epoch: 24400, acc: 0.907, loss: 0.286\n",
            "epoch: 24500, acc: 0.897, loss: 0.350\n",
            "epoch: 24600, acc: 0.920, loss: 0.290\n",
            "epoch: 24700, acc: 0.890, loss: 0.369\n",
            "epoch: 24800, acc: 0.893, loss: 0.347\n",
            "epoch: 24900, acc: 0.890, loss: 0.371\n",
            "epoch: 25000, acc: 0.927, loss: 0.253\n",
            "epoch: 25100, acc: 0.877, loss: 0.423\n",
            "epoch: 25200, acc: 0.880, loss: 0.452\n",
            "epoch: 25300, acc: 0.917, loss: 0.291\n",
            "epoch: 25400, acc: 0.897, loss: 0.342\n",
            "epoch: 25500, acc: 0.877, loss: 0.406\n",
            "epoch: 25600, acc: 0.923, loss: 0.248\n",
            "epoch: 25700, acc: 0.920, loss: 0.260\n",
            "epoch: 25800, acc: 0.903, loss: 0.329\n",
            "epoch: 25900, acc: 0.880, loss: 0.412\n",
            "epoch: 26000, acc: 0.930, loss: 0.237\n",
            "epoch: 26100, acc: 0.907, loss: 0.334\n",
            "epoch: 26200, acc: 0.897, loss: 0.352\n",
            "epoch: 26300, acc: 0.930, loss: 0.226\n",
            "epoch: 26400, acc: 0.917, loss: 0.262\n",
            "epoch: 26500, acc: 0.913, loss: 0.279\n",
            "epoch: 26600, acc: 0.903, loss: 0.306\n",
            "epoch: 26700, acc: 0.923, loss: 0.262\n",
            "epoch: 26800, acc: 0.900, loss: 0.342\n",
            "epoch: 26900, acc: 0.913, loss: 0.313\n",
            "epoch: 27000, acc: 0.903, loss: 0.343\n",
            "epoch: 27100, acc: 0.917, loss: 0.273\n",
            "epoch: 27200, acc: 0.933, loss: 0.216\n",
            "epoch: 27300, acc: 0.913, loss: 0.296\n",
            "epoch: 27400, acc: 0.913, loss: 0.280\n",
            "epoch: 27500, acc: 0.930, loss: 0.229\n",
            "epoch: 27600, acc: 0.910, loss: 0.306\n",
            "epoch: 27700, acc: 0.917, loss: 0.271\n",
            "epoch: 27800, acc: 0.917, loss: 0.278\n",
            "epoch: 27900, acc: 0.933, loss: 0.258\n",
            "epoch: 28000, acc: 0.893, loss: 0.344\n",
            "epoch: 28100, acc: 0.913, loss: 0.298\n",
            "epoch: 28200, acc: 0.907, loss: 0.307\n",
            "epoch: 28300, acc: 0.907, loss: 0.325\n",
            "epoch: 28400, acc: 0.933, loss: 0.199\n",
            "epoch: 28500, acc: 0.933, loss: 0.238\n",
            "epoch: 28600, acc: 0.937, loss: 0.210\n",
            "epoch: 28700, acc: 0.927, loss: 0.259\n",
            "epoch: 28800, acc: 0.897, loss: 0.351\n",
            "epoch: 28900, acc: 0.927, loss: 0.263\n",
            "epoch: 29000, acc: 0.557, loss: 3.273\n",
            "epoch: 29100, acc: 0.923, loss: 0.271\n",
            "epoch: 29200, acc: 0.923, loss: 0.270\n",
            "epoch: 29300, acc: 0.907, loss: 0.321\n",
            "epoch: 29400, acc: 0.893, loss: 0.356\n",
            "epoch: 29500, acc: 0.920, loss: 0.270\n",
            "epoch: 29600, acc: 0.910, loss: 0.308\n",
            "epoch: 29700, acc: 0.940, loss: 0.190\n",
            "epoch: 29800, acc: 0.910, loss: 0.321\n",
            "epoch: 29900, acc: 0.927, loss: 0.223\n",
            "epoch: 30000, acc: 0.923, loss: 0.247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Adding decay to Optimizer SGD</b></span></h1>"
      ],
      "metadata": {
        "id": "ulYxCvvIDa8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "    # The init method now accepts a decay rate\n",
        "    def __init__(self, learning_rate=1.0, decay=0.0):\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "\n",
        "    # This method is called BEFORE updating the parameters\n",
        "    def pre_update_params(self):\n",
        "        # If decay is used, update the current learning rate\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.initial_learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # The update method now uses the 'current_learning_rate'\n",
        "    def update_params(self, layer):\n",
        "        layer.weights += -self.current_learning_rate * layer.dweights\n",
        "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "    # This method is called AFTER updating the parameters\n",
        "    def post_update_params(self):\n",
        "        # Increment the iteration counter for the next decay calculation\n",
        "        self.iterations += 1"
      ],
      "metadata": {
        "id": "Spy14X8VUUXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (output of previous layer)\n",
        "# and 3 output values (for 3 classes)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(decay=1e-3)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(30001):\n",
        "    # Forward pass through first dense layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Forward pass through ReLU activation\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Forward pass through second dense layer\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Forward pass through activation/loss function\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy and loss every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # 3. Update Parameters\n",
        "    optimizer.pre_update_params() # First, update the learning rate\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params() # Then, increment the iteration counter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C179_XaQUjBt",
        "outputId": "5d4cf1d0-209c-49df-cee1-2b10fd218f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.280, loss: 1.099, lr: 1.0\n",
            "epoch: 100, acc: 0.440, loss: 1.076, lr: 0.9099181073703367\n",
            "epoch: 200, acc: 0.457, loss: 1.063, lr: 0.8340283569641367\n",
            "epoch: 300, acc: 0.443, loss: 1.061, lr: 0.7698229407236336\n",
            "epoch: 400, acc: 0.450, loss: 1.060, lr: 0.7147962830593281\n",
            "epoch: 500, acc: 0.450, loss: 1.059, lr: 0.66711140760507\n",
            "epoch: 600, acc: 0.450, loss: 1.058, lr: 0.6253908692933083\n",
            "epoch: 700, acc: 0.453, loss: 1.057, lr: 0.5885815185403178\n",
            "epoch: 800, acc: 0.450, loss: 1.055, lr: 0.5558643690939411\n",
            "epoch: 900, acc: 0.453, loss: 1.052, lr: 0.526592943654555\n",
            "epoch: 1000, acc: 0.450, loss: 1.049, lr: 0.5002501250625312\n",
            "epoch: 1100, acc: 0.447, loss: 1.045, lr: 0.4764173415912339\n",
            "epoch: 1200, acc: 0.450, loss: 1.041, lr: 0.45475216007276037\n",
            "epoch: 1300, acc: 0.447, loss: 1.035, lr: 0.43497172683775553\n",
            "epoch: 1400, acc: 0.450, loss: 1.028, lr: 0.4168403501458941\n",
            "epoch: 1500, acc: 0.453, loss: 1.019, lr: 0.4001600640256102\n",
            "epoch: 1600, acc: 0.463, loss: 1.009, lr: 0.3847633705271258\n",
            "epoch: 1700, acc: 0.467, loss: 0.998, lr: 0.3705075954057058\n",
            "epoch: 1800, acc: 0.487, loss: 0.987, lr: 0.35727045373347627\n",
            "epoch: 1900, acc: 0.490, loss: 0.975, lr: 0.3449465332873405\n",
            "epoch: 2000, acc: 0.513, loss: 0.964, lr: 0.33344448149383127\n",
            "epoch: 2100, acc: 0.530, loss: 0.955, lr: 0.32268473701193934\n",
            "epoch: 2200, acc: 0.477, loss: 0.960, lr: 0.31259768677711786\n",
            "epoch: 2300, acc: 0.463, loss: 0.955, lr: 0.3031221582297666\n",
            "epoch: 2400, acc: 0.467, loss: 0.949, lr: 0.29420417769932333\n",
            "epoch: 2500, acc: 0.467, loss: 0.944, lr: 0.2857959416976279\n",
            "epoch: 2600, acc: 0.473, loss: 0.937, lr: 0.2778549597110308\n",
            "epoch: 2700, acc: 0.480, loss: 0.929, lr: 0.2703433360367667\n",
            "epoch: 2800, acc: 0.493, loss: 0.920, lr: 0.26322716504343247\n",
            "epoch: 2900, acc: 0.500, loss: 0.913, lr: 0.25647601949217746\n",
            "epoch: 3000, acc: 0.507, loss: 0.906, lr: 0.25006251562890724\n",
            "epoch: 3100, acc: 0.517, loss: 0.899, lr: 0.2439619419370578\n",
            "epoch: 3200, acc: 0.520, loss: 0.892, lr: 0.23815194093831865\n",
            "epoch: 3300, acc: 0.523, loss: 0.885, lr: 0.23261223540358225\n",
            "epoch: 3400, acc: 0.533, loss: 0.879, lr: 0.22732439190725165\n",
            "epoch: 3500, acc: 0.540, loss: 0.874, lr: 0.22227161591464767\n",
            "epoch: 3600, acc: 0.547, loss: 0.870, lr: 0.21743857360295715\n",
            "epoch: 3700, acc: 0.547, loss: 0.865, lr: 0.21281123643328367\n",
            "epoch: 3800, acc: 0.553, loss: 0.860, lr: 0.20837674515524068\n",
            "epoch: 3900, acc: 0.550, loss: 0.855, lr: 0.20412329046744235\n",
            "epoch: 4000, acc: 0.553, loss: 0.852, lr: 0.2000400080016003\n",
            "epoch: 4100, acc: 0.560, loss: 0.847, lr: 0.19611688566385566\n",
            "epoch: 4200, acc: 0.557, loss: 0.842, lr: 0.19234468166955185\n",
            "epoch: 4300, acc: 0.577, loss: 0.835, lr: 0.18871485185884126\n",
            "epoch: 4400, acc: 0.583, loss: 0.831, lr: 0.18521948508983144\n",
            "epoch: 4500, acc: 0.587, loss: 0.827, lr: 0.18185124568103292\n",
            "epoch: 4600, acc: 0.590, loss: 0.823, lr: 0.1786033220217896\n",
            "epoch: 4700, acc: 0.587, loss: 0.820, lr: 0.1754693805930865\n",
            "epoch: 4800, acc: 0.590, loss: 0.816, lr: 0.17244352474564578\n",
            "epoch: 4900, acc: 0.590, loss: 0.812, lr: 0.16952025767079165\n",
            "epoch: 5000, acc: 0.590, loss: 0.809, lr: 0.16669444907484582\n",
            "epoch: 5100, acc: 0.593, loss: 0.806, lr: 0.16396130513198884\n",
            "epoch: 5200, acc: 0.597, loss: 0.803, lr: 0.16131634134537828\n",
            "epoch: 5300, acc: 0.597, loss: 0.800, lr: 0.15875535799333226\n",
            "epoch: 5400, acc: 0.600, loss: 0.796, lr: 0.1562744178777934\n",
            "epoch: 5500, acc: 0.603, loss: 0.792, lr: 0.15386982612709646\n",
            "epoch: 5600, acc: 0.603, loss: 0.789, lr: 0.15153811183512653\n",
            "epoch: 5700, acc: 0.607, loss: 0.785, lr: 0.14927601134497687\n",
            "epoch: 5800, acc: 0.620, loss: 0.782, lr: 0.14708045300779526\n",
            "epoch: 5900, acc: 0.627, loss: 0.778, lr: 0.14494854326714016\n",
            "epoch: 6000, acc: 0.627, loss: 0.774, lr: 0.1428775539362766\n",
            "epoch: 6100, acc: 0.630, loss: 0.771, lr: 0.1408649105507818\n",
            "epoch: 6200, acc: 0.633, loss: 0.768, lr: 0.13890818169190167\n",
            "epoch: 6300, acc: 0.637, loss: 0.765, lr: 0.13700506918755992\n",
            "epoch: 6400, acc: 0.637, loss: 0.761, lr: 0.13515339910798757\n",
            "epoch: 6500, acc: 0.640, loss: 0.758, lr: 0.13335111348179757\n",
            "epoch: 6600, acc: 0.640, loss: 0.754, lr: 0.13159626266614027\n",
            "epoch: 6700, acc: 0.650, loss: 0.751, lr: 0.12988699831146902\n",
            "epoch: 6800, acc: 0.657, loss: 0.747, lr: 0.12822156686754713\n",
            "epoch: 6900, acc: 0.657, loss: 0.744, lr: 0.126598303582732\n",
            "epoch: 7000, acc: 0.657, loss: 0.740, lr: 0.12501562695336915\n",
            "epoch: 7100, acc: 0.653, loss: 0.737, lr: 0.12347203358439313\n",
            "epoch: 7200, acc: 0.653, loss: 0.734, lr: 0.12196609342602757\n",
            "epoch: 7300, acc: 0.653, loss: 0.731, lr: 0.12049644535486204\n",
            "epoch: 7400, acc: 0.653, loss: 0.727, lr: 0.11906179307060363\n",
            "epoch: 7500, acc: 0.653, loss: 0.723, lr: 0.11766090128250381\n",
            "epoch: 7600, acc: 0.660, loss: 0.720, lr: 0.11629259216187929\n",
            "epoch: 7700, acc: 0.657, loss: 0.717, lr: 0.11495574203931487\n",
            "epoch: 7800, acc: 0.663, loss: 0.714, lr: 0.11364927832708263\n",
            "epoch: 7900, acc: 0.667, loss: 0.710, lr: 0.11237217664906168\n",
            "epoch: 8000, acc: 0.677, loss: 0.706, lr: 0.11112345816201799\n",
            "epoch: 8100, acc: 0.677, loss: 0.702, lr: 0.10990218705352237\n",
            "epoch: 8200, acc: 0.680, loss: 0.698, lr: 0.10870746820306555\n",
            "epoch: 8300, acc: 0.687, loss: 0.695, lr: 0.1075384449940854\n",
            "epoch: 8400, acc: 0.687, loss: 0.692, lr: 0.10639429726566654\n",
            "epoch: 8500, acc: 0.687, loss: 0.690, lr: 0.10527423939362038\n",
            "epoch: 8600, acc: 0.687, loss: 0.688, lr: 0.10417751849150952\n",
            "epoch: 8700, acc: 0.690, loss: 0.685, lr: 0.10310341272296113\n",
            "epoch: 8800, acc: 0.697, loss: 0.681, lr: 0.1020512297173181\n",
            "epoch: 8900, acc: 0.697, loss: 0.679, lr: 0.10102030508132134\n",
            "epoch: 9000, acc: 0.703, loss: 0.675, lr: 0.1000100010001\n",
            "epoch: 9100, acc: 0.707, loss: 0.671, lr: 0.09901970492127933\n",
            "epoch: 9200, acc: 0.707, loss: 0.668, lr: 0.09804882831650162\n",
            "epoch: 9300, acc: 0.707, loss: 0.664, lr: 0.09709680551509856\n",
            "epoch: 9400, acc: 0.707, loss: 0.661, lr: 0.09616309260505818\n",
            "epoch: 9500, acc: 0.707, loss: 0.658, lr: 0.09524716639679968\n",
            "epoch: 9600, acc: 0.707, loss: 0.655, lr: 0.09434852344560807\n",
            "epoch: 9700, acc: 0.707, loss: 0.653, lr: 0.09346667912889055\n",
            "epoch: 9800, acc: 0.710, loss: 0.650, lr: 0.09260116677470137\n",
            "epoch: 9900, acc: 0.713, loss: 0.647, lr: 0.09175153683824203\n",
            "epoch: 10000, acc: 0.710, loss: 0.644, lr: 0.09091735612328393\n",
            "epoch: 10100, acc: 0.710, loss: 0.641, lr: 0.09009820704567979\n",
            "epoch: 10200, acc: 0.717, loss: 0.638, lr: 0.0892936869363336\n",
            "epoch: 10300, acc: 0.717, loss: 0.635, lr: 0.08850340738118417\n",
            "epoch: 10400, acc: 0.707, loss: 0.632, lr: 0.08772699359592946\n",
            "epoch: 10500, acc: 0.710, loss: 0.629, lr: 0.08696408383337681\n",
            "epoch: 10600, acc: 0.707, loss: 0.626, lr: 0.08621432882145012\n",
            "epoch: 10700, acc: 0.710, loss: 0.623, lr: 0.08547739123001966\n",
            "epoch: 10800, acc: 0.710, loss: 0.620, lr: 0.08475294516484448\n",
            "epoch: 10900, acc: 0.710, loss: 0.617, lr: 0.08404067568703252\n",
            "epoch: 11000, acc: 0.713, loss: 0.614, lr: 0.08334027835652971\n",
            "epoch: 11100, acc: 0.713, loss: 0.611, lr: 0.08265145879824778\n",
            "epoch: 11200, acc: 0.717, loss: 0.608, lr: 0.08197393228953193\n",
            "epoch: 11300, acc: 0.717, loss: 0.605, lr: 0.08130742336775348\n",
            "epoch: 11400, acc: 0.713, loss: 0.603, lr: 0.08065166545689167\n",
            "epoch: 11500, acc: 0.710, loss: 0.600, lr: 0.08000640051204096\n",
            "epoch: 11600, acc: 0.710, loss: 0.597, lr: 0.07937137868084769\n",
            "epoch: 11700, acc: 0.717, loss: 0.594, lr: 0.07874635798094339\n",
            "epoch: 11800, acc: 0.720, loss: 0.592, lr: 0.07813110399249942\n",
            "epoch: 11900, acc: 0.720, loss: 0.589, lr: 0.07752538956508256\n",
            "epoch: 12000, acc: 0.720, loss: 0.587, lr: 0.07692899453804139\n",
            "epoch: 12100, acc: 0.737, loss: 0.583, lr: 0.07634170547370028\n",
            "epoch: 12200, acc: 0.737, loss: 0.580, lr: 0.07576331540268202\n",
            "epoch: 12300, acc: 0.733, loss: 0.578, lr: 0.07519362358072036\n",
            "epoch: 12400, acc: 0.737, loss: 0.575, lr: 0.07463243525636241\n",
            "epoch: 12500, acc: 0.743, loss: 0.573, lr: 0.07407956144899622\n",
            "epoch: 12600, acc: 0.750, loss: 0.571, lr: 0.07353481873667181\n",
            "epoch: 12700, acc: 0.750, loss: 0.568, lr: 0.07299802905321556\n",
            "epoch: 12800, acc: 0.747, loss: 0.566, lr: 0.07246901949416625\n",
            "epoch: 12900, acc: 0.750, loss: 0.564, lr: 0.07194762213108856\n",
            "epoch: 13000, acc: 0.750, loss: 0.562, lr: 0.07143367383384527\n",
            "epoch: 13100, acc: 0.750, loss: 0.560, lr: 0.07092701610043266\n",
            "epoch: 13200, acc: 0.753, loss: 0.558, lr: 0.07042749489400663\n",
            "epoch: 13300, acc: 0.760, loss: 0.556, lr: 0.06993496048674733\n",
            "epoch: 13400, acc: 0.760, loss: 0.554, lr: 0.06944926731022988\n",
            "epoch: 13500, acc: 0.763, loss: 0.552, lr: 0.06897027381198703\n",
            "epoch: 13600, acc: 0.770, loss: 0.550, lr: 0.06849784231796699\n",
            "epoch: 13700, acc: 0.770, loss: 0.549, lr: 0.06803183890060549\n",
            "epoch: 13800, acc: 0.770, loss: 0.547, lr: 0.06757213325224677\n",
            "epoch: 13900, acc: 0.770, loss: 0.546, lr: 0.06711859856366198\n",
            "epoch: 14000, acc: 0.770, loss: 0.544, lr: 0.06667111140742715\n",
            "epoch: 14100, acc: 0.770, loss: 0.542, lr: 0.0662295516259355\n",
            "epoch: 14200, acc: 0.773, loss: 0.541, lr: 0.06579380222383052\n",
            "epoch: 14300, acc: 0.777, loss: 0.539, lr: 0.06536374926465782\n",
            "epoch: 14400, acc: 0.777, loss: 0.538, lr: 0.06493928177154361\n",
            "epoch: 14500, acc: 0.777, loss: 0.536, lr: 0.06452029163171817\n",
            "epoch: 14600, acc: 0.777, loss: 0.535, lr: 0.06410667350471184\n",
            "epoch: 14700, acc: 0.777, loss: 0.534, lr: 0.0636983247340595\n",
            "epoch: 14800, acc: 0.780, loss: 0.532, lr: 0.06329514526235838\n",
            "epoch: 14900, acc: 0.780, loss: 0.530, lr: 0.06289703754953141\n",
            "epoch: 15000, acc: 0.783, loss: 0.529, lr: 0.06250390649415588\n",
            "epoch: 15100, acc: 0.783, loss: 0.528, lr: 0.06211565935772408\n",
            "epoch: 15200, acc: 0.790, loss: 0.526, lr: 0.061732205691709376\n",
            "epoch: 15300, acc: 0.787, loss: 0.525, lr: 0.061353457267317016\n",
            "epoch: 15400, acc: 0.790, loss: 0.524, lr: 0.06097932800780535\n",
            "epoch: 15500, acc: 0.793, loss: 0.522, lr: 0.060609733923268065\n",
            "epoch: 15600, acc: 0.793, loss: 0.521, lr: 0.060244593047773964\n",
            "epoch: 15700, acc: 0.793, loss: 0.520, lr: 0.0598838253787652\n",
            "epoch: 15800, acc: 0.793, loss: 0.519, lr: 0.059527352818620156\n",
            "epoch: 15900, acc: 0.793, loss: 0.517, lr: 0.05917509911829102\n",
            "epoch: 16000, acc: 0.793, loss: 0.516, lr: 0.058826989822930754\n",
            "epoch: 16100, acc: 0.793, loss: 0.515, lr: 0.058482952219428036\n",
            "epoch: 16200, acc: 0.793, loss: 0.514, lr: 0.05814291528577242\n",
            "epoch: 16300, acc: 0.793, loss: 0.513, lr: 0.05780680964217585\n",
            "epoch: 16400, acc: 0.797, loss: 0.512, lr: 0.05747456750387953\n",
            "epoch: 16500, acc: 0.800, loss: 0.511, lr: 0.05714612263557918\n",
            "epoch: 16600, acc: 0.797, loss: 0.510, lr: 0.05682141030740383\n",
            "epoch: 16700, acc: 0.797, loss: 0.509, lr: 0.056500367252387135\n",
            "epoch: 16800, acc: 0.797, loss: 0.508, lr: 0.05618293162537221\n",
            "epoch: 16900, acc: 0.797, loss: 0.506, lr: 0.055869042963294036\n",
            "epoch: 17000, acc: 0.797, loss: 0.505, lr: 0.055558642146785936\n",
            "epoch: 17100, acc: 0.797, loss: 0.504, lr: 0.05525167136305873\n",
            "epoch: 17200, acc: 0.797, loss: 0.503, lr: 0.05494807407000384\n",
            "epoch: 17300, acc: 0.797, loss: 0.503, lr: 0.0546477949614733\n",
            "epoch: 17400, acc: 0.797, loss: 0.502, lr: 0.05435077993369205\n",
            "epoch: 17500, acc: 0.797, loss: 0.501, lr: 0.05405697605275961\n",
            "epoch: 17600, acc: 0.797, loss: 0.500, lr: 0.05376633152320017\n",
            "epoch: 17700, acc: 0.797, loss: 0.499, lr: 0.05347879565752179\n",
            "epoch: 17800, acc: 0.797, loss: 0.498, lr: 0.05319431884674717\n",
            "epoch: 17900, acc: 0.800, loss: 0.497, lr: 0.05291285253187999\n",
            "epoch: 18000, acc: 0.800, loss: 0.496, lr: 0.05263434917627244\n",
            "epoch: 18100, acc: 0.800, loss: 0.495, lr: 0.05235876223886067\n",
            "epoch: 18200, acc: 0.797, loss: 0.494, lr: 0.052086046148236885\n",
            "epoch: 18300, acc: 0.793, loss: 0.493, lr: 0.05181615627752734\n",
            "epoch: 18400, acc: 0.793, loss: 0.492, lr: 0.05154904892004742\n",
            "epoch: 18500, acc: 0.797, loss: 0.491, lr: 0.051284681265705935\n",
            "epoch: 18600, acc: 0.797, loss: 0.490, lr: 0.05102301137813154\n",
            "epoch: 18700, acc: 0.797, loss: 0.489, lr: 0.050763998172496064\n",
            "epoch: 18800, acc: 0.797, loss: 0.489, lr: 0.0505076013940098\n",
            "epoch: 18900, acc: 0.797, loss: 0.488, lr: 0.05025378159706518\n",
            "epoch: 19000, acc: 0.800, loss: 0.487, lr: 0.050002500125006254\n",
            "epoch: 19100, acc: 0.800, loss: 0.486, lr: 0.04975371909050202\n",
            "epoch: 19200, acc: 0.800, loss: 0.485, lr: 0.04950740135650279\n",
            "epoch: 19300, acc: 0.800, loss: 0.485, lr: 0.0492635105177595\n",
            "epoch: 19400, acc: 0.800, loss: 0.484, lr: 0.049022010882886415\n",
            "epoch: 19500, acc: 0.800, loss: 0.483, lr: 0.048782867456949125\n",
            "epoch: 19600, acc: 0.803, loss: 0.482, lr: 0.048546045924559446\n",
            "epoch: 19700, acc: 0.803, loss: 0.482, lr: 0.04831151263346055\n",
            "epoch: 19800, acc: 0.807, loss: 0.481, lr: 0.04807923457858551\n",
            "epoch: 19900, acc: 0.807, loss: 0.480, lr: 0.047849179386573515\n",
            "epoch: 20000, acc: 0.807, loss: 0.479, lr: 0.047621315300728606\n",
            "epoch: 20100, acc: 0.807, loss: 0.479, lr: 0.04739561116640599\n",
            "epoch: 20200, acc: 0.807, loss: 0.478, lr: 0.04717203641681211\n",
            "epoch: 20300, acc: 0.807, loss: 0.477, lr: 0.04695056105920466\n",
            "epoch: 20400, acc: 0.807, loss: 0.477, lr: 0.046731155661479507\n",
            "epoch: 20500, acc: 0.810, loss: 0.476, lr: 0.046513791339132055\n",
            "epoch: 20600, acc: 0.810, loss: 0.475, lr: 0.046298439742580674\n",
            "epoch: 20700, acc: 0.810, loss: 0.474, lr: 0.046085073044840774\n",
            "epoch: 20800, acc: 0.810, loss: 0.474, lr: 0.04587366392953805\n",
            "epoch: 20900, acc: 0.810, loss: 0.473, lr: 0.04566418557925019\n",
            "epoch: 21000, acc: 0.813, loss: 0.472, lr: 0.045456611664166556\n",
            "epoch: 21100, acc: 0.813, loss: 0.472, lr: 0.045250916331055706\n",
            "epoch: 21200, acc: 0.813, loss: 0.471, lr: 0.04504707419253119\n",
            "epoch: 21300, acc: 0.813, loss: 0.470, lr: 0.04484506031660613\n",
            "epoch: 21400, acc: 0.813, loss: 0.470, lr: 0.044644850216527525\n",
            "epoch: 21500, acc: 0.813, loss: 0.469, lr: 0.044446419840881816\n",
            "epoch: 21600, acc: 0.813, loss: 0.468, lr: 0.04424974556396301\n",
            "epoch: 21700, acc: 0.813, loss: 0.468, lr: 0.044054804176395436\n",
            "epoch: 21800, acc: 0.813, loss: 0.467, lr: 0.043861572876003334\n",
            "epoch: 21900, acc: 0.813, loss: 0.466, lr: 0.0436700292589196\n",
            "epoch: 22000, acc: 0.813, loss: 0.466, lr: 0.043480151310926564\n",
            "epoch: 22100, acc: 0.813, loss: 0.465, lr: 0.0432919173990216\n",
            "epoch: 22200, acc: 0.813, loss: 0.465, lr: 0.043105306263201\n",
            "epoch: 22300, acc: 0.813, loss: 0.464, lr: 0.0429202970084553\n",
            "epoch: 22400, acc: 0.817, loss: 0.463, lr: 0.04273686909696996\n",
            "epoch: 22500, acc: 0.817, loss: 0.463, lr: 0.04255500234052513\n",
            "epoch: 22600, acc: 0.817, loss: 0.462, lr: 0.04237467689308869\n",
            "epoch: 22700, acc: 0.820, loss: 0.462, lr: 0.042195873243596776\n",
            "epoch: 22800, acc: 0.820, loss: 0.461, lr: 0.04201857220891634\n",
            "epoch: 22900, acc: 0.820, loss: 0.460, lr: 0.04184275492698439\n",
            "epoch: 23000, acc: 0.820, loss: 0.460, lr: 0.04166840285011876\n",
            "epoch: 23100, acc: 0.820, loss: 0.459, lr: 0.041495497738495375\n",
            "epoch: 23200, acc: 0.820, loss: 0.459, lr: 0.041324021653787346\n",
            "epoch: 23300, acc: 0.820, loss: 0.458, lr: 0.04115395695296103\n",
            "epoch: 23400, acc: 0.823, loss: 0.458, lr: 0.04098528628222468\n",
            "epoch: 23500, acc: 0.823, loss: 0.457, lr: 0.04081799257112535\n",
            "epoch: 23600, acc: 0.820, loss: 0.457, lr: 0.04065205902678971\n",
            "epoch: 23700, acc: 0.823, loss: 0.456, lr: 0.04048746912830479\n",
            "epoch: 23800, acc: 0.823, loss: 0.456, lr: 0.040324206621234725\n",
            "epoch: 23900, acc: 0.823, loss: 0.455, lr: 0.04016225551226957\n",
            "epoch: 24000, acc: 0.823, loss: 0.455, lr: 0.040001600064002565\n",
            "epoch: 24100, acc: 0.823, loss: 0.454, lr: 0.039842224789832265\n",
            "epoch: 24200, acc: 0.823, loss: 0.454, lr: 0.03968411444898607\n",
            "epoch: 24300, acc: 0.823, loss: 0.453, lr: 0.03952725404166173\n",
            "epoch: 24400, acc: 0.823, loss: 0.453, lr: 0.03937162880428363\n",
            "epoch: 24500, acc: 0.827, loss: 0.452, lr: 0.03921722420487078\n",
            "epoch: 24600, acc: 0.827, loss: 0.452, lr: 0.03906402593851322\n",
            "epoch: 24700, acc: 0.827, loss: 0.451, lr: 0.0389120199229542\n",
            "epoch: 24800, acc: 0.827, loss: 0.451, lr: 0.03876119229427497\n",
            "epoch: 24900, acc: 0.827, loss: 0.450, lr: 0.03861152940267964\n",
            "epoch: 25000, acc: 0.827, loss: 0.450, lr: 0.03846301780837725\n",
            "epoch: 25100, acc: 0.827, loss: 0.449, lr: 0.03831564427755853\n",
            "epoch: 25200, acc: 0.827, loss: 0.449, lr: 0.03816939577846482\n",
            "epoch: 25300, acc: 0.827, loss: 0.448, lr: 0.038024259477546674\n",
            "epoch: 25400, acc: 0.827, loss: 0.448, lr: 0.037880222735709686\n",
            "epoch: 25500, acc: 0.827, loss: 0.447, lr: 0.03773727310464546\n",
            "epoch: 25600, acc: 0.827, loss: 0.447, lr: 0.03759539832324523\n",
            "epoch: 25700, acc: 0.827, loss: 0.447, lr: 0.03745458631409416\n",
            "epoch: 25800, acc: 0.827, loss: 0.446, lr: 0.03731482518004403\n",
            "epoch: 25900, acc: 0.823, loss: 0.446, lr: 0.03717610320086248\n",
            "epoch: 26000, acc: 0.823, loss: 0.445, lr: 0.03703840882995667\n",
            "epoch: 26100, acc: 0.823, loss: 0.445, lr: 0.036901730691169414\n",
            "epoch: 26200, acc: 0.823, loss: 0.444, lr: 0.03676605757564616\n",
            "epoch: 26300, acc: 0.823, loss: 0.444, lr: 0.03663137843877065\n",
            "epoch: 26400, acc: 0.823, loss: 0.443, lr: 0.03649768239716778\n",
            "epoch: 26500, acc: 0.823, loss: 0.443, lr: 0.03636495872577185\n",
            "epoch: 26600, acc: 0.823, loss: 0.443, lr: 0.036233196854958516\n",
            "epoch: 26700, acc: 0.823, loss: 0.442, lr: 0.0361023863677389\n",
            "epoch: 26800, acc: 0.823, loss: 0.442, lr: 0.03597251699701428\n",
            "epoch: 26900, acc: 0.823, loss: 0.441, lr: 0.035843578622889706\n",
            "epoch: 27000, acc: 0.823, loss: 0.441, lr: 0.035715561270045354\n",
            "epoch: 27100, acc: 0.823, loss: 0.441, lr: 0.03558845510516388\n",
            "epoch: 27200, acc: 0.823, loss: 0.440, lr: 0.03546225043441257\n",
            "epoch: 27300, acc: 0.823, loss: 0.440, lr: 0.035336937700978836\n",
            "epoch: 27400, acc: 0.833, loss: 0.439, lr: 0.03521250748265784\n",
            "epoch: 27500, acc: 0.833, loss: 0.439, lr: 0.03508895048949086\n",
            "epoch: 27600, acc: 0.830, loss: 0.439, lr: 0.0349662575614532\n",
            "epoch: 27700, acc: 0.830, loss: 0.438, lr: 0.03484441966619046\n",
            "epoch: 27800, acc: 0.830, loss: 0.438, lr: 0.034723427896801974\n",
            "epoch: 27900, acc: 0.830, loss: 0.437, lr: 0.03460327346967023\n",
            "epoch: 28000, acc: 0.830, loss: 0.437, lr: 0.03448394772233525\n",
            "epoch: 28100, acc: 0.830, loss: 0.437, lr: 0.034365442111412764\n",
            "epoch: 28200, acc: 0.837, loss: 0.436, lr: 0.03424774821055515\n",
            "epoch: 28300, acc: 0.837, loss: 0.436, lr: 0.03413085770845421\n",
            "epoch: 28400, acc: 0.833, loss: 0.436, lr: 0.034014762406884586\n",
            "epoch: 28500, acc: 0.837, loss: 0.435, lr: 0.033899454218787074\n",
            "epoch: 28600, acc: 0.833, loss: 0.435, lr: 0.033784925166390756\n",
            "epoch: 28700, acc: 0.837, loss: 0.434, lr: 0.03367116737937304\n",
            "epoch: 28800, acc: 0.837, loss: 0.434, lr: 0.033558173093056816\n",
            "epoch: 28900, acc: 0.840, loss: 0.434, lr: 0.0334459346466437\n",
            "epoch: 29000, acc: 0.837, loss: 0.433, lr: 0.03333444448148271\n",
            "epoch: 29100, acc: 0.837, loss: 0.433, lr: 0.0332236951393734\n",
            "epoch: 29200, acc: 0.840, loss: 0.433, lr: 0.033113679260902674\n",
            "epoch: 29300, acc: 0.840, loss: 0.432, lr: 0.033004389583814644\n",
            "epoch: 29400, acc: 0.840, loss: 0.432, lr: 0.032895818941412545\n",
            "epoch: 29500, acc: 0.840, loss: 0.432, lr: 0.03278796026099216\n",
            "epoch: 29600, acc: 0.840, loss: 0.431, lr: 0.03268080656230596\n",
            "epoch: 29700, acc: 0.840, loss: 0.431, lr: 0.0325743509560572\n",
            "epoch: 29800, acc: 0.843, loss: 0.431, lr: 0.032468586642423455\n",
            "epoch: 29900, acc: 0.843, loss: 0.430, lr: 0.03236350690960872\n",
            "epoch: 30000, acc: 0.843, loss: 0.430, lr: 0.032259105132423624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Adding momentum to Optimizer SGD</b></span></h1>"
      ],
      "metadata": {
        "id": "cyPGxt6tY_nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "    # Init now includes a momentum parameter\n",
        "    def __init__(self, learning_rate=1.0, decay=1e-3, momentum=0.0):\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # ... (pre_update_params method is the same) ...\n",
        "    # This method is called BEFORE updating the parameters\n",
        "    def pre_update_params(self):\n",
        "        # If decay is used, update the current learning rate\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.initial_learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # --- NEW MOMENTUM LOGIC ---\n",
        "        # If momentum is used, the update is modified\n",
        "        if self.momentum:\n",
        "            # Create momentum arrays if they don't exist yet\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Calculate the weight update with momentum\n",
        "            weight_updates = (self.momentum * layer.weight_momentums) - \\\n",
        "                             (self.current_learning_rate * layer.dweights)\n",
        "            layer.weight_momentums = weight_updates # Save for next iteration\n",
        "\n",
        "            # Calculate the bias update with momentum\n",
        "            bias_updates = (self.momentum * layer.bias_momentums) - \\\n",
        "                           (self.current_learning_rate * layer.dbiases)\n",
        "            layer.bias_momentums = bias_updates # Save for next iteration\n",
        "\n",
        "        # Standard SGD update (if no momentum)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "        # Apply the final update to the layer's parameters\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    # ... (post_update_params method is the same) ...\n",
        "    # This method is called AFTER updating the parameters\n",
        "    def post_update_params(self):\n",
        "        # Increment the iteration counter for the next decay calculation\n",
        "        self.iterations += 1"
      ],
      "metadata": {
        "id": "kLtj2P3uWbae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (output of previous layer)\n",
        "# and 3 output values (for 3 classes)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(30001):\n",
        "    # Forward pass through first dense layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Forward pass through ReLU activation\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Forward pass through second dense layer\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Forward pass through activation/loss function\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy and loss every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # 3. Update Parameters\n",
        "    optimizer.pre_update_params() # First, update the learning rate\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params() # Then, increment the iteration counter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e6ELWOuBZg2c",
        "outputId": "9ea4bd0d-5a8c-4367-8299-83eac2fceca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.323, loss: 1.099, lr: 1.0\n",
            "epoch: 100, acc: 0.453, loss: 1.045, lr: 0.9099181073703367\n",
            "epoch: 200, acc: 0.463, loss: 0.978, lr: 0.8340283569641367\n",
            "epoch: 300, acc: 0.607, loss: 0.832, lr: 0.7698229407236336\n",
            "epoch: 400, acc: 0.623, loss: 0.727, lr: 0.7147962830593281\n",
            "epoch: 500, acc: 0.670, loss: 0.697, lr: 0.66711140760507\n",
            "epoch: 600, acc: 0.760, loss: 0.513, lr: 0.6253908692933083\n",
            "epoch: 700, acc: 0.810, loss: 0.420, lr: 0.5885815185403178\n",
            "epoch: 800, acc: 0.840, loss: 0.358, lr: 0.5558643690939411\n",
            "epoch: 900, acc: 0.840, loss: 0.325, lr: 0.526592943654555\n",
            "epoch: 1000, acc: 0.870, loss: 0.296, lr: 0.5002501250625312\n",
            "epoch: 1100, acc: 0.897, loss: 0.268, lr: 0.4764173415912339\n",
            "epoch: 1200, acc: 0.900, loss: 0.254, lr: 0.45475216007276037\n",
            "epoch: 1300, acc: 0.900, loss: 0.245, lr: 0.43497172683775553\n",
            "epoch: 1400, acc: 0.900, loss: 0.237, lr: 0.4168403501458941\n",
            "epoch: 1500, acc: 0.900, loss: 0.228, lr: 0.4001600640256102\n",
            "epoch: 1600, acc: 0.910, loss: 0.223, lr: 0.3847633705271258\n",
            "epoch: 1700, acc: 0.913, loss: 0.219, lr: 0.3705075954057058\n",
            "epoch: 1800, acc: 0.913, loss: 0.216, lr: 0.35727045373347627\n",
            "epoch: 1900, acc: 0.917, loss: 0.213, lr: 0.3449465332873405\n",
            "epoch: 2000, acc: 0.913, loss: 0.211, lr: 0.33344448149383127\n",
            "epoch: 2100, acc: 0.910, loss: 0.209, lr: 0.32268473701193934\n",
            "epoch: 2200, acc: 0.917, loss: 0.207, lr: 0.31259768677711786\n",
            "epoch: 2300, acc: 0.913, loss: 0.205, lr: 0.3031221582297666\n",
            "epoch: 2400, acc: 0.920, loss: 0.203, lr: 0.29420417769932333\n",
            "epoch: 2500, acc: 0.913, loss: 0.201, lr: 0.2857959416976279\n",
            "epoch: 2600, acc: 0.920, loss: 0.199, lr: 0.2778549597110308\n",
            "epoch: 2700, acc: 0.920, loss: 0.198, lr: 0.2703433360367667\n",
            "epoch: 2800, acc: 0.920, loss: 0.197, lr: 0.26322716504343247\n",
            "epoch: 2900, acc: 0.923, loss: 0.196, lr: 0.25647601949217746\n",
            "epoch: 3000, acc: 0.923, loss: 0.195, lr: 0.25006251562890724\n",
            "epoch: 3100, acc: 0.923, loss: 0.194, lr: 0.2439619419370578\n",
            "epoch: 3200, acc: 0.923, loss: 0.193, lr: 0.23815194093831865\n",
            "epoch: 3300, acc: 0.927, loss: 0.192, lr: 0.23261223540358225\n",
            "epoch: 3400, acc: 0.927, loss: 0.191, lr: 0.22732439190725165\n",
            "epoch: 3500, acc: 0.923, loss: 0.191, lr: 0.22227161591464767\n",
            "epoch: 3600, acc: 0.923, loss: 0.190, lr: 0.21743857360295715\n",
            "epoch: 3700, acc: 0.930, loss: 0.189, lr: 0.21281123643328367\n",
            "epoch: 3800, acc: 0.930, loss: 0.189, lr: 0.20837674515524068\n",
            "epoch: 3900, acc: 0.930, loss: 0.188, lr: 0.20412329046744235\n",
            "epoch: 4000, acc: 0.930, loss: 0.188, lr: 0.2000400080016003\n",
            "epoch: 4100, acc: 0.930, loss: 0.187, lr: 0.19611688566385566\n",
            "epoch: 4200, acc: 0.930, loss: 0.187, lr: 0.19234468166955185\n",
            "epoch: 4300, acc: 0.927, loss: 0.186, lr: 0.18871485185884126\n",
            "epoch: 4400, acc: 0.927, loss: 0.185, lr: 0.18521948508983144\n",
            "epoch: 4500, acc: 0.917, loss: 0.185, lr: 0.18185124568103292\n",
            "epoch: 4600, acc: 0.917, loss: 0.184, lr: 0.1786033220217896\n",
            "epoch: 4700, acc: 0.917, loss: 0.184, lr: 0.1754693805930865\n",
            "epoch: 4800, acc: 0.917, loss: 0.184, lr: 0.17244352474564578\n",
            "epoch: 4900, acc: 0.920, loss: 0.183, lr: 0.16952025767079165\n",
            "epoch: 5000, acc: 0.917, loss: 0.183, lr: 0.16669444907484582\n",
            "epoch: 5100, acc: 0.917, loss: 0.182, lr: 0.16396130513198884\n",
            "epoch: 5200, acc: 0.917, loss: 0.182, lr: 0.16131634134537828\n",
            "epoch: 5300, acc: 0.917, loss: 0.182, lr: 0.15875535799333226\n",
            "epoch: 5400, acc: 0.917, loss: 0.181, lr: 0.1562744178777934\n",
            "epoch: 5500, acc: 0.917, loss: 0.181, lr: 0.15386982612709646\n",
            "epoch: 5600, acc: 0.917, loss: 0.181, lr: 0.15153811183512653\n",
            "epoch: 5700, acc: 0.917, loss: 0.181, lr: 0.14927601134497687\n",
            "epoch: 5800, acc: 0.917, loss: 0.180, lr: 0.14708045300779526\n",
            "epoch: 5900, acc: 0.917, loss: 0.180, lr: 0.14494854326714016\n",
            "epoch: 6000, acc: 0.917, loss: 0.180, lr: 0.1428775539362766\n",
            "epoch: 6100, acc: 0.917, loss: 0.180, lr: 0.1408649105507818\n",
            "epoch: 6200, acc: 0.917, loss: 0.179, lr: 0.13890818169190167\n",
            "epoch: 6300, acc: 0.917, loss: 0.179, lr: 0.13700506918755992\n",
            "epoch: 6400, acc: 0.917, loss: 0.179, lr: 0.13515339910798757\n",
            "epoch: 6500, acc: 0.920, loss: 0.179, lr: 0.13335111348179757\n",
            "epoch: 6600, acc: 0.917, loss: 0.179, lr: 0.13159626266614027\n",
            "epoch: 6700, acc: 0.917, loss: 0.178, lr: 0.12988699831146902\n",
            "epoch: 6800, acc: 0.917, loss: 0.178, lr: 0.12822156686754713\n",
            "epoch: 6900, acc: 0.920, loss: 0.178, lr: 0.126598303582732\n",
            "epoch: 7000, acc: 0.917, loss: 0.178, lr: 0.12501562695336915\n",
            "epoch: 7100, acc: 0.917, loss: 0.178, lr: 0.12347203358439313\n",
            "epoch: 7200, acc: 0.917, loss: 0.177, lr: 0.12196609342602757\n",
            "epoch: 7300, acc: 0.917, loss: 0.177, lr: 0.12049644535486204\n",
            "epoch: 7400, acc: 0.920, loss: 0.177, lr: 0.11906179307060363\n",
            "epoch: 7500, acc: 0.917, loss: 0.176, lr: 0.11766090128250381\n",
            "epoch: 7600, acc: 0.920, loss: 0.176, lr: 0.11629259216187929\n",
            "epoch: 7700, acc: 0.923, loss: 0.176, lr: 0.11495574203931487\n",
            "epoch: 7800, acc: 0.920, loss: 0.176, lr: 0.11364927832708263\n",
            "epoch: 7900, acc: 0.920, loss: 0.176, lr: 0.11237217664906168\n",
            "epoch: 8000, acc: 0.920, loss: 0.175, lr: 0.11112345816201799\n",
            "epoch: 8100, acc: 0.923, loss: 0.175, lr: 0.10990218705352237\n",
            "epoch: 8200, acc: 0.923, loss: 0.175, lr: 0.10870746820306555\n",
            "epoch: 8300, acc: 0.920, loss: 0.175, lr: 0.1075384449940854\n",
            "epoch: 8400, acc: 0.920, loss: 0.175, lr: 0.10639429726566654\n",
            "epoch: 8500, acc: 0.920, loss: 0.175, lr: 0.10527423939362038\n",
            "epoch: 8600, acc: 0.920, loss: 0.175, lr: 0.10417751849150952\n",
            "epoch: 8700, acc: 0.920, loss: 0.174, lr: 0.10310341272296113\n",
            "epoch: 8800, acc: 0.923, loss: 0.174, lr: 0.1020512297173181\n",
            "epoch: 8900, acc: 0.923, loss: 0.174, lr: 0.10102030508132134\n",
            "epoch: 9000, acc: 0.923, loss: 0.174, lr: 0.1000100010001\n",
            "epoch: 9100, acc: 0.920, loss: 0.173, lr: 0.09901970492127933\n",
            "epoch: 9200, acc: 0.923, loss: 0.173, lr: 0.09804882831650162\n",
            "epoch: 9300, acc: 0.923, loss: 0.173, lr: 0.09709680551509856\n",
            "epoch: 9400, acc: 0.927, loss: 0.173, lr: 0.09616309260505818\n",
            "epoch: 9500, acc: 0.923, loss: 0.173, lr: 0.09524716639679968\n",
            "epoch: 9600, acc: 0.927, loss: 0.173, lr: 0.09434852344560807\n",
            "epoch: 9700, acc: 0.927, loss: 0.173, lr: 0.09346667912889055\n",
            "epoch: 9800, acc: 0.923, loss: 0.173, lr: 0.09260116677470137\n",
            "epoch: 9900, acc: 0.927, loss: 0.172, lr: 0.09175153683824203\n",
            "epoch: 10000, acc: 0.927, loss: 0.172, lr: 0.09091735612328393\n",
            "epoch: 10100, acc: 0.927, loss: 0.172, lr: 0.09009820704567979\n",
            "epoch: 10200, acc: 0.923, loss: 0.172, lr: 0.0892936869363336\n",
            "epoch: 10300, acc: 0.923, loss: 0.172, lr: 0.08850340738118417\n",
            "epoch: 10400, acc: 0.927, loss: 0.172, lr: 0.08772699359592946\n",
            "epoch: 10500, acc: 0.923, loss: 0.172, lr: 0.08696408383337681\n",
            "epoch: 10600, acc: 0.927, loss: 0.172, lr: 0.08621432882145012\n",
            "epoch: 10700, acc: 0.923, loss: 0.172, lr: 0.08547739123001966\n",
            "epoch: 10800, acc: 0.923, loss: 0.171, lr: 0.08475294516484448\n",
            "epoch: 10900, acc: 0.923, loss: 0.171, lr: 0.08404067568703252\n",
            "epoch: 11000, acc: 0.923, loss: 0.171, lr: 0.08334027835652971\n",
            "epoch: 11100, acc: 0.927, loss: 0.171, lr: 0.08265145879824778\n",
            "epoch: 11200, acc: 0.927, loss: 0.171, lr: 0.08197393228953193\n",
            "epoch: 11300, acc: 0.923, loss: 0.171, lr: 0.08130742336775348\n",
            "epoch: 11400, acc: 0.927, loss: 0.171, lr: 0.08065166545689167\n",
            "epoch: 11500, acc: 0.923, loss: 0.171, lr: 0.08000640051204096\n",
            "epoch: 11600, acc: 0.923, loss: 0.171, lr: 0.07937137868084769\n",
            "epoch: 11700, acc: 0.923, loss: 0.171, lr: 0.07874635798094339\n",
            "epoch: 11800, acc: 0.923, loss: 0.171, lr: 0.07813110399249942\n",
            "epoch: 11900, acc: 0.923, loss: 0.170, lr: 0.07752538956508256\n",
            "epoch: 12000, acc: 0.927, loss: 0.170, lr: 0.07692899453804139\n",
            "epoch: 12100, acc: 0.923, loss: 0.170, lr: 0.07634170547370028\n",
            "epoch: 12200, acc: 0.927, loss: 0.170, lr: 0.07576331540268202\n",
            "epoch: 12300, acc: 0.927, loss: 0.170, lr: 0.07519362358072036\n",
            "epoch: 12400, acc: 0.927, loss: 0.170, lr: 0.07463243525636241\n",
            "epoch: 12500, acc: 0.927, loss: 0.170, lr: 0.07407956144899622\n",
            "epoch: 12600, acc: 0.927, loss: 0.170, lr: 0.07353481873667181\n",
            "epoch: 12700, acc: 0.927, loss: 0.170, lr: 0.07299802905321556\n",
            "epoch: 12800, acc: 0.927, loss: 0.170, lr: 0.07246901949416625\n",
            "epoch: 12900, acc: 0.927, loss: 0.170, lr: 0.07194762213108856\n",
            "epoch: 13000, acc: 0.927, loss: 0.170, lr: 0.07143367383384527\n",
            "epoch: 13100, acc: 0.927, loss: 0.170, lr: 0.07092701610043266\n",
            "epoch: 13200, acc: 0.927, loss: 0.170, lr: 0.07042749489400663\n",
            "epoch: 13300, acc: 0.930, loss: 0.169, lr: 0.06993496048674733\n",
            "epoch: 13400, acc: 0.930, loss: 0.169, lr: 0.06944926731022988\n",
            "epoch: 13500, acc: 0.930, loss: 0.169, lr: 0.06897027381198703\n",
            "epoch: 13600, acc: 0.930, loss: 0.169, lr: 0.06849784231796699\n",
            "epoch: 13700, acc: 0.930, loss: 0.169, lr: 0.06803183890060549\n",
            "epoch: 13800, acc: 0.930, loss: 0.169, lr: 0.06757213325224677\n",
            "epoch: 13900, acc: 0.930, loss: 0.169, lr: 0.06711859856366198\n",
            "epoch: 14000, acc: 0.930, loss: 0.169, lr: 0.06667111140742715\n",
            "epoch: 14100, acc: 0.930, loss: 0.169, lr: 0.0662295516259355\n",
            "epoch: 14200, acc: 0.930, loss: 0.169, lr: 0.06579380222383052\n",
            "epoch: 14300, acc: 0.930, loss: 0.169, lr: 0.06536374926465782\n",
            "epoch: 14400, acc: 0.927, loss: 0.169, lr: 0.06493928177154361\n",
            "epoch: 14500, acc: 0.930, loss: 0.169, lr: 0.06452029163171817\n",
            "epoch: 14600, acc: 0.930, loss: 0.169, lr: 0.06410667350471184\n",
            "epoch: 14700, acc: 0.930, loss: 0.169, lr: 0.0636983247340595\n",
            "epoch: 14800, acc: 0.930, loss: 0.169, lr: 0.06329514526235838\n",
            "epoch: 14900, acc: 0.930, loss: 0.169, lr: 0.06289703754953141\n",
            "epoch: 15000, acc: 0.930, loss: 0.168, lr: 0.06250390649415588\n",
            "epoch: 15100, acc: 0.930, loss: 0.168, lr: 0.06211565935772408\n",
            "epoch: 15200, acc: 0.930, loss: 0.168, lr: 0.061732205691709376\n",
            "epoch: 15300, acc: 0.930, loss: 0.168, lr: 0.061353457267317016\n",
            "epoch: 15400, acc: 0.930, loss: 0.168, lr: 0.06097932800780535\n",
            "epoch: 15500, acc: 0.930, loss: 0.168, lr: 0.060609733923268065\n",
            "epoch: 15600, acc: 0.930, loss: 0.168, lr: 0.060244593047773964\n",
            "epoch: 15700, acc: 0.930, loss: 0.168, lr: 0.0598838253787652\n",
            "epoch: 15800, acc: 0.930, loss: 0.168, lr: 0.059527352818620156\n",
            "epoch: 15900, acc: 0.930, loss: 0.168, lr: 0.05917509911829102\n",
            "epoch: 16000, acc: 0.930, loss: 0.168, lr: 0.058826989822930754\n",
            "epoch: 16100, acc: 0.930, loss: 0.168, lr: 0.058482952219428036\n",
            "epoch: 16200, acc: 0.930, loss: 0.168, lr: 0.05814291528577242\n",
            "epoch: 16300, acc: 0.930, loss: 0.168, lr: 0.05780680964217585\n",
            "epoch: 16400, acc: 0.930, loss: 0.168, lr: 0.05747456750387953\n",
            "epoch: 16500, acc: 0.930, loss: 0.168, lr: 0.05714612263557918\n",
            "epoch: 16600, acc: 0.930, loss: 0.168, lr: 0.05682141030740383\n",
            "epoch: 16700, acc: 0.930, loss: 0.168, lr: 0.056500367252387135\n",
            "epoch: 16800, acc: 0.930, loss: 0.168, lr: 0.05618293162537221\n",
            "epoch: 16900, acc: 0.930, loss: 0.168, lr: 0.055869042963294036\n",
            "epoch: 17000, acc: 0.930, loss: 0.167, lr: 0.055558642146785936\n",
            "epoch: 17100, acc: 0.930, loss: 0.167, lr: 0.05525167136305873\n",
            "epoch: 17200, acc: 0.930, loss: 0.167, lr: 0.05494807407000384\n",
            "epoch: 17300, acc: 0.930, loss: 0.167, lr: 0.0546477949614733\n",
            "epoch: 17400, acc: 0.930, loss: 0.167, lr: 0.05435077993369205\n",
            "epoch: 17500, acc: 0.930, loss: 0.167, lr: 0.05405697605275961\n",
            "epoch: 17600, acc: 0.930, loss: 0.167, lr: 0.05376633152320017\n",
            "epoch: 17700, acc: 0.927, loss: 0.167, lr: 0.05347879565752179\n",
            "epoch: 17800, acc: 0.927, loss: 0.167, lr: 0.05319431884674717\n",
            "epoch: 17900, acc: 0.927, loss: 0.167, lr: 0.05291285253187999\n",
            "epoch: 18000, acc: 0.930, loss: 0.167, lr: 0.05263434917627244\n",
            "epoch: 18100, acc: 0.927, loss: 0.167, lr: 0.05235876223886067\n",
            "epoch: 18200, acc: 0.927, loss: 0.167, lr: 0.052086046148236885\n",
            "epoch: 18300, acc: 0.927, loss: 0.167, lr: 0.05181615627752734\n",
            "epoch: 18400, acc: 0.927, loss: 0.167, lr: 0.05154904892004742\n",
            "epoch: 18500, acc: 0.927, loss: 0.167, lr: 0.051284681265705935\n",
            "epoch: 18600, acc: 0.927, loss: 0.167, lr: 0.05102301137813154\n",
            "epoch: 18700, acc: 0.927, loss: 0.167, lr: 0.050763998172496064\n",
            "epoch: 18800, acc: 0.927, loss: 0.167, lr: 0.0505076013940098\n",
            "epoch: 18900, acc: 0.927, loss: 0.167, lr: 0.05025378159706518\n",
            "epoch: 19000, acc: 0.927, loss: 0.167, lr: 0.050002500125006254\n",
            "epoch: 19100, acc: 0.927, loss: 0.167, lr: 0.04975371909050202\n",
            "epoch: 19200, acc: 0.927, loss: 0.167, lr: 0.04950740135650279\n",
            "epoch: 19300, acc: 0.927, loss: 0.167, lr: 0.0492635105177595\n",
            "epoch: 19400, acc: 0.927, loss: 0.167, lr: 0.049022010882886415\n",
            "epoch: 19500, acc: 0.927, loss: 0.167, lr: 0.048782867456949125\n",
            "epoch: 19600, acc: 0.927, loss: 0.167, lr: 0.048546045924559446\n",
            "epoch: 19700, acc: 0.927, loss: 0.166, lr: 0.04831151263346055\n",
            "epoch: 19800, acc: 0.927, loss: 0.166, lr: 0.04807923457858551\n",
            "epoch: 19900, acc: 0.927, loss: 0.166, lr: 0.047849179386573515\n",
            "epoch: 20000, acc: 0.927, loss: 0.166, lr: 0.047621315300728606\n",
            "epoch: 20100, acc: 0.927, loss: 0.166, lr: 0.04739561116640599\n",
            "epoch: 20200, acc: 0.927, loss: 0.166, lr: 0.04717203641681211\n",
            "epoch: 20300, acc: 0.927, loss: 0.166, lr: 0.04695056105920466\n",
            "epoch: 20400, acc: 0.927, loss: 0.166, lr: 0.046731155661479507\n",
            "epoch: 20500, acc: 0.927, loss: 0.166, lr: 0.046513791339132055\n",
            "epoch: 20600, acc: 0.927, loss: 0.166, lr: 0.046298439742580674\n",
            "epoch: 20700, acc: 0.927, loss: 0.166, lr: 0.046085073044840774\n",
            "epoch: 20800, acc: 0.927, loss: 0.166, lr: 0.04587366392953805\n",
            "epoch: 20900, acc: 0.927, loss: 0.166, lr: 0.04566418557925019\n",
            "epoch: 21000, acc: 0.927, loss: 0.166, lr: 0.045456611664166556\n",
            "epoch: 21100, acc: 0.927, loss: 0.166, lr: 0.045250916331055706\n",
            "epoch: 21200, acc: 0.927, loss: 0.166, lr: 0.04504707419253119\n",
            "epoch: 21300, acc: 0.927, loss: 0.166, lr: 0.04484506031660613\n",
            "epoch: 21400, acc: 0.927, loss: 0.166, lr: 0.044644850216527525\n",
            "epoch: 21500, acc: 0.927, loss: 0.166, lr: 0.044446419840881816\n",
            "epoch: 21600, acc: 0.927, loss: 0.166, lr: 0.04424974556396301\n",
            "epoch: 21700, acc: 0.927, loss: 0.166, lr: 0.044054804176395436\n",
            "epoch: 21800, acc: 0.927, loss: 0.166, lr: 0.043861572876003334\n",
            "epoch: 21900, acc: 0.927, loss: 0.166, lr: 0.0436700292589196\n",
            "epoch: 22000, acc: 0.927, loss: 0.166, lr: 0.043480151310926564\n",
            "epoch: 22100, acc: 0.927, loss: 0.166, lr: 0.0432919173990216\n",
            "epoch: 22200, acc: 0.927, loss: 0.166, lr: 0.043105306263201\n",
            "epoch: 22300, acc: 0.927, loss: 0.166, lr: 0.0429202970084553\n",
            "epoch: 22400, acc: 0.927, loss: 0.166, lr: 0.04273686909696996\n",
            "epoch: 22500, acc: 0.927, loss: 0.166, lr: 0.04255500234052513\n",
            "epoch: 22600, acc: 0.927, loss: 0.166, lr: 0.04237467689308869\n",
            "epoch: 22700, acc: 0.927, loss: 0.166, lr: 0.042195873243596776\n",
            "epoch: 22800, acc: 0.927, loss: 0.166, lr: 0.04201857220891634\n",
            "epoch: 22900, acc: 0.927, loss: 0.166, lr: 0.04184275492698439\n",
            "epoch: 23000, acc: 0.927, loss: 0.166, lr: 0.04166840285011876\n",
            "epoch: 23100, acc: 0.927, loss: 0.165, lr: 0.041495497738495375\n",
            "epoch: 23200, acc: 0.927, loss: 0.165, lr: 0.041324021653787346\n",
            "epoch: 23300, acc: 0.927, loss: 0.165, lr: 0.04115395695296103\n",
            "epoch: 23400, acc: 0.927, loss: 0.165, lr: 0.04098528628222468\n",
            "epoch: 23500, acc: 0.927, loss: 0.165, lr: 0.04081799257112535\n",
            "epoch: 23600, acc: 0.927, loss: 0.165, lr: 0.04065205902678971\n",
            "epoch: 23700, acc: 0.927, loss: 0.165, lr: 0.04048746912830479\n",
            "epoch: 23800, acc: 0.927, loss: 0.165, lr: 0.040324206621234725\n",
            "epoch: 23900, acc: 0.927, loss: 0.165, lr: 0.04016225551226957\n",
            "epoch: 24000, acc: 0.927, loss: 0.165, lr: 0.040001600064002565\n",
            "epoch: 24100, acc: 0.927, loss: 0.165, lr: 0.039842224789832265\n",
            "epoch: 24200, acc: 0.927, loss: 0.165, lr: 0.03968411444898607\n",
            "epoch: 24300, acc: 0.927, loss: 0.165, lr: 0.03952725404166173\n",
            "epoch: 24400, acc: 0.927, loss: 0.165, lr: 0.03937162880428363\n",
            "epoch: 24500, acc: 0.927, loss: 0.165, lr: 0.03921722420487078\n",
            "epoch: 24600, acc: 0.927, loss: 0.165, lr: 0.03906402593851322\n",
            "epoch: 24700, acc: 0.927, loss: 0.165, lr: 0.0389120199229542\n",
            "epoch: 24800, acc: 0.927, loss: 0.165, lr: 0.03876119229427497\n",
            "epoch: 24900, acc: 0.927, loss: 0.165, lr: 0.03861152940267964\n",
            "epoch: 25000, acc: 0.927, loss: 0.165, lr: 0.03846301780837725\n",
            "epoch: 25100, acc: 0.927, loss: 0.165, lr: 0.03831564427755853\n",
            "epoch: 25200, acc: 0.927, loss: 0.165, lr: 0.03816939577846482\n",
            "epoch: 25300, acc: 0.927, loss: 0.165, lr: 0.038024259477546674\n",
            "epoch: 25400, acc: 0.927, loss: 0.165, lr: 0.037880222735709686\n",
            "epoch: 25500, acc: 0.927, loss: 0.165, lr: 0.03773727310464546\n",
            "epoch: 25600, acc: 0.927, loss: 0.165, lr: 0.03759539832324523\n",
            "epoch: 25700, acc: 0.927, loss: 0.165, lr: 0.03745458631409416\n",
            "epoch: 25800, acc: 0.923, loss: 0.165, lr: 0.03731482518004403\n",
            "epoch: 25900, acc: 0.927, loss: 0.165, lr: 0.03717610320086248\n",
            "epoch: 26000, acc: 0.923, loss: 0.165, lr: 0.03703840882995667\n",
            "epoch: 26100, acc: 0.923, loss: 0.165, lr: 0.036901730691169414\n",
            "epoch: 26200, acc: 0.923, loss: 0.165, lr: 0.03676605757564616\n",
            "epoch: 26300, acc: 0.923, loss: 0.165, lr: 0.03663137843877065\n",
            "epoch: 26400, acc: 0.923, loss: 0.165, lr: 0.03649768239716778\n",
            "epoch: 26500, acc: 0.923, loss: 0.165, lr: 0.03636495872577185\n",
            "epoch: 26600, acc: 0.923, loss: 0.165, lr: 0.036233196854958516\n",
            "epoch: 26700, acc: 0.923, loss: 0.165, lr: 0.0361023863677389\n",
            "epoch: 26800, acc: 0.923, loss: 0.165, lr: 0.03597251699701428\n",
            "epoch: 26900, acc: 0.923, loss: 0.165, lr: 0.035843578622889706\n",
            "epoch: 27000, acc: 0.923, loss: 0.165, lr: 0.035715561270045354\n",
            "epoch: 27100, acc: 0.923, loss: 0.164, lr: 0.03558845510516388\n",
            "epoch: 27200, acc: 0.923, loss: 0.164, lr: 0.03546225043441257\n",
            "epoch: 27300, acc: 0.923, loss: 0.164, lr: 0.035336937700978836\n",
            "epoch: 27400, acc: 0.923, loss: 0.164, lr: 0.03521250748265784\n",
            "epoch: 27500, acc: 0.923, loss: 0.164, lr: 0.03508895048949086\n",
            "epoch: 27600, acc: 0.923, loss: 0.164, lr: 0.0349662575614532\n",
            "epoch: 27700, acc: 0.923, loss: 0.164, lr: 0.03484441966619046\n",
            "epoch: 27800, acc: 0.923, loss: 0.164, lr: 0.034723427896801974\n",
            "epoch: 27900, acc: 0.923, loss: 0.164, lr: 0.03460327346967023\n",
            "epoch: 28000, acc: 0.923, loss: 0.164, lr: 0.03448394772233525\n",
            "epoch: 28100, acc: 0.923, loss: 0.164, lr: 0.034365442111412764\n",
            "epoch: 28200, acc: 0.923, loss: 0.164, lr: 0.03424774821055515\n",
            "epoch: 28300, acc: 0.923, loss: 0.164, lr: 0.03413085770845421\n",
            "epoch: 28400, acc: 0.923, loss: 0.164, lr: 0.034014762406884586\n",
            "epoch: 28500, acc: 0.923, loss: 0.164, lr: 0.033899454218787074\n",
            "epoch: 28600, acc: 0.923, loss: 0.164, lr: 0.033784925166390756\n",
            "epoch: 28700, acc: 0.923, loss: 0.164, lr: 0.03367116737937304\n",
            "epoch: 28800, acc: 0.923, loss: 0.164, lr: 0.033558173093056816\n",
            "epoch: 28900, acc: 0.923, loss: 0.164, lr: 0.0334459346466437\n",
            "epoch: 29000, acc: 0.923, loss: 0.164, lr: 0.03333444448148271\n",
            "epoch: 29100, acc: 0.923, loss: 0.164, lr: 0.0332236951393734\n",
            "epoch: 29200, acc: 0.923, loss: 0.164, lr: 0.033113679260902674\n",
            "epoch: 29300, acc: 0.923, loss: 0.164, lr: 0.033004389583814644\n",
            "epoch: 29400, acc: 0.923, loss: 0.164, lr: 0.032895818941412545\n",
            "epoch: 29500, acc: 0.923, loss: 0.164, lr: 0.03278796026099216\n",
            "epoch: 29600, acc: 0.923, loss: 0.164, lr: 0.03268080656230596\n",
            "epoch: 29700, acc: 0.923, loss: 0.164, lr: 0.0325743509560572\n",
            "epoch: 29800, acc: 0.923, loss: 0.164, lr: 0.032468586642423455\n",
            "epoch: 29900, acc: 0.923, loss: 0.164, lr: 0.03236350690960872\n",
            "epoch: 30000, acc: 0.923, loss: 0.164, lr: 0.032259105132423624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Optimizer AdaGrad</b></span></h1>"
      ],
      "metadata": {
        "id": "dt53SE9gDBjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_AdaGrad:\n",
        "    def __init__(self, learning_rate=1.0, decay=0., epsilon=1e-7):\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # ... (pre_update_params method is the same) ...\n",
        "    # This method is called BEFORE updating the parameters\n",
        "    def pre_update_params(self):\n",
        "        # If decay is used, update the current learning rate\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.initial_learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # Create cache arrays if they don't exist yet\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # --- Update the cache with the square of the current gradients ---\n",
        "        layer.weight_cache += layer.dweights**2\n",
        "        layer.bias_cache += layer.dbiases**2\n",
        "\n",
        "        # --- Perform the AdaGrad update ---\n",
        "        # Update weights\n",
        "        layer.weights += -self.current_learning_rate * layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        # Update biases\n",
        "        layer.biases += -self.current_learning_rate * layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # ... (post_update_params method is the same) ...\n",
        "    # This method is called AFTER updating the parameters\n",
        "    def post_update_params(self):\n",
        "        # Increment the iteration counter for the next decay calculation\n",
        "        self.iterations += 1"
      ],
      "metadata": {
        "id": "Ow1vocPBDCQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (output of previous layer)\n",
        "# and 3 output values (for 3 classes)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_AdaGrad(learning_rate=0.1, decay=1e-3)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(30001):\n",
        "    # Forward pass through first dense layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Forward pass through ReLU activation\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Forward pass through second dense layer\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Forward pass through activation/loss function\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy and loss every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # 3. Update Parameters\n",
        "    optimizer.pre_update_params() # First, update the learning rate\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params() # Then, increment the iteration counter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jRH3yXIvEXAo",
        "outputId": "67d60e04-f0c9-4d2f-fc3d-8ba831f3c458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.333, loss: 1.099, lr: 0.1\n",
            "epoch: 100, acc: 0.540, loss: 0.957, lr: 0.09099181073703368\n",
            "epoch: 200, acc: 0.593, loss: 0.887, lr: 0.08340283569641367\n",
            "epoch: 300, acc: 0.637, loss: 0.838, lr: 0.07698229407236336\n",
            "epoch: 400, acc: 0.650, loss: 0.799, lr: 0.07147962830593281\n",
            "epoch: 500, acc: 0.667, loss: 0.767, lr: 0.066711140760507\n",
            "epoch: 600, acc: 0.687, loss: 0.738, lr: 0.06253908692933084\n",
            "epoch: 700, acc: 0.710, loss: 0.714, lr: 0.05885815185403179\n",
            "epoch: 800, acc: 0.727, loss: 0.691, lr: 0.05558643690939411\n",
            "epoch: 900, acc: 0.740, loss: 0.673, lr: 0.0526592943654555\n",
            "epoch: 1000, acc: 0.753, loss: 0.657, lr: 0.05002501250625312\n",
            "epoch: 1100, acc: 0.750, loss: 0.641, lr: 0.04764173415912339\n",
            "epoch: 1200, acc: 0.763, loss: 0.626, lr: 0.04547521600727604\n",
            "epoch: 1300, acc: 0.770, loss: 0.612, lr: 0.04349717268377556\n",
            "epoch: 1400, acc: 0.770, loss: 0.598, lr: 0.041684035014589414\n",
            "epoch: 1500, acc: 0.780, loss: 0.586, lr: 0.040016006402561026\n",
            "epoch: 1600, acc: 0.783, loss: 0.575, lr: 0.038476337052712584\n",
            "epoch: 1700, acc: 0.783, loss: 0.565, lr: 0.037050759540570584\n",
            "epoch: 1800, acc: 0.787, loss: 0.556, lr: 0.03572704537334763\n",
            "epoch: 1900, acc: 0.797, loss: 0.548, lr: 0.03449465332873405\n",
            "epoch: 2000, acc: 0.803, loss: 0.539, lr: 0.03334444814938313\n",
            "epoch: 2100, acc: 0.803, loss: 0.530, lr: 0.032268473701193935\n",
            "epoch: 2200, acc: 0.807, loss: 0.522, lr: 0.03125976867771179\n",
            "epoch: 2300, acc: 0.813, loss: 0.514, lr: 0.030312215822976663\n",
            "epoch: 2400, acc: 0.813, loss: 0.507, lr: 0.029420417769932334\n",
            "epoch: 2500, acc: 0.807, loss: 0.500, lr: 0.02857959416976279\n",
            "epoch: 2600, acc: 0.813, loss: 0.493, lr: 0.02778549597110308\n",
            "epoch: 2700, acc: 0.820, loss: 0.486, lr: 0.027034333603676672\n",
            "epoch: 2800, acc: 0.827, loss: 0.480, lr: 0.026322716504343247\n",
            "epoch: 2900, acc: 0.830, loss: 0.475, lr: 0.02564760194921775\n",
            "epoch: 3000, acc: 0.827, loss: 0.470, lr: 0.025006251562890727\n",
            "epoch: 3100, acc: 0.830, loss: 0.466, lr: 0.024396194193705784\n",
            "epoch: 3200, acc: 0.837, loss: 0.462, lr: 0.023815194093831867\n",
            "epoch: 3300, acc: 0.843, loss: 0.458, lr: 0.023261223540358228\n",
            "epoch: 3400, acc: 0.847, loss: 0.454, lr: 0.022732439190725165\n",
            "epoch: 3500, acc: 0.853, loss: 0.450, lr: 0.02222716159146477\n",
            "epoch: 3600, acc: 0.857, loss: 0.447, lr: 0.021743857360295715\n",
            "epoch: 3700, acc: 0.860, loss: 0.443, lr: 0.02128112364332837\n",
            "epoch: 3800, acc: 0.860, loss: 0.440, lr: 0.02083767451552407\n",
            "epoch: 3900, acc: 0.860, loss: 0.437, lr: 0.020412329046744237\n",
            "epoch: 4000, acc: 0.860, loss: 0.434, lr: 0.02000400080016003\n",
            "epoch: 4100, acc: 0.860, loss: 0.431, lr: 0.019611688566385566\n",
            "epoch: 4200, acc: 0.863, loss: 0.428, lr: 0.019234468166955187\n",
            "epoch: 4300, acc: 0.863, loss: 0.425, lr: 0.018871485185884128\n",
            "epoch: 4400, acc: 0.860, loss: 0.423, lr: 0.018521948508983144\n",
            "epoch: 4500, acc: 0.863, loss: 0.421, lr: 0.018185124568103294\n",
            "epoch: 4600, acc: 0.863, loss: 0.418, lr: 0.01786033220217896\n",
            "epoch: 4700, acc: 0.863, loss: 0.416, lr: 0.01754693805930865\n",
            "epoch: 4800, acc: 0.863, loss: 0.414, lr: 0.01724435247456458\n",
            "epoch: 4900, acc: 0.863, loss: 0.412, lr: 0.016952025767079167\n",
            "epoch: 5000, acc: 0.867, loss: 0.410, lr: 0.016669444907484583\n",
            "epoch: 5100, acc: 0.867, loss: 0.408, lr: 0.016396130513198885\n",
            "epoch: 5200, acc: 0.867, loss: 0.406, lr: 0.016131634134537828\n",
            "epoch: 5300, acc: 0.863, loss: 0.404, lr: 0.015875535799333228\n",
            "epoch: 5400, acc: 0.863, loss: 0.403, lr: 0.01562744178777934\n",
            "epoch: 5500, acc: 0.867, loss: 0.401, lr: 0.015386982612709647\n",
            "epoch: 5600, acc: 0.867, loss: 0.399, lr: 0.015153811183512653\n",
            "epoch: 5700, acc: 0.867, loss: 0.398, lr: 0.014927601134497688\n",
            "epoch: 5800, acc: 0.867, loss: 0.396, lr: 0.014708045300779527\n",
            "epoch: 5900, acc: 0.867, loss: 0.395, lr: 0.014494854326714017\n",
            "epoch: 6000, acc: 0.867, loss: 0.393, lr: 0.014287755393627661\n",
            "epoch: 6100, acc: 0.867, loss: 0.392, lr: 0.01408649105507818\n",
            "epoch: 6200, acc: 0.867, loss: 0.390, lr: 0.013890818169190168\n",
            "epoch: 6300, acc: 0.867, loss: 0.389, lr: 0.013700506918755993\n",
            "epoch: 6400, acc: 0.867, loss: 0.388, lr: 0.013515339910798757\n",
            "epoch: 6500, acc: 0.867, loss: 0.386, lr: 0.013335111348179758\n",
            "epoch: 6600, acc: 0.867, loss: 0.385, lr: 0.013159626266614028\n",
            "epoch: 6700, acc: 0.867, loss: 0.384, lr: 0.012988699831146902\n",
            "epoch: 6800, acc: 0.867, loss: 0.383, lr: 0.012822156686754713\n",
            "epoch: 6900, acc: 0.867, loss: 0.381, lr: 0.0126598303582732\n",
            "epoch: 7000, acc: 0.867, loss: 0.380, lr: 0.012501562695336916\n",
            "epoch: 7100, acc: 0.870, loss: 0.379, lr: 0.012347203358439314\n",
            "epoch: 7200, acc: 0.870, loss: 0.378, lr: 0.012196609342602758\n",
            "epoch: 7300, acc: 0.870, loss: 0.377, lr: 0.012049644535486204\n",
            "epoch: 7400, acc: 0.873, loss: 0.376, lr: 0.011906179307060364\n",
            "epoch: 7500, acc: 0.877, loss: 0.375, lr: 0.011766090128250382\n",
            "epoch: 7600, acc: 0.877, loss: 0.374, lr: 0.01162925921618793\n",
            "epoch: 7700, acc: 0.877, loss: 0.373, lr: 0.011495574203931488\n",
            "epoch: 7800, acc: 0.880, loss: 0.372, lr: 0.011364927832708264\n",
            "epoch: 7900, acc: 0.880, loss: 0.371, lr: 0.011237217664906169\n",
            "epoch: 8000, acc: 0.880, loss: 0.370, lr: 0.0111123458162018\n",
            "epoch: 8100, acc: 0.880, loss: 0.369, lr: 0.010990218705352238\n",
            "epoch: 8200, acc: 0.880, loss: 0.368, lr: 0.010870746820306556\n",
            "epoch: 8300, acc: 0.880, loss: 0.367, lr: 0.01075384449940854\n",
            "epoch: 8400, acc: 0.880, loss: 0.367, lr: 0.010639429726566655\n",
            "epoch: 8500, acc: 0.880, loss: 0.366, lr: 0.010527423939362039\n",
            "epoch: 8600, acc: 0.880, loss: 0.365, lr: 0.010417751849150954\n",
            "epoch: 8700, acc: 0.880, loss: 0.364, lr: 0.010310341272296113\n",
            "epoch: 8800, acc: 0.883, loss: 0.363, lr: 0.01020512297173181\n",
            "epoch: 8900, acc: 0.883, loss: 0.362, lr: 0.010102030508132135\n",
            "epoch: 9000, acc: 0.883, loss: 0.361, lr: 0.010001000100010001\n",
            "epoch: 9100, acc: 0.883, loss: 0.361, lr: 0.009901970492127933\n",
            "epoch: 9200, acc: 0.883, loss: 0.360, lr: 0.009804882831650163\n",
            "epoch: 9300, acc: 0.883, loss: 0.359, lr: 0.009709680551509857\n",
            "epoch: 9400, acc: 0.883, loss: 0.358, lr: 0.009616309260505818\n",
            "epoch: 9500, acc: 0.883, loss: 0.358, lr: 0.00952471663967997\n",
            "epoch: 9600, acc: 0.883, loss: 0.357, lr: 0.009434852344560807\n",
            "epoch: 9700, acc: 0.883, loss: 0.356, lr: 0.009346667912889055\n",
            "epoch: 9800, acc: 0.883, loss: 0.356, lr: 0.009260116677470138\n",
            "epoch: 9900, acc: 0.887, loss: 0.355, lr: 0.009175153683824202\n",
            "epoch: 10000, acc: 0.887, loss: 0.354, lr: 0.009091735612328393\n",
            "epoch: 10100, acc: 0.887, loss: 0.354, lr: 0.00900982070456798\n",
            "epoch: 10200, acc: 0.887, loss: 0.353, lr: 0.00892936869363336\n",
            "epoch: 10300, acc: 0.887, loss: 0.352, lr: 0.008850340738118417\n",
            "epoch: 10400, acc: 0.887, loss: 0.352, lr: 0.008772699359592946\n",
            "epoch: 10500, acc: 0.887, loss: 0.351, lr: 0.008696408383337682\n",
            "epoch: 10600, acc: 0.887, loss: 0.351, lr: 0.008621432882145013\n",
            "epoch: 10700, acc: 0.887, loss: 0.350, lr: 0.008547739123001967\n",
            "epoch: 10800, acc: 0.887, loss: 0.349, lr: 0.008475294516484448\n",
            "epoch: 10900, acc: 0.887, loss: 0.349, lr: 0.008404067568703252\n",
            "epoch: 11000, acc: 0.887, loss: 0.348, lr: 0.008334027835652971\n",
            "epoch: 11100, acc: 0.887, loss: 0.348, lr: 0.008265145879824779\n",
            "epoch: 11200, acc: 0.887, loss: 0.347, lr: 0.008197393228953192\n",
            "epoch: 11300, acc: 0.887, loss: 0.347, lr: 0.008130742336775349\n",
            "epoch: 11400, acc: 0.887, loss: 0.346, lr: 0.008065166545689168\n",
            "epoch: 11500, acc: 0.887, loss: 0.345, lr: 0.008000640051204096\n",
            "epoch: 11600, acc: 0.887, loss: 0.345, lr: 0.00793713786808477\n",
            "epoch: 11700, acc: 0.890, loss: 0.344, lr: 0.007874635798094339\n",
            "epoch: 11800, acc: 0.890, loss: 0.344, lr: 0.007813110399249942\n",
            "epoch: 11900, acc: 0.890, loss: 0.343, lr: 0.007752538956508257\n",
            "epoch: 12000, acc: 0.890, loss: 0.343, lr: 0.00769289945380414\n",
            "epoch: 12100, acc: 0.890, loss: 0.342, lr: 0.007634170547370029\n",
            "epoch: 12200, acc: 0.890, loss: 0.342, lr: 0.007576331540268203\n",
            "epoch: 12300, acc: 0.890, loss: 0.342, lr: 0.007519362358072036\n",
            "epoch: 12400, acc: 0.890, loss: 0.341, lr: 0.007463243525636242\n",
            "epoch: 12500, acc: 0.890, loss: 0.341, lr: 0.007407956144899622\n",
            "epoch: 12600, acc: 0.890, loss: 0.340, lr: 0.007353481873667181\n",
            "epoch: 12700, acc: 0.890, loss: 0.340, lr: 0.007299802905321557\n",
            "epoch: 12800, acc: 0.890, loss: 0.339, lr: 0.007246901949416625\n",
            "epoch: 12900, acc: 0.890, loss: 0.339, lr: 0.0071947622131088565\n",
            "epoch: 13000, acc: 0.890, loss: 0.338, lr: 0.0071433673833845275\n",
            "epoch: 13100, acc: 0.890, loss: 0.338, lr: 0.007092701610043266\n",
            "epoch: 13200, acc: 0.890, loss: 0.338, lr: 0.007042749489400663\n",
            "epoch: 13300, acc: 0.893, loss: 0.337, lr: 0.006993496048674733\n",
            "epoch: 13400, acc: 0.893, loss: 0.337, lr: 0.006944926731022988\n",
            "epoch: 13500, acc: 0.893, loss: 0.336, lr: 0.006897027381198704\n",
            "epoch: 13600, acc: 0.893, loss: 0.336, lr: 0.006849784231796699\n",
            "epoch: 13700, acc: 0.893, loss: 0.335, lr: 0.0068031838900605495\n",
            "epoch: 13800, acc: 0.893, loss: 0.335, lr: 0.006757213325224678\n",
            "epoch: 13900, acc: 0.893, loss: 0.335, lr: 0.006711859856366198\n",
            "epoch: 14000, acc: 0.893, loss: 0.334, lr: 0.006667111140742716\n",
            "epoch: 14100, acc: 0.893, loss: 0.334, lr: 0.0066229551625935495\n",
            "epoch: 14200, acc: 0.893, loss: 0.333, lr: 0.006579380222383052\n",
            "epoch: 14300, acc: 0.893, loss: 0.333, lr: 0.006536374926465783\n",
            "epoch: 14400, acc: 0.893, loss: 0.333, lr: 0.006493928177154361\n",
            "epoch: 14500, acc: 0.893, loss: 0.332, lr: 0.006452029163171818\n",
            "epoch: 14600, acc: 0.893, loss: 0.332, lr: 0.006410667350471185\n",
            "epoch: 14700, acc: 0.893, loss: 0.332, lr: 0.00636983247340595\n",
            "epoch: 14800, acc: 0.893, loss: 0.331, lr: 0.006329514526235838\n",
            "epoch: 14900, acc: 0.893, loss: 0.331, lr: 0.0062897037549531415\n",
            "epoch: 15000, acc: 0.893, loss: 0.331, lr: 0.006250390649415589\n",
            "epoch: 15100, acc: 0.893, loss: 0.330, lr: 0.006211565935772409\n",
            "epoch: 15200, acc: 0.893, loss: 0.330, lr: 0.006173220569170938\n",
            "epoch: 15300, acc: 0.893, loss: 0.329, lr: 0.006135345726731702\n",
            "epoch: 15400, acc: 0.893, loss: 0.329, lr: 0.006097932800780536\n",
            "epoch: 15500, acc: 0.893, loss: 0.329, lr: 0.006060973392326807\n",
            "epoch: 15600, acc: 0.893, loss: 0.328, lr: 0.006024459304777397\n",
            "epoch: 15700, acc: 0.893, loss: 0.328, lr: 0.005988382537876521\n",
            "epoch: 15800, acc: 0.893, loss: 0.328, lr: 0.005952735281862016\n",
            "epoch: 15900, acc: 0.893, loss: 0.327, lr: 0.005917509911829102\n",
            "epoch: 16000, acc: 0.893, loss: 0.327, lr: 0.0058826989822930754\n",
            "epoch: 16100, acc: 0.893, loss: 0.327, lr: 0.005848295221942804\n",
            "epoch: 16200, acc: 0.893, loss: 0.326, lr: 0.005814291528577243\n",
            "epoch: 16300, acc: 0.893, loss: 0.326, lr: 0.005780680964217585\n",
            "epoch: 16400, acc: 0.893, loss: 0.326, lr: 0.005747456750387954\n",
            "epoch: 16500, acc: 0.893, loss: 0.326, lr: 0.0057146122635579185\n",
            "epoch: 16600, acc: 0.893, loss: 0.325, lr: 0.005682141030740383\n",
            "epoch: 16700, acc: 0.893, loss: 0.325, lr: 0.0056500367252387135\n",
            "epoch: 16800, acc: 0.890, loss: 0.325, lr: 0.005618293162537221\n",
            "epoch: 16900, acc: 0.890, loss: 0.324, lr: 0.005586904296329404\n",
            "epoch: 17000, acc: 0.890, loss: 0.324, lr: 0.005555864214678594\n",
            "epoch: 17100, acc: 0.890, loss: 0.324, lr: 0.005525167136305874\n",
            "epoch: 17200, acc: 0.890, loss: 0.323, lr: 0.005494807407000385\n",
            "epoch: 17300, acc: 0.890, loss: 0.323, lr: 0.005464779496147331\n",
            "epoch: 17400, acc: 0.890, loss: 0.323, lr: 0.005435077993369205\n",
            "epoch: 17500, acc: 0.890, loss: 0.323, lr: 0.005405697605275961\n",
            "epoch: 17600, acc: 0.890, loss: 0.322, lr: 0.005376633152320017\n",
            "epoch: 17700, acc: 0.890, loss: 0.322, lr: 0.005347879565752179\n",
            "epoch: 17800, acc: 0.890, loss: 0.322, lr: 0.0053194318846747175\n",
            "epoch: 17900, acc: 0.890, loss: 0.322, lr: 0.005291285253187999\n",
            "epoch: 18000, acc: 0.890, loss: 0.321, lr: 0.005263434917627244\n",
            "epoch: 18100, acc: 0.890, loss: 0.321, lr: 0.005235876223886067\n",
            "epoch: 18200, acc: 0.890, loss: 0.321, lr: 0.005208604614823689\n",
            "epoch: 18300, acc: 0.890, loss: 0.321, lr: 0.005181615627752734\n",
            "epoch: 18400, acc: 0.890, loss: 0.320, lr: 0.005154904892004743\n",
            "epoch: 18500, acc: 0.890, loss: 0.320, lr: 0.005128468126570594\n",
            "epoch: 18600, acc: 0.890, loss: 0.320, lr: 0.005102301137813154\n",
            "epoch: 18700, acc: 0.890, loss: 0.320, lr: 0.005076399817249607\n",
            "epoch: 18800, acc: 0.890, loss: 0.319, lr: 0.005050760139400981\n",
            "epoch: 18900, acc: 0.890, loss: 0.319, lr: 0.0050253781597065185\n",
            "epoch: 19000, acc: 0.890, loss: 0.319, lr: 0.005000250012500626\n",
            "epoch: 19100, acc: 0.890, loss: 0.319, lr: 0.0049753719090502024\n",
            "epoch: 19200, acc: 0.890, loss: 0.318, lr: 0.0049507401356502796\n",
            "epoch: 19300, acc: 0.890, loss: 0.318, lr: 0.0049263510517759505\n",
            "epoch: 19400, acc: 0.890, loss: 0.318, lr: 0.004902201088288642\n",
            "epoch: 19500, acc: 0.890, loss: 0.318, lr: 0.004878286745694913\n",
            "epoch: 19600, acc: 0.890, loss: 0.317, lr: 0.004854604592455945\n",
            "epoch: 19700, acc: 0.890, loss: 0.317, lr: 0.004831151263346056\n",
            "epoch: 19800, acc: 0.890, loss: 0.317, lr: 0.004807923457858552\n",
            "epoch: 19900, acc: 0.890, loss: 0.317, lr: 0.004784917938657352\n",
            "epoch: 20000, acc: 0.890, loss: 0.317, lr: 0.004762131530072861\n",
            "epoch: 20100, acc: 0.890, loss: 0.316, lr: 0.0047395611166406\n",
            "epoch: 20200, acc: 0.890, loss: 0.316, lr: 0.004717203641681211\n",
            "epoch: 20300, acc: 0.890, loss: 0.316, lr: 0.004695056105920466\n",
            "epoch: 20400, acc: 0.890, loss: 0.316, lr: 0.004673115566147951\n",
            "epoch: 20500, acc: 0.890, loss: 0.316, lr: 0.004651379133913206\n",
            "epoch: 20600, acc: 0.890, loss: 0.315, lr: 0.004629843974258067\n",
            "epoch: 20700, acc: 0.890, loss: 0.315, lr: 0.004608507304484077\n",
            "epoch: 20800, acc: 0.890, loss: 0.315, lr: 0.004587366392953806\n",
            "epoch: 20900, acc: 0.890, loss: 0.315, lr: 0.00456641855792502\n",
            "epoch: 21000, acc: 0.890, loss: 0.314, lr: 0.004545661166416656\n",
            "epoch: 21100, acc: 0.890, loss: 0.314, lr: 0.004525091633105571\n",
            "epoch: 21200, acc: 0.890, loss: 0.314, lr: 0.004504707419253119\n",
            "epoch: 21300, acc: 0.890, loss: 0.314, lr: 0.0044845060316606134\n",
            "epoch: 21400, acc: 0.890, loss: 0.314, lr: 0.004464485021652752\n",
            "epoch: 21500, acc: 0.890, loss: 0.313, lr: 0.004444641984088182\n",
            "epoch: 21600, acc: 0.890, loss: 0.313, lr: 0.004424974556396301\n",
            "epoch: 21700, acc: 0.890, loss: 0.313, lr: 0.004405480417639543\n",
            "epoch: 21800, acc: 0.890, loss: 0.313, lr: 0.004386157287600333\n",
            "epoch: 21900, acc: 0.893, loss: 0.313, lr: 0.00436700292589196\n",
            "epoch: 22000, acc: 0.893, loss: 0.312, lr: 0.004348015131092657\n",
            "epoch: 22100, acc: 0.893, loss: 0.312, lr: 0.00432919173990216\n",
            "epoch: 22200, acc: 0.893, loss: 0.312, lr: 0.0043105306263201\n",
            "epoch: 22300, acc: 0.893, loss: 0.312, lr: 0.00429202970084553\n",
            "epoch: 22400, acc: 0.893, loss: 0.312, lr: 0.004273686909696996\n",
            "epoch: 22500, acc: 0.893, loss: 0.311, lr: 0.004255500234052513\n",
            "epoch: 22600, acc: 0.893, loss: 0.311, lr: 0.0042374676893088695\n",
            "epoch: 22700, acc: 0.893, loss: 0.311, lr: 0.0042195873243596775\n",
            "epoch: 22800, acc: 0.893, loss: 0.311, lr: 0.004201857220891634\n",
            "epoch: 22900, acc: 0.893, loss: 0.311, lr: 0.004184275492698439\n",
            "epoch: 23000, acc: 0.893, loss: 0.310, lr: 0.004166840285011876\n",
            "epoch: 23100, acc: 0.893, loss: 0.310, lr: 0.004149549773849538\n",
            "epoch: 23200, acc: 0.893, loss: 0.310, lr: 0.0041324021653787344\n",
            "epoch: 23300, acc: 0.893, loss: 0.310, lr: 0.004115395695296103\n",
            "epoch: 23400, acc: 0.893, loss: 0.310, lr: 0.004098528628222468\n",
            "epoch: 23500, acc: 0.893, loss: 0.310, lr: 0.004081799257112536\n",
            "epoch: 23600, acc: 0.893, loss: 0.309, lr: 0.004065205902678971\n",
            "epoch: 23700, acc: 0.890, loss: 0.309, lr: 0.004048746912830479\n",
            "epoch: 23800, acc: 0.890, loss: 0.309, lr: 0.0040324206621234725\n",
            "epoch: 23900, acc: 0.890, loss: 0.309, lr: 0.004016225551226957\n",
            "epoch: 24000, acc: 0.890, loss: 0.309, lr: 0.004000160006400257\n",
            "epoch: 24100, acc: 0.890, loss: 0.309, lr: 0.003984222478983226\n",
            "epoch: 24200, acc: 0.890, loss: 0.308, lr: 0.003968411444898607\n",
            "epoch: 24300, acc: 0.890, loss: 0.308, lr: 0.003952725404166173\n",
            "epoch: 24400, acc: 0.890, loss: 0.308, lr: 0.003937162880428363\n",
            "epoch: 24500, acc: 0.890, loss: 0.308, lr: 0.003921722420487078\n",
            "epoch: 24600, acc: 0.890, loss: 0.308, lr: 0.003906402593851322\n",
            "epoch: 24700, acc: 0.890, loss: 0.308, lr: 0.00389120199229542\n",
            "epoch: 24800, acc: 0.890, loss: 0.307, lr: 0.0038761192294274973\n",
            "epoch: 24900, acc: 0.890, loss: 0.307, lr: 0.003861152940267964\n",
            "epoch: 25000, acc: 0.890, loss: 0.307, lr: 0.003846301780837725\n",
            "epoch: 25100, acc: 0.890, loss: 0.307, lr: 0.003831564427755853\n",
            "epoch: 25200, acc: 0.890, loss: 0.307, lr: 0.0038169395778464826\n",
            "epoch: 25300, acc: 0.890, loss: 0.307, lr: 0.0038024259477546676\n",
            "epoch: 25400, acc: 0.890, loss: 0.306, lr: 0.0037880222735709687\n",
            "epoch: 25500, acc: 0.890, loss: 0.306, lr: 0.003773727310464546\n",
            "epoch: 25600, acc: 0.890, loss: 0.306, lr: 0.003759539832324523\n",
            "epoch: 25700, acc: 0.890, loss: 0.306, lr: 0.003745458631409416\n",
            "epoch: 25800, acc: 0.890, loss: 0.306, lr: 0.003731482518004403\n",
            "epoch: 25900, acc: 0.890, loss: 0.306, lr: 0.0037176103200862484\n",
            "epoch: 26000, acc: 0.890, loss: 0.306, lr: 0.003703840882995667\n",
            "epoch: 26100, acc: 0.890, loss: 0.305, lr: 0.0036901730691169415\n",
            "epoch: 26200, acc: 0.890, loss: 0.305, lr: 0.0036766057575646164\n",
            "epoch: 26300, acc: 0.890, loss: 0.305, lr: 0.0036631378438770654\n",
            "epoch: 26400, acc: 0.890, loss: 0.305, lr: 0.0036497682397167784\n",
            "epoch: 26500, acc: 0.890, loss: 0.305, lr: 0.003636495872577185\n",
            "epoch: 26600, acc: 0.890, loss: 0.305, lr: 0.0036233196854958518\n",
            "epoch: 26700, acc: 0.890, loss: 0.305, lr: 0.0036102386367738905\n",
            "epoch: 26800, acc: 0.890, loss: 0.304, lr: 0.003597251699701428\n",
            "epoch: 26900, acc: 0.893, loss: 0.304, lr: 0.003584357862288971\n",
            "epoch: 27000, acc: 0.893, loss: 0.304, lr: 0.0035715561270045354\n",
            "epoch: 27100, acc: 0.893, loss: 0.304, lr: 0.0035588455105163884\n",
            "epoch: 27200, acc: 0.893, loss: 0.304, lr: 0.003546225043441257\n",
            "epoch: 27300, acc: 0.893, loss: 0.304, lr: 0.003533693770097884\n",
            "epoch: 27400, acc: 0.893, loss: 0.304, lr: 0.0035212507482657845\n",
            "epoch: 27500, acc: 0.893, loss: 0.304, lr: 0.003508895048949086\n",
            "epoch: 27600, acc: 0.893, loss: 0.303, lr: 0.0034966257561453197\n",
            "epoch: 27700, acc: 0.893, loss: 0.303, lr: 0.003484441966619046\n",
            "epoch: 27800, acc: 0.893, loss: 0.303, lr: 0.0034723427896801978\n",
            "epoch: 27900, acc: 0.893, loss: 0.303, lr: 0.0034603273469670233\n",
            "epoch: 28000, acc: 0.893, loss: 0.303, lr: 0.003448394772233525\n",
            "epoch: 28100, acc: 0.893, loss: 0.303, lr: 0.0034365442111412765\n",
            "epoch: 28200, acc: 0.893, loss: 0.303, lr: 0.0034247748210555155\n",
            "epoch: 28300, acc: 0.893, loss: 0.302, lr: 0.0034130857708454215\n",
            "epoch: 28400, acc: 0.893, loss: 0.302, lr: 0.003401476240688459\n",
            "epoch: 28500, acc: 0.893, loss: 0.302, lr: 0.0033899454218787074\n",
            "epoch: 28600, acc: 0.893, loss: 0.302, lr: 0.0033784925166390756\n",
            "epoch: 28700, acc: 0.893, loss: 0.302, lr: 0.0033671167379373044\n",
            "epoch: 28800, acc: 0.893, loss: 0.302, lr: 0.003355817309305682\n",
            "epoch: 28900, acc: 0.893, loss: 0.302, lr: 0.0033445934646643702\n",
            "epoch: 29000, acc: 0.893, loss: 0.302, lr: 0.0033334444481482716\n",
            "epoch: 29100, acc: 0.893, loss: 0.302, lr: 0.00332236951393734\n",
            "epoch: 29200, acc: 0.893, loss: 0.301, lr: 0.003311367926090268\n",
            "epoch: 29300, acc: 0.893, loss: 0.301, lr: 0.0033004389583814646\n",
            "epoch: 29400, acc: 0.893, loss: 0.301, lr: 0.0032895818941412547\n",
            "epoch: 29500, acc: 0.893, loss: 0.301, lr: 0.0032787960260992166\n",
            "epoch: 29600, acc: 0.893, loss: 0.301, lr: 0.003268080656230596\n",
            "epoch: 29700, acc: 0.893, loss: 0.301, lr: 0.0032574350956057203\n",
            "epoch: 29800, acc: 0.893, loss: 0.301, lr: 0.0032468586642423456\n",
            "epoch: 29900, acc: 0.893, loss: 0.301, lr: 0.0032363506909608725\n",
            "epoch: 30000, acc: 0.893, loss: 0.300, lr: 0.0032259105132423625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Optimizer RMSProp</b></span></h1>"
      ],
      "metadata": {
        "id": "Q7SKrVbCHpD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_RMSProp:\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho  # decay rate for cache averaging\n",
        "\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.initial_learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # Create cache arrays if they don’t exist\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with moving average of squared gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
        "        layer.bias_cache   = self.rho * layer.bias_cache  + (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "        # Parameter update\n",
        "        layer.weights += -self.current_learning_rate * layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases  += -self.current_learning_rate * layer.dbiases / \\\n",
        "                         (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "bNfV4ELUHph9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (output of previous layer)\n",
        "# and 3 output values (for 3 classes)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_RMSProp(learning_rate=0.1, decay=1e-3, rho=0.99)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(30001):\n",
        "    # Forward pass through first dense layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Forward pass through ReLU activation\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Forward pass through second dense layer\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Forward pass through activation/loss function\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy and loss every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # 3. Update Parameters\n",
        "    optimizer.pre_update_params() # First, update the learning rate\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params() # Then, increment the iteration counter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MoVa_rY-Hv1c",
        "outputId": "6a88ae3f-d6e4-4498-e0f9-0fbf0079184b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.390, loss: 1.099, lr: 0.1\n",
            "epoch: 100, acc: 0.440, loss: 1.037, lr: 0.09099181073703368\n",
            "epoch: 200, acc: 0.470, loss: 0.975, lr: 0.08340283569641367\n",
            "epoch: 300, acc: 0.513, loss: 0.959, lr: 0.07698229407236336\n",
            "epoch: 400, acc: 0.507, loss: 0.938, lr: 0.07147962830593281\n",
            "epoch: 500, acc: 0.510, loss: 0.924, lr: 0.066711140760507\n",
            "epoch: 600, acc: 0.513, loss: 0.907, lr: 0.06253908692933084\n",
            "epoch: 700, acc: 0.567, loss: 0.801, lr: 0.05885815185403179\n",
            "epoch: 800, acc: 0.637, loss: 0.728, lr: 0.05558643690939411\n",
            "epoch: 900, acc: 0.620, loss: 0.710, lr: 0.0526592943654555\n",
            "epoch: 1000, acc: 0.680, loss: 0.661, lr: 0.05002501250625312\n",
            "epoch: 1100, acc: 0.657, loss: 0.625, lr: 0.04764173415912339\n",
            "epoch: 1200, acc: 0.717, loss: 0.593, lr: 0.04547521600727604\n",
            "epoch: 1300, acc: 0.600, loss: 0.919, lr: 0.04349717268377556\n",
            "epoch: 1400, acc: 0.727, loss: 0.571, lr: 0.041684035014589414\n",
            "epoch: 1500, acc: 0.713, loss: 0.555, lr: 0.040016006402561026\n",
            "epoch: 1600, acc: 0.730, loss: 0.572, lr: 0.038476337052712584\n",
            "epoch: 1700, acc: 0.700, loss: 0.539, lr: 0.037050759540570584\n",
            "epoch: 1800, acc: 0.760, loss: 0.522, lr: 0.03572704537334763\n",
            "epoch: 1900, acc: 0.773, loss: 0.515, lr: 0.03449465332873405\n",
            "epoch: 2000, acc: 0.753, loss: 0.509, lr: 0.03334444814938313\n",
            "epoch: 2100, acc: 0.793, loss: 0.488, lr: 0.032268473701193935\n",
            "epoch: 2200, acc: 0.757, loss: 0.558, lr: 0.03125976867771179\n",
            "epoch: 2300, acc: 0.793, loss: 0.475, lr: 0.030312215822976663\n",
            "epoch: 2400, acc: 0.777, loss: 0.498, lr: 0.029420417769932334\n",
            "epoch: 2500, acc: 0.793, loss: 0.487, lr: 0.02857959416976279\n",
            "epoch: 2600, acc: 0.797, loss: 0.466, lr: 0.02778549597110308\n",
            "epoch: 2700, acc: 0.820, loss: 0.460, lr: 0.027034333603676672\n",
            "epoch: 2800, acc: 0.830, loss: 0.450, lr: 0.026322716504343247\n",
            "epoch: 2900, acc: 0.827, loss: 0.444, lr: 0.02564760194921775\n",
            "epoch: 3000, acc: 0.790, loss: 0.439, lr: 0.025006251562890727\n",
            "epoch: 3100, acc: 0.807, loss: 0.441, lr: 0.024396194193705784\n",
            "epoch: 3200, acc: 0.790, loss: 0.429, lr: 0.023815194093831867\n",
            "epoch: 3300, acc: 0.783, loss: 0.427, lr: 0.023261223540358228\n",
            "epoch: 3400, acc: 0.833, loss: 0.419, lr: 0.022732439190725165\n",
            "epoch: 3500, acc: 0.800, loss: 0.424, lr: 0.02222716159146477\n",
            "epoch: 3600, acc: 0.813, loss: 0.417, lr: 0.021743857360295715\n",
            "epoch: 3700, acc: 0.823, loss: 0.410, lr: 0.02128112364332837\n",
            "epoch: 3800, acc: 0.807, loss: 0.420, lr: 0.02083767451552407\n",
            "epoch: 3900, acc: 0.827, loss: 0.411, lr: 0.020412329046744237\n",
            "epoch: 4000, acc: 0.837, loss: 0.402, lr: 0.02000400080016003\n",
            "epoch: 4100, acc: 0.823, loss: 0.403, lr: 0.019611688566385566\n",
            "epoch: 4200, acc: 0.840, loss: 0.391, lr: 0.019234468166955187\n",
            "epoch: 4300, acc: 0.840, loss: 0.390, lr: 0.018871485185884128\n",
            "epoch: 4400, acc: 0.850, loss: 0.384, lr: 0.018521948508983144\n",
            "epoch: 4500, acc: 0.847, loss: 0.384, lr: 0.018185124568103294\n",
            "epoch: 4600, acc: 0.847, loss: 0.385, lr: 0.01786033220217896\n",
            "epoch: 4700, acc: 0.850, loss: 0.386, lr: 0.01754693805930865\n",
            "epoch: 4800, acc: 0.847, loss: 0.390, lr: 0.01724435247456458\n",
            "epoch: 4900, acc: 0.850, loss: 0.378, lr: 0.016952025767079167\n",
            "epoch: 5000, acc: 0.847, loss: 0.383, lr: 0.016669444907484583\n",
            "epoch: 5100, acc: 0.850, loss: 0.392, lr: 0.016396130513198885\n",
            "epoch: 5200, acc: 0.860, loss: 0.370, lr: 0.016131634134537828\n",
            "epoch: 5300, acc: 0.860, loss: 0.374, lr: 0.015875535799333228\n",
            "epoch: 5400, acc: 0.857, loss: 0.376, lr: 0.01562744178777934\n",
            "epoch: 5500, acc: 0.857, loss: 0.383, lr: 0.015386982612709647\n",
            "epoch: 5600, acc: 0.857, loss: 0.367, lr: 0.015153811183512653\n",
            "epoch: 5700, acc: 0.860, loss: 0.377, lr: 0.014927601134497688\n",
            "epoch: 5800, acc: 0.867, loss: 0.367, lr: 0.014708045300779527\n",
            "epoch: 5900, acc: 0.857, loss: 0.380, lr: 0.014494854326714017\n",
            "epoch: 6000, acc: 0.863, loss: 0.369, lr: 0.014287755393627661\n",
            "epoch: 6100, acc: 0.863, loss: 0.364, lr: 0.01408649105507818\n",
            "epoch: 6200, acc: 0.850, loss: 0.365, lr: 0.013890818169190168\n",
            "epoch: 6300, acc: 0.857, loss: 0.363, lr: 0.013700506918755993\n",
            "epoch: 6400, acc: 0.860, loss: 0.373, lr: 0.013515339910798757\n",
            "epoch: 6500, acc: 0.867, loss: 0.359, lr: 0.013335111348179758\n",
            "epoch: 6600, acc: 0.870, loss: 0.358, lr: 0.013159626266614028\n",
            "epoch: 6700, acc: 0.870, loss: 0.356, lr: 0.012988699831146902\n",
            "epoch: 6800, acc: 0.863, loss: 0.355, lr: 0.012822156686754713\n",
            "epoch: 6900, acc: 0.863, loss: 0.360, lr: 0.0126598303582732\n",
            "epoch: 7000, acc: 0.867, loss: 0.355, lr: 0.012501562695336916\n",
            "epoch: 7100, acc: 0.860, loss: 0.362, lr: 0.012347203358439314\n",
            "epoch: 7200, acc: 0.867, loss: 0.354, lr: 0.012196609342602758\n",
            "epoch: 7300, acc: 0.863, loss: 0.359, lr: 0.012049644535486204\n",
            "epoch: 7400, acc: 0.873, loss: 0.356, lr: 0.011906179307060364\n",
            "epoch: 7500, acc: 0.867, loss: 0.351, lr: 0.011766090128250382\n",
            "epoch: 7600, acc: 0.870, loss: 0.350, lr: 0.01162925921618793\n",
            "epoch: 7700, acc: 0.863, loss: 0.356, lr: 0.011495574203931488\n",
            "epoch: 7800, acc: 0.863, loss: 0.348, lr: 0.011364927832708264\n",
            "epoch: 7900, acc: 0.867, loss: 0.351, lr: 0.011237217664906169\n",
            "epoch: 8000, acc: 0.867, loss: 0.356, lr: 0.0111123458162018\n",
            "epoch: 8100, acc: 0.870, loss: 0.347, lr: 0.010990218705352238\n",
            "epoch: 8200, acc: 0.873, loss: 0.345, lr: 0.010870746820306556\n",
            "epoch: 8300, acc: 0.863, loss: 0.346, lr: 0.01075384449940854\n",
            "epoch: 8400, acc: 0.867, loss: 0.347, lr: 0.010639429726566655\n",
            "epoch: 8500, acc: 0.867, loss: 0.346, lr: 0.010527423939362039\n",
            "epoch: 8600, acc: 0.870, loss: 0.346, lr: 0.010417751849150954\n",
            "epoch: 8700, acc: 0.870, loss: 0.345, lr: 0.010310341272296113\n",
            "epoch: 8800, acc: 0.873, loss: 0.344, lr: 0.01020512297173181\n",
            "epoch: 8900, acc: 0.873, loss: 0.343, lr: 0.010102030508132135\n",
            "epoch: 9000, acc: 0.870, loss: 0.343, lr: 0.010001000100010001\n",
            "epoch: 9100, acc: 0.870, loss: 0.340, lr: 0.009901970492127933\n",
            "epoch: 9200, acc: 0.877, loss: 0.340, lr: 0.009804882831650163\n",
            "epoch: 9300, acc: 0.873, loss: 0.341, lr: 0.009709680551509857\n",
            "epoch: 9400, acc: 0.870, loss: 0.341, lr: 0.009616309260505818\n",
            "epoch: 9500, acc: 0.873, loss: 0.339, lr: 0.00952471663967997\n",
            "epoch: 9600, acc: 0.873, loss: 0.342, lr: 0.009434852344560807\n",
            "epoch: 9700, acc: 0.870, loss: 0.339, lr: 0.009346667912889055\n",
            "epoch: 9800, acc: 0.873, loss: 0.339, lr: 0.009260116677470138\n",
            "epoch: 9900, acc: 0.877, loss: 0.338, lr: 0.009175153683824202\n",
            "epoch: 10000, acc: 0.873, loss: 0.336, lr: 0.009091735612328393\n",
            "epoch: 10100, acc: 0.873, loss: 0.337, lr: 0.00900982070456798\n",
            "epoch: 10200, acc: 0.877, loss: 0.337, lr: 0.00892936869363336\n",
            "epoch: 10300, acc: 0.870, loss: 0.333, lr: 0.008850340738118417\n",
            "epoch: 10400, acc: 0.877, loss: 0.346, lr: 0.008772699359592946\n",
            "epoch: 10500, acc: 0.873, loss: 0.336, lr: 0.008696408383337682\n",
            "epoch: 10600, acc: 0.873, loss: 0.334, lr: 0.008621432882145013\n",
            "epoch: 10700, acc: 0.873, loss: 0.334, lr: 0.008547739123001967\n",
            "epoch: 10800, acc: 0.877, loss: 0.334, lr: 0.008475294516484448\n",
            "epoch: 10900, acc: 0.877, loss: 0.332, lr: 0.008404067568703252\n",
            "epoch: 11000, acc: 0.873, loss: 0.329, lr: 0.008334027835652971\n",
            "epoch: 11100, acc: 0.877, loss: 0.332, lr: 0.008265145879824779\n",
            "epoch: 11200, acc: 0.873, loss: 0.331, lr: 0.008197393228953192\n",
            "epoch: 11300, acc: 0.873, loss: 0.328, lr: 0.008130742336775349\n",
            "epoch: 11400, acc: 0.873, loss: 0.331, lr: 0.008065166545689168\n",
            "epoch: 11500, acc: 0.873, loss: 0.328, lr: 0.008000640051204096\n",
            "epoch: 11600, acc: 0.873, loss: 0.329, lr: 0.00793713786808477\n",
            "epoch: 11700, acc: 0.873, loss: 0.326, lr: 0.007874635798094339\n",
            "epoch: 11800, acc: 0.873, loss: 0.326, lr: 0.007813110399249942\n",
            "epoch: 11900, acc: 0.873, loss: 0.325, lr: 0.007752538956508257\n",
            "epoch: 12000, acc: 0.873, loss: 0.325, lr: 0.00769289945380414\n",
            "epoch: 12100, acc: 0.877, loss: 0.324, lr: 0.007634170547370029\n",
            "epoch: 12200, acc: 0.870, loss: 0.328, lr: 0.007576331540268203\n",
            "epoch: 12300, acc: 0.877, loss: 0.323, lr: 0.007519362358072036\n",
            "epoch: 12400, acc: 0.877, loss: 0.323, lr: 0.007463243525636242\n",
            "epoch: 12500, acc: 0.877, loss: 0.323, lr: 0.007407956144899622\n",
            "epoch: 12600, acc: 0.883, loss: 0.320, lr: 0.007353481873667181\n",
            "epoch: 12700, acc: 0.877, loss: 0.322, lr: 0.007299802905321557\n",
            "epoch: 12800, acc: 0.877, loss: 0.321, lr: 0.007246901949416625\n",
            "epoch: 12900, acc: 0.880, loss: 0.321, lr: 0.0071947622131088565\n",
            "epoch: 13000, acc: 0.877, loss: 0.322, lr: 0.0071433673833845275\n",
            "epoch: 13100, acc: 0.873, loss: 0.323, lr: 0.007092701610043266\n",
            "epoch: 13200, acc: 0.873, loss: 0.320, lr: 0.007042749489400663\n",
            "epoch: 13300, acc: 0.880, loss: 0.318, lr: 0.006993496048674733\n",
            "epoch: 13400, acc: 0.883, loss: 0.316, lr: 0.006944926731022988\n",
            "epoch: 13500, acc: 0.883, loss: 0.318, lr: 0.006897027381198704\n",
            "epoch: 13600, acc: 0.880, loss: 0.317, lr: 0.006849784231796699\n",
            "epoch: 13700, acc: 0.887, loss: 0.316, lr: 0.0068031838900605495\n",
            "epoch: 13800, acc: 0.880, loss: 0.317, lr: 0.006757213325224678\n",
            "epoch: 13900, acc: 0.873, loss: 0.328, lr: 0.006711859856366198\n",
            "epoch: 14000, acc: 0.873, loss: 0.317, lr: 0.006667111140742716\n",
            "epoch: 14100, acc: 0.873, loss: 0.319, lr: 0.0066229551625935495\n",
            "epoch: 14200, acc: 0.880, loss: 0.315, lr: 0.006579380222383052\n",
            "epoch: 14300, acc: 0.880, loss: 0.314, lr: 0.006536374926465783\n",
            "epoch: 14400, acc: 0.883, loss: 0.314, lr: 0.006493928177154361\n",
            "epoch: 14500, acc: 0.883, loss: 0.313, lr: 0.006452029163171818\n",
            "epoch: 14600, acc: 0.883, loss: 0.312, lr: 0.006410667350471185\n",
            "epoch: 14700, acc: 0.880, loss: 0.313, lr: 0.00636983247340595\n",
            "epoch: 14800, acc: 0.880, loss: 0.312, lr: 0.006329514526235838\n",
            "epoch: 14900, acc: 0.880, loss: 0.311, lr: 0.0062897037549531415\n",
            "epoch: 15000, acc: 0.883, loss: 0.312, lr: 0.006250390649415589\n",
            "epoch: 15100, acc: 0.883, loss: 0.310, lr: 0.006211565935772409\n",
            "epoch: 15200, acc: 0.880, loss: 0.308, lr: 0.006173220569170938\n",
            "epoch: 15300, acc: 0.883, loss: 0.309, lr: 0.006135345726731702\n",
            "epoch: 15400, acc: 0.877, loss: 0.307, lr: 0.006097932800780536\n",
            "epoch: 15500, acc: 0.883, loss: 0.305, lr: 0.006060973392326807\n",
            "epoch: 15600, acc: 0.883, loss: 0.303, lr: 0.006024459304777397\n",
            "epoch: 15700, acc: 0.890, loss: 0.302, lr: 0.005988382537876521\n",
            "epoch: 15800, acc: 0.883, loss: 0.301, lr: 0.005952735281862016\n",
            "epoch: 15900, acc: 0.887, loss: 0.301, lr: 0.005917509911829102\n",
            "epoch: 16000, acc: 0.887, loss: 0.300, lr: 0.0058826989822930754\n",
            "epoch: 16100, acc: 0.887, loss: 0.298, lr: 0.005848295221942804\n",
            "epoch: 16200, acc: 0.887, loss: 0.297, lr: 0.005814291528577243\n",
            "epoch: 16300, acc: 0.887, loss: 0.298, lr: 0.005780680964217585\n",
            "epoch: 16400, acc: 0.883, loss: 0.296, lr: 0.005747456750387954\n",
            "epoch: 16500, acc: 0.890, loss: 0.297, lr: 0.0057146122635579185\n",
            "epoch: 16600, acc: 0.890, loss: 0.295, lr: 0.005682141030740383\n",
            "epoch: 16700, acc: 0.880, loss: 0.297, lr: 0.0056500367252387135\n",
            "epoch: 16800, acc: 0.890, loss: 0.295, lr: 0.005618293162537221\n",
            "epoch: 16900, acc: 0.890, loss: 0.296, lr: 0.005586904296329404\n",
            "epoch: 17000, acc: 0.890, loss: 0.295, lr: 0.005555864214678594\n",
            "epoch: 17100, acc: 0.890, loss: 0.295, lr: 0.005525167136305874\n",
            "epoch: 17200, acc: 0.890, loss: 0.293, lr: 0.005494807407000385\n",
            "epoch: 17300, acc: 0.890, loss: 0.294, lr: 0.005464779496147331\n",
            "epoch: 17400, acc: 0.890, loss: 0.294, lr: 0.005435077993369205\n",
            "epoch: 17500, acc: 0.890, loss: 0.293, lr: 0.005405697605275961\n",
            "epoch: 17600, acc: 0.890, loss: 0.292, lr: 0.005376633152320017\n",
            "epoch: 17700, acc: 0.890, loss: 0.292, lr: 0.005347879565752179\n",
            "epoch: 17800, acc: 0.890, loss: 0.292, lr: 0.0053194318846747175\n",
            "epoch: 17900, acc: 0.890, loss: 0.292, lr: 0.005291285253187999\n",
            "epoch: 18000, acc: 0.890, loss: 0.291, lr: 0.005263434917627244\n",
            "epoch: 18100, acc: 0.890, loss: 0.291, lr: 0.005235876223886067\n",
            "epoch: 18200, acc: 0.890, loss: 0.290, lr: 0.005208604614823689\n",
            "epoch: 18300, acc: 0.890, loss: 0.290, lr: 0.005181615627752734\n",
            "epoch: 18400, acc: 0.890, loss: 0.293, lr: 0.005154904892004743\n",
            "epoch: 18500, acc: 0.887, loss: 0.294, lr: 0.005128468126570594\n",
            "epoch: 18600, acc: 0.890, loss: 0.291, lr: 0.005102301137813154\n",
            "epoch: 18700, acc: 0.890, loss: 0.290, lr: 0.005076399817249607\n",
            "epoch: 18800, acc: 0.890, loss: 0.288, lr: 0.005050760139400981\n",
            "epoch: 18900, acc: 0.890, loss: 0.289, lr: 0.0050253781597065185\n",
            "epoch: 19000, acc: 0.887, loss: 0.292, lr: 0.005000250012500626\n",
            "epoch: 19100, acc: 0.887, loss: 0.289, lr: 0.0049753719090502024\n",
            "epoch: 19200, acc: 0.890, loss: 0.287, lr: 0.0049507401356502796\n",
            "epoch: 19300, acc: 0.890, loss: 0.288, lr: 0.0049263510517759505\n",
            "epoch: 19400, acc: 0.890, loss: 0.289, lr: 0.004902201088288642\n",
            "epoch: 19500, acc: 0.893, loss: 0.290, lr: 0.004878286745694913\n",
            "epoch: 19600, acc: 0.890, loss: 0.287, lr: 0.004854604592455945\n",
            "epoch: 19700, acc: 0.890, loss: 0.287, lr: 0.004831151263346056\n",
            "epoch: 19800, acc: 0.893, loss: 0.286, lr: 0.004807923457858552\n",
            "epoch: 19900, acc: 0.890, loss: 0.287, lr: 0.004784917938657352\n",
            "epoch: 20000, acc: 0.887, loss: 0.290, lr: 0.004762131530072861\n",
            "epoch: 20100, acc: 0.890, loss: 0.287, lr: 0.0047395611166406\n",
            "epoch: 20200, acc: 0.890, loss: 0.286, lr: 0.004717203641681211\n",
            "epoch: 20300, acc: 0.893, loss: 0.285, lr: 0.004695056105920466\n",
            "epoch: 20400, acc: 0.893, loss: 0.285, lr: 0.004673115566147951\n",
            "epoch: 20500, acc: 0.893, loss: 0.285, lr: 0.004651379133913206\n",
            "epoch: 20600, acc: 0.890, loss: 0.286, lr: 0.004629843974258067\n",
            "epoch: 20700, acc: 0.887, loss: 0.286, lr: 0.004608507304484077\n",
            "epoch: 20800, acc: 0.893, loss: 0.285, lr: 0.004587366392953806\n",
            "epoch: 20900, acc: 0.890, loss: 0.288, lr: 0.00456641855792502\n",
            "epoch: 21000, acc: 0.893, loss: 0.286, lr: 0.004545661166416656\n",
            "epoch: 21100, acc: 0.893, loss: 0.285, lr: 0.004525091633105571\n",
            "epoch: 21200, acc: 0.893, loss: 0.285, lr: 0.004504707419253119\n",
            "epoch: 21300, acc: 0.887, loss: 0.285, lr: 0.0044845060316606134\n",
            "epoch: 21400, acc: 0.893, loss: 0.286, lr: 0.004464485021652752\n",
            "epoch: 21500, acc: 0.893, loss: 0.284, lr: 0.004444641984088182\n",
            "epoch: 21600, acc: 0.893, loss: 0.284, lr: 0.004424974556396301\n",
            "epoch: 21700, acc: 0.890, loss: 0.286, lr: 0.004405480417639543\n",
            "epoch: 21800, acc: 0.893, loss: 0.284, lr: 0.004386157287600333\n",
            "epoch: 21900, acc: 0.893, loss: 0.284, lr: 0.00436700292589196\n",
            "epoch: 22000, acc: 0.893, loss: 0.284, lr: 0.004348015131092657\n",
            "epoch: 22100, acc: 0.893, loss: 0.283, lr: 0.00432919173990216\n",
            "epoch: 22200, acc: 0.893, loss: 0.283, lr: 0.0043105306263201\n",
            "epoch: 22300, acc: 0.893, loss: 0.283, lr: 0.00429202970084553\n",
            "epoch: 22400, acc: 0.893, loss: 0.282, lr: 0.004273686909696996\n",
            "epoch: 22500, acc: 0.893, loss: 0.284, lr: 0.004255500234052513\n",
            "epoch: 22600, acc: 0.893, loss: 0.283, lr: 0.0042374676893088695\n",
            "epoch: 22700, acc: 0.893, loss: 0.283, lr: 0.0042195873243596775\n",
            "epoch: 22800, acc: 0.893, loss: 0.283, lr: 0.004201857220891634\n",
            "epoch: 22900, acc: 0.893, loss: 0.282, lr: 0.004184275492698439\n",
            "epoch: 23000, acc: 0.893, loss: 0.283, lr: 0.004166840285011876\n",
            "epoch: 23100, acc: 0.893, loss: 0.282, lr: 0.004149549773849538\n",
            "epoch: 23200, acc: 0.893, loss: 0.281, lr: 0.0041324021653787344\n",
            "epoch: 23300, acc: 0.893, loss: 0.282, lr: 0.004115395695296103\n",
            "epoch: 23400, acc: 0.893, loss: 0.282, lr: 0.004098528628222468\n",
            "epoch: 23500, acc: 0.893, loss: 0.281, lr: 0.004081799257112536\n",
            "epoch: 23600, acc: 0.893, loss: 0.281, lr: 0.004065205902678971\n",
            "epoch: 23700, acc: 0.893, loss: 0.281, lr: 0.004048746912830479\n",
            "epoch: 23800, acc: 0.893, loss: 0.282, lr: 0.0040324206621234725\n",
            "epoch: 23900, acc: 0.893, loss: 0.281, lr: 0.004016225551226957\n",
            "epoch: 24000, acc: 0.893, loss: 0.281, lr: 0.004000160006400257\n",
            "epoch: 24100, acc: 0.893, loss: 0.281, lr: 0.003984222478983226\n",
            "epoch: 24200, acc: 0.893, loss: 0.281, lr: 0.003968411444898607\n",
            "epoch: 24300, acc: 0.893, loss: 0.281, lr: 0.003952725404166173\n",
            "epoch: 24400, acc: 0.893, loss: 0.281, lr: 0.003937162880428363\n",
            "epoch: 24500, acc: 0.893, loss: 0.280, lr: 0.003921722420487078\n",
            "epoch: 24600, acc: 0.893, loss: 0.280, lr: 0.003906402593851322\n",
            "epoch: 24700, acc: 0.893, loss: 0.280, lr: 0.00389120199229542\n",
            "epoch: 24800, acc: 0.893, loss: 0.280, lr: 0.0038761192294274973\n",
            "epoch: 24900, acc: 0.893, loss: 0.280, lr: 0.003861152940267964\n",
            "epoch: 25000, acc: 0.893, loss: 0.280, lr: 0.003846301780837725\n",
            "epoch: 25100, acc: 0.893, loss: 0.279, lr: 0.003831564427755853\n",
            "epoch: 25200, acc: 0.893, loss: 0.279, lr: 0.0038169395778464826\n",
            "epoch: 25300, acc: 0.893, loss: 0.279, lr: 0.0038024259477546676\n",
            "epoch: 25400, acc: 0.893, loss: 0.279, lr: 0.0037880222735709687\n",
            "epoch: 25500, acc: 0.893, loss: 0.279, lr: 0.003773727310464546\n",
            "epoch: 25600, acc: 0.893, loss: 0.279, lr: 0.003759539832324523\n",
            "epoch: 25700, acc: 0.893, loss: 0.279, lr: 0.003745458631409416\n",
            "epoch: 25800, acc: 0.893, loss: 0.279, lr: 0.003731482518004403\n",
            "epoch: 25900, acc: 0.893, loss: 0.279, lr: 0.0037176103200862484\n",
            "epoch: 26000, acc: 0.893, loss: 0.279, lr: 0.003703840882995667\n",
            "epoch: 26100, acc: 0.893, loss: 0.279, lr: 0.0036901730691169415\n",
            "epoch: 26200, acc: 0.893, loss: 0.279, lr: 0.0036766057575646164\n",
            "epoch: 26300, acc: 0.893, loss: 0.278, lr: 0.0036631378438770654\n",
            "epoch: 26400, acc: 0.893, loss: 0.280, lr: 0.0036497682397167784\n",
            "epoch: 26500, acc: 0.893, loss: 0.278, lr: 0.003636495872577185\n",
            "epoch: 26600, acc: 0.900, loss: 0.279, lr: 0.0036233196854958518\n",
            "epoch: 26700, acc: 0.897, loss: 0.278, lr: 0.0036102386367738905\n",
            "epoch: 26800, acc: 0.897, loss: 0.279, lr: 0.003597251699701428\n",
            "epoch: 26900, acc: 0.893, loss: 0.278, lr: 0.003584357862288971\n",
            "epoch: 27000, acc: 0.893, loss: 0.278, lr: 0.0035715561270045354\n",
            "epoch: 27100, acc: 0.893, loss: 0.278, lr: 0.0035588455105163884\n",
            "epoch: 27200, acc: 0.890, loss: 0.277, lr: 0.003546225043441257\n",
            "epoch: 27300, acc: 0.893, loss: 0.279, lr: 0.003533693770097884\n",
            "epoch: 27400, acc: 0.893, loss: 0.278, lr: 0.0035212507482657845\n",
            "epoch: 27500, acc: 0.893, loss: 0.278, lr: 0.003508895048949086\n",
            "epoch: 27600, acc: 0.893, loss: 0.277, lr: 0.0034966257561453197\n",
            "epoch: 27700, acc: 0.897, loss: 0.277, lr: 0.003484441966619046\n",
            "epoch: 27800, acc: 0.893, loss: 0.277, lr: 0.0034723427896801978\n",
            "epoch: 27900, acc: 0.897, loss: 0.278, lr: 0.0034603273469670233\n",
            "epoch: 28000, acc: 0.893, loss: 0.278, lr: 0.003448394772233525\n",
            "epoch: 28100, acc: 0.890, loss: 0.277, lr: 0.0034365442111412765\n",
            "epoch: 28200, acc: 0.897, loss: 0.277, lr: 0.0034247748210555155\n",
            "epoch: 28300, acc: 0.897, loss: 0.277, lr: 0.0034130857708454215\n",
            "epoch: 28400, acc: 0.893, loss: 0.277, lr: 0.003401476240688459\n",
            "epoch: 28500, acc: 0.897, loss: 0.277, lr: 0.0033899454218787074\n",
            "epoch: 28600, acc: 0.897, loss: 0.278, lr: 0.0033784925166390756\n",
            "epoch: 28700, acc: 0.890, loss: 0.276, lr: 0.0033671167379373044\n",
            "epoch: 28800, acc: 0.893, loss: 0.276, lr: 0.003355817309305682\n",
            "epoch: 28900, acc: 0.897, loss: 0.277, lr: 0.0033445934646643702\n",
            "epoch: 29000, acc: 0.890, loss: 0.276, lr: 0.0033334444481482716\n",
            "epoch: 29100, acc: 0.893, loss: 0.278, lr: 0.00332236951393734\n",
            "epoch: 29200, acc: 0.897, loss: 0.276, lr: 0.003311367926090268\n",
            "epoch: 29300, acc: 0.890, loss: 0.276, lr: 0.0033004389583814646\n",
            "epoch: 29400, acc: 0.900, loss: 0.278, lr: 0.0032895818941412547\n",
            "epoch: 29500, acc: 0.897, loss: 0.276, lr: 0.0032787960260992166\n",
            "epoch: 29600, acc: 0.897, loss: 0.276, lr: 0.003268080656230596\n",
            "epoch: 29700, acc: 0.897, loss: 0.276, lr: 0.0032574350956057203\n",
            "epoch: 29800, acc: 0.897, loss: 0.277, lr: 0.0032468586642423456\n",
            "epoch: 29900, acc: 0.897, loss: 0.276, lr: 0.0032363506909608725\n",
            "epoch: 30000, acc: 0.897, loss: 0.276, lr: 0.0032259105132423625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Optimizer Adam</b></span></h1>"
      ],
      "metadata": {
        "id": "ZBklUA9XLQoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_Adam:\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.initial_learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # Initialize moment and cache arrays if not already\n",
        "        if not hasattr(layer, 'weight_momentums'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums   = np.zeros_like(layer.biases)\n",
        "            layer.weight_cache     = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache       = np.zeros_like(layer.biases)\n",
        "\n",
        "        # --- Update momentums (m) ---\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums   = self.beta_1 * layer.bias_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # --- Update cache (v) ---\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_cache   = self.beta_2 * layer.bias_cache + \\\n",
        "                             (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "        # --- Bias correction ---\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # --- Parameter update ---\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "            weight_momentums_corrected / \\\n",
        "            (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "            bias_momentums_corrected / \\\n",
        "            (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "8JlrHzrULRD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (output of previous layer)\n",
        "# and 3 output values (for 3 classes)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate=0.1, decay=1e-3, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "    # Forward pass through first dense layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Forward pass through ReLU activation\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Forward pass through second dense layer\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Forward pass through activation/loss function\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy and loss every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # 3. Update Parameters\n",
        "    optimizer.pre_update_params() # First, update the learning rate\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params() # Then, increment the iteration counter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NaanUZUNLZfc",
        "outputId": "f858f4cf-6607-4abd-bded-d54d52ec2fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.1\n",
            "epoch: 100, acc: 0.820, loss: 0.468, lr: 0.09099181073703368\n",
            "epoch: 200, acc: 0.877, loss: 0.298, lr: 0.08340283569641367\n",
            "epoch: 300, acc: 0.900, loss: 0.231, lr: 0.07698229407236336\n",
            "epoch: 400, acc: 0.920, loss: 0.193, lr: 0.07147962830593281\n",
            "epoch: 500, acc: 0.930, loss: 0.168, lr: 0.066711140760507\n",
            "epoch: 600, acc: 0.943, loss: 0.151, lr: 0.06253908692933084\n",
            "epoch: 700, acc: 0.937, loss: 0.146, lr: 0.05885815185403179\n",
            "epoch: 800, acc: 0.947, loss: 0.132, lr: 0.05558643690939411\n",
            "epoch: 900, acc: 0.957, loss: 0.124, lr: 0.0526592943654555\n",
            "epoch: 1000, acc: 0.963, loss: 0.118, lr: 0.05002501250625312\n",
            "epoch: 1100, acc: 0.963, loss: 0.113, lr: 0.04764173415912339\n",
            "epoch: 1200, acc: 0.963, loss: 0.109, lr: 0.04547521600727604\n",
            "epoch: 1300, acc: 0.963, loss: 0.106, lr: 0.04349717268377556\n",
            "epoch: 1400, acc: 0.963, loss: 0.103, lr: 0.041684035014589414\n",
            "epoch: 1500, acc: 0.970, loss: 0.100, lr: 0.040016006402561026\n",
            "epoch: 1600, acc: 0.970, loss: 0.098, lr: 0.038476337052712584\n",
            "epoch: 1700, acc: 0.970, loss: 0.097, lr: 0.037050759540570584\n",
            "epoch: 1800, acc: 0.963, loss: 0.095, lr: 0.03572704537334763\n",
            "epoch: 1900, acc: 0.970, loss: 0.094, lr: 0.03449465332873405\n",
            "epoch: 2000, acc: 0.970, loss: 0.092, lr: 0.03334444814938313\n",
            "epoch: 2100, acc: 0.970, loss: 0.091, lr: 0.032268473701193935\n",
            "epoch: 2200, acc: 0.967, loss: 0.090, lr: 0.03125976867771179\n",
            "epoch: 2300, acc: 0.973, loss: 0.089, lr: 0.030312215822976663\n",
            "epoch: 2400, acc: 0.970, loss: 0.088, lr: 0.029420417769932334\n",
            "epoch: 2500, acc: 0.973, loss: 0.087, lr: 0.02857959416976279\n",
            "epoch: 2600, acc: 0.970, loss: 0.086, lr: 0.02778549597110308\n",
            "epoch: 2700, acc: 0.970, loss: 0.086, lr: 0.027034333603676672\n",
            "epoch: 2800, acc: 0.977, loss: 0.084, lr: 0.026322716504343247\n",
            "epoch: 2900, acc: 0.970, loss: 0.083, lr: 0.02564760194921775\n",
            "epoch: 3000, acc: 0.977, loss: 0.082, lr: 0.025006251562890727\n",
            "epoch: 3100, acc: 0.973, loss: 0.082, lr: 0.024396194193705784\n",
            "epoch: 3200, acc: 0.973, loss: 0.081, lr: 0.023815194093831867\n",
            "epoch: 3300, acc: 0.977, loss: 0.080, lr: 0.023261223540358228\n",
            "epoch: 3400, acc: 0.973, loss: 0.078, lr: 0.022732439190725165\n",
            "epoch: 3500, acc: 0.973, loss: 0.077, lr: 0.02222716159146477\n",
            "epoch: 3600, acc: 0.977, loss: 0.076, lr: 0.021743857360295715\n",
            "epoch: 3700, acc: 0.973, loss: 0.075, lr: 0.02128112364332837\n",
            "epoch: 3800, acc: 0.973, loss: 0.073, lr: 0.02083767451552407\n",
            "epoch: 3900, acc: 0.973, loss: 0.073, lr: 0.020412329046744237\n",
            "epoch: 4000, acc: 0.977, loss: 0.072, lr: 0.02000400080016003\n",
            "epoch: 4100, acc: 0.977, loss: 0.071, lr: 0.019611688566385566\n",
            "epoch: 4200, acc: 0.973, loss: 0.070, lr: 0.019234468166955187\n",
            "epoch: 4300, acc: 0.977, loss: 0.068, lr: 0.018871485185884128\n",
            "epoch: 4400, acc: 0.977, loss: 0.067, lr: 0.018521948508983144\n",
            "epoch: 4500, acc: 0.973, loss: 0.067, lr: 0.018185124568103294\n",
            "epoch: 4600, acc: 0.973, loss: 0.066, lr: 0.01786033220217896\n",
            "epoch: 4700, acc: 0.977, loss: 0.065, lr: 0.01754693805930865\n",
            "epoch: 4800, acc: 0.977, loss: 0.065, lr: 0.01724435247456458\n",
            "epoch: 4900, acc: 0.977, loss: 0.064, lr: 0.016952025767079167\n",
            "epoch: 5000, acc: 0.977, loss: 0.063, lr: 0.016669444907484583\n",
            "epoch: 5100, acc: 0.977, loss: 0.063, lr: 0.016396130513198885\n",
            "epoch: 5200, acc: 0.977, loss: 0.062, lr: 0.016131634134537828\n",
            "epoch: 5300, acc: 0.980, loss: 0.061, lr: 0.015875535799333228\n",
            "epoch: 5400, acc: 0.980, loss: 0.060, lr: 0.01562744178777934\n",
            "epoch: 5500, acc: 0.980, loss: 0.059, lr: 0.015386982612709647\n",
            "epoch: 5600, acc: 0.980, loss: 0.059, lr: 0.015153811183512653\n",
            "epoch: 5700, acc: 0.980, loss: 0.058, lr: 0.014927601134497688\n",
            "epoch: 5800, acc: 0.980, loss: 0.058, lr: 0.014708045300779527\n",
            "epoch: 5900, acc: 0.980, loss: 0.057, lr: 0.014494854326714017\n",
            "epoch: 6000, acc: 0.980, loss: 0.057, lr: 0.014287755393627661\n",
            "epoch: 6100, acc: 0.983, loss: 0.056, lr: 0.01408649105507818\n",
            "epoch: 6200, acc: 0.980, loss: 0.056, lr: 0.013890818169190168\n",
            "epoch: 6300, acc: 0.980, loss: 0.055, lr: 0.013700506918755993\n",
            "epoch: 6400, acc: 0.983, loss: 0.055, lr: 0.013515339910798757\n",
            "epoch: 6500, acc: 0.980, loss: 0.055, lr: 0.013335111348179758\n",
            "epoch: 6600, acc: 0.983, loss: 0.054, lr: 0.013159626266614028\n",
            "epoch: 6700, acc: 0.983, loss: 0.054, lr: 0.012988699831146902\n",
            "epoch: 6800, acc: 0.983, loss: 0.054, lr: 0.012822156686754713\n",
            "epoch: 6900, acc: 0.983, loss: 0.053, lr: 0.0126598303582732\n",
            "epoch: 7000, acc: 0.983, loss: 0.053, lr: 0.012501562695336916\n",
            "epoch: 7100, acc: 0.983, loss: 0.053, lr: 0.012347203358439314\n",
            "epoch: 7200, acc: 0.983, loss: 0.053, lr: 0.012196609342602758\n",
            "epoch: 7300, acc: 0.983, loss: 0.052, lr: 0.012049644535486204\n",
            "epoch: 7400, acc: 0.983, loss: 0.051, lr: 0.011906179307060364\n",
            "epoch: 7500, acc: 0.983, loss: 0.051, lr: 0.011766090128250382\n",
            "epoch: 7600, acc: 0.983, loss: 0.050, lr: 0.01162925921618793\n",
            "epoch: 7700, acc: 0.983, loss: 0.050, lr: 0.011495574203931488\n",
            "epoch: 7800, acc: 0.983, loss: 0.050, lr: 0.011364927832708264\n",
            "epoch: 7900, acc: 0.983, loss: 0.050, lr: 0.011237217664906169\n",
            "epoch: 8000, acc: 0.983, loss: 0.050, lr: 0.0111123458162018\n",
            "epoch: 8100, acc: 0.983, loss: 0.049, lr: 0.010990218705352238\n",
            "epoch: 8200, acc: 0.983, loss: 0.049, lr: 0.010870746820306556\n",
            "epoch: 8300, acc: 0.983, loss: 0.049, lr: 0.01075384449940854\n",
            "epoch: 8400, acc: 0.983, loss: 0.049, lr: 0.010639429726566655\n",
            "epoch: 8500, acc: 0.983, loss: 0.049, lr: 0.010527423939362039\n",
            "epoch: 8600, acc: 0.983, loss: 0.049, lr: 0.010417751849150954\n",
            "epoch: 8700, acc: 0.983, loss: 0.048, lr: 0.010310341272296113\n",
            "epoch: 8800, acc: 0.983, loss: 0.048, lr: 0.01020512297173181\n",
            "epoch: 8900, acc: 0.983, loss: 0.048, lr: 0.010102030508132135\n",
            "epoch: 9000, acc: 0.983, loss: 0.048, lr: 0.010001000100010001\n",
            "epoch: 9100, acc: 0.983, loss: 0.048, lr: 0.009901970492127933\n",
            "epoch: 9200, acc: 0.983, loss: 0.048, lr: 0.009804882831650163\n",
            "epoch: 9300, acc: 0.983, loss: 0.048, lr: 0.009709680551509857\n",
            "epoch: 9400, acc: 0.983, loss: 0.047, lr: 0.009616309260505818\n",
            "epoch: 9500, acc: 0.983, loss: 0.047, lr: 0.00952471663967997\n",
            "epoch: 9600, acc: 0.983, loss: 0.047, lr: 0.009434852344560807\n",
            "epoch: 9700, acc: 0.983, loss: 0.047, lr: 0.009346667912889055\n",
            "epoch: 9800, acc: 0.983, loss: 0.047, lr: 0.009260116677470138\n",
            "epoch: 9900, acc: 0.983, loss: 0.047, lr: 0.009175153683824202\n",
            "epoch: 10000, acc: 0.983, loss: 0.047, lr: 0.009091735612328393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Evaluating model's performance on new data</b></span></h1>"
      ],
      "metadata": {
        "id": "xxovvhCYMs4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create a new test dataset ---\n",
        "X_test, y_test = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# --- Perform a forward pass on the test data ---\n",
        "# We use the existing, trained layers (dense1, activation1, etc.)\n",
        "dense1.forward(X_test)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "loss = loss_activation.forward(dense2.output, y_test)\n",
        "\n",
        "# --- Calculate accuracy on the test data ---\n",
        "predictions = np.argmax(loss_activation.output, axis=1)\n",
        "if len(y_test.shape) == 2:\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "\n",
        "print(f'Validation Accuracy: {accuracy:.3f}, Loss: {loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXQbhKAhMttM",
        "outputId": "946cbbda-4e3b-4f9e-93ad-da8e1dd1f28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.830, Loss: 1.056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Hyperparameter tuning (Learning rate)</b></span></h1>"
      ],
      "metadata": {
        "id": "Md67FgfrPFJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume 'spiral_data' is the function from your course that creates the dataset\n",
        "# Let's create a large dataset for this example\n",
        "X_full, y_full = spiral_data(samples=1000, classes=3)\n",
        "\n",
        "# --- Step 1: Split the full dataset to create a Test Set (20%) ---\n",
        "# The test set is now locked away and will not be touched until the very end.\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Step 2: Split the remaining data to create a Validation Set (25% of the 80%) ---\n",
        "# This creates a 60% train, 20% validation, 20% test split of the original data.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Full dataset shape: {X_full.shape}\")\n",
        "print(\"---\")\n",
        "print(f\"Training set shape:   {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}\")\n",
        "print(f\"Test set shape:       {X_test.shape}\")\n",
        "\n",
        "# Now you would:\n",
        "# 1. Train multiple models with different hyperparameters using (X_train, y_train).\n",
        "# 2. Evaluate each one on (X_val, y_val) to pick the best model.\n",
        "# 3. Train your single best model on the combined (X_train_val, y_train_val).\n",
        "# 4. Perform a final, one-time evaluation on (X_test, y_test).\n",
        "\n",
        "# Example: Try different learning rates\n",
        "learning_rates = [0.01, 0.05, 0.1]\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    # Reinitialize model\n",
        "    dense1 = Layer_Dense(2, 64)\n",
        "    activation1 = Activation_ReLU()\n",
        "    dense2 = Layer_Dense(64, 3)\n",
        "    loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "    optimizer = Optimizer_Adam(learning_rate=lr)\n",
        "\n",
        "    # Training loop (shorter for validation runs)\n",
        "    for epoch in range(1001):\n",
        "        # Forward pass\n",
        "        dense1.forward(X_train)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        loss = loss_activation.forward(dense2.output, y_train)\n",
        "\n",
        "        # Accuracy\n",
        "        predictions = np.argmax(loss_activation.output, axis=1)\n",
        "        if len(y_train.shape) == 2:\n",
        "            y_train_eval = np.argmax(y_train, axis=1)\n",
        "        else:\n",
        "            y_train_eval = y_train\n",
        "        acc = np.mean(predictions == y_train_eval)\n",
        "\n",
        "        # Backward pass\n",
        "        loss_activation.backward(loss_activation.output, y_train)\n",
        "        dense2.backward(loss_activation.dinputs)\n",
        "        activation1.backward(dense2.dinputs)\n",
        "        dense1.backward(activation1.dinputs)\n",
        "\n",
        "        # Update params\n",
        "        optimizer.pre_update_params()\n",
        "        optimizer.update_params(dense1)\n",
        "        optimizer.update_params(dense2)\n",
        "        optimizer.post_update_params()\n",
        "\n",
        "    # After training, evaluate on validation set\n",
        "    dense1.forward(X_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    val_loss = loss_activation.forward(dense2.output, y_val)\n",
        "\n",
        "    val_predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y_val.shape) == 2:\n",
        "        y_val_eval = np.argmax(y_val, axis=1)\n",
        "    else:\n",
        "        y_val_eval = y_val\n",
        "    val_acc = np.mean(val_predictions == y_val_eval)\n",
        "\n",
        "    results.append((lr, val_loss, val_acc))\n",
        "    print(f\"LR={lr} → Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Pick the best model (based on validation accuracy) ---\n",
        "best_lr, best_val_loss, best_val_acc = max(results, key=lambda x: x[2])\n",
        "print(\"\\nBest hyperparameter: LR =\", best_lr)\n",
        "\n",
        "\n",
        "# --- Step 3: Retrain the best model on TRAIN+VAL (X_train_val, y_train_val) ---\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "optimizer = Optimizer_Adam(learning_rate=best_lr)\n",
        "\n",
        "for epoch in range(3001):  # longer training since this is the final model\n",
        "    dense1.forward(X_train_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    loss = loss_activation.forward(dense2.output, y_train_val)\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y_train_val)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n",
        "\n",
        "    if not epoch % 500:\n",
        "        preds = np.argmax(loss_activation.output, axis=1)\n",
        "        y_eval = np.argmax(y_train_val, axis=1) if len(y_train_val.shape) == 2 else y_train_val\n",
        "        acc = np.mean(preds == y_eval)\n",
        "        print(f\"Epoch {epoch} → Loss: {loss:.3f}, Acc: {acc:.3f}\")\n",
        "\n",
        "\n",
        "# --- Step 4: Final one-time evaluation on TEST set ---\n",
        "dense1.forward(X_test)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "test_loss = loss_activation.forward(dense2.output, y_test)\n",
        "\n",
        "test_preds = np.argmax(loss_activation.output, axis=1)\n",
        "y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test\n",
        "test_acc = np.mean(test_preds == y_test_eval)\n",
        "\n",
        "print(\"\\n--- Final Test Evaluation ---\")\n",
        "print(f\"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFu0s3R4PGVH",
        "outputId": "5c5ce11e-595a-401d-bec4-96dd9a68c227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset shape: (3000, 2)\n",
            "---\n",
            "Training set shape:   (1800, 2)\n",
            "Validation set shape: (600, 2)\n",
            "Test set shape:       (600, 2)\n",
            "LR=0.01 → Val Loss: 0.589, Val Acc: 0.763\n",
            "LR=0.05 → Val Loss: 0.494, Val Acc: 0.795\n",
            "LR=0.1 → Val Loss: 0.326, Val Acc: 0.878\n",
            "\n",
            "Best hyperparameter: LR = 0.1\n",
            "Epoch 0 → Loss: 1.099, Acc: 0.305\n",
            "Epoch 500 → Loss: 0.289, Acc: 0.881\n",
            "Epoch 1000 → Loss: 0.251, Acc: 0.895\n",
            "Epoch 1500 → Loss: 0.244, Acc: 0.895\n",
            "Epoch 2000 → Loss: 0.234, Acc: 0.897\n",
            "Epoch 2500 → Loss: 0.224, Acc: 0.901\n",
            "Epoch 3000 → Loss: 0.220, Acc: 0.902\n",
            "\n",
            "--- Final Test Evaluation ---\n",
            "Test Loss: 0.310, Test Accuracy: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>L1/L2 Regularization</b></span></h1>"
      ],
      "metadata": {
        "id": "tuX1u-UR62PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer_Dense:\n",
        "    \"\"\"\n",
        "    A dense (fully-connected) layer with L1 and L2 regularization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "        # Set regularization strength for L1 and L2 penalties\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Store inputs for use in the backward pass\n",
        "        self.inputs = inputs\n",
        "        # Calculate the output of the layer\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # --- Add regularization gradients ---\n",
        "\n",
        "        # L1 regularization on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            # Create a matrix of 1s with the same shape as weights\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            # Set -1 for weights that are negative\n",
        "            dL1[self.weights < 0] = -1\n",
        "            # Add the L1 gradient to the original weight gradients\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "\n",
        "        # L2 regularization on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            # Add the L2 gradient (2 * lambda * w) to the original weight gradients\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "\n",
        "        # L1 regularization on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "\n",
        "        # L2 regularization on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "        # Gradient on inputs (to be passed to the previous layer)\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "XO2EYJxX623P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Base Loss class\n",
        "class Loss:\n",
        "    # Calculates the regularization loss from all trainable layers\n",
        "    def regularization_loss(self, layer):\n",
        "        regularization_loss = 0\n",
        "        # L1 regularization - weights\n",
        "        if layer.weight_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.weights))\n",
        "        # L2 regularization - weights\n",
        "        if layer.weight_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "                                   np.sum(layer.weights * layer.weights)\n",
        "        # L1 regularization - biases\n",
        "        if layer.bias_regularizer_l1 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "                                   np.sum(np.abs(layer.biases))\n",
        "        # L2 regularization - biases\n",
        "        if layer.bias_regularizer_l2 > 0:\n",
        "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "                                   np.sum(layer.biases * layer.biases)\n",
        "        return regularization_loss\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y):\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "        # Return loss\n",
        "        return data_loss"
      ],
      "metadata": {
        "id": "qhQ9X-oC7Lq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values –\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "        # Mask values – only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "        # --- NEW BACKWARD METHOD ---\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # dvalues are the predicted probabilities from Softmax\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse (integers), turn them into a one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "ntJv9RJC_HoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Training Script ---\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# --- Create Model ---\n",
        "# NEW: Add L2 regularization strength to the dense layers\n",
        "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(64, 3, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "    # --- Forward Pass ---\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # NEW: Calculate the data loss\n",
        "    data_loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # NEW: Calculate the regularization loss\n",
        "    reg_loss = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "               loss_activation.loss.regularization_loss(dense2)\n",
        "\n",
        "    # NEW: Combine data and regularization loss\n",
        "    loss = data_loss + reg_loss\n",
        "\n",
        "    # --- Calculate Accuracy ---\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy, loss, and learning rate every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' + # NEW: Print total loss\n",
        "              f'data_loss: {data_loss:.3f}, ' + # NEW: Print data loss\n",
        "              f'reg_loss: {reg_loss:.3f}), ' + # NEW: Print regularization loss\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # --- Backward Pass ---\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # --- Update Parameters ---\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dryhkd_a9tVZ",
        "outputId": "8ba3ad10-7775-4c6b-d231-d5b2d7fab3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.330, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.05\n",
            "epoch: 100, acc: 0.640, loss: 0.938 (data_loss: 0.841, reg_loss: 0.097), lr: 0.04999752512250644\n",
            "epoch: 200, acc: 0.707, loss: 0.907 (data_loss: 0.749, reg_loss: 0.157), lr: 0.04999502549496326\n",
            "epoch: 300, acc: 0.753, loss: 0.902 (data_loss: 0.718, reg_loss: 0.184), lr: 0.049992526117345455\n",
            "epoch: 400, acc: 0.760, loss: 0.896 (data_loss: 0.699, reg_loss: 0.197), lr: 0.04999002698961558\n",
            "epoch: 500, acc: 0.760, loss: 0.896 (data_loss: 0.693, reg_loss: 0.203), lr: 0.049987528111736124\n",
            "epoch: 600, acc: 0.743, loss: 0.900 (data_loss: 0.693, reg_loss: 0.207), lr: 0.049985029483669646\n",
            "epoch: 700, acc: 0.753, loss: 0.890 (data_loss: 0.676, reg_loss: 0.214), lr: 0.049982531105378675\n",
            "epoch: 800, acc: 0.780, loss: 0.885 (data_loss: 0.662, reg_loss: 0.223), lr: 0.04998003297682575\n",
            "epoch: 900, acc: 0.797, loss: 0.883 (data_loss: 0.648, reg_loss: 0.235), lr: 0.049977535097973466\n",
            "epoch: 1000, acc: 0.783, loss: 0.888 (data_loss: 0.647, reg_loss: 0.241), lr: 0.049975037468784345\n",
            "epoch: 1100, acc: 0.807, loss: 0.879 (data_loss: 0.634, reg_loss: 0.245), lr: 0.049972540089220974\n",
            "epoch: 1200, acc: 0.817, loss: 0.877 (data_loss: 0.630, reg_loss: 0.247), lr: 0.04997004295924593\n",
            "epoch: 1300, acc: 0.823, loss: 0.879 (data_loss: 0.630, reg_loss: 0.249), lr: 0.04996754607882181\n",
            "epoch: 1400, acc: 0.803, loss: 0.877 (data_loss: 0.627, reg_loss: 0.250), lr: 0.049965049447911185\n",
            "epoch: 1500, acc: 0.807, loss: 0.877 (data_loss: 0.625, reg_loss: 0.252), lr: 0.04996255306647668\n",
            "epoch: 1600, acc: 0.800, loss: 0.876 (data_loss: 0.623, reg_loss: 0.252), lr: 0.049960056934480884\n",
            "epoch: 1700, acc: 0.823, loss: 0.876 (data_loss: 0.623, reg_loss: 0.253), lr: 0.04995756105188642\n",
            "epoch: 1800, acc: 0.823, loss: 0.878 (data_loss: 0.624, reg_loss: 0.253), lr: 0.049955065418655915\n",
            "epoch: 1900, acc: 0.817, loss: 0.877 (data_loss: 0.623, reg_loss: 0.254), lr: 0.04995257003475201\n",
            "epoch: 2000, acc: 0.827, loss: 0.876 (data_loss: 0.622, reg_loss: 0.254), lr: 0.04995007490013731\n",
            "epoch: 2100, acc: 0.813, loss: 0.875 (data_loss: 0.621, reg_loss: 0.254), lr: 0.0499475800147745\n",
            "epoch: 2200, acc: 0.813, loss: 0.876 (data_loss: 0.621, reg_loss: 0.254), lr: 0.0499450853786262\n",
            "epoch: 2300, acc: 0.813, loss: 0.875 (data_loss: 0.621, reg_loss: 0.254), lr: 0.0499425909916551\n",
            "epoch: 2400, acc: 0.820, loss: 0.876 (data_loss: 0.621, reg_loss: 0.254), lr: 0.04994009685382384\n",
            "epoch: 2500, acc: 0.827, loss: 0.879 (data_loss: 0.625, reg_loss: 0.255), lr: 0.04993760296509512\n",
            "epoch: 2600, acc: 0.807, loss: 0.875 (data_loss: 0.620, reg_loss: 0.255), lr: 0.049935109325431604\n",
            "epoch: 2700, acc: 0.803, loss: 0.875 (data_loss: 0.620, reg_loss: 0.255), lr: 0.049932615934796004\n",
            "epoch: 2800, acc: 0.823, loss: 0.876 (data_loss: 0.621, reg_loss: 0.255), lr: 0.04993012279315098\n",
            "epoch: 2900, acc: 0.817, loss: 0.876 (data_loss: 0.621, reg_loss: 0.255), lr: 0.049927629900459285\n",
            "epoch: 3000, acc: 0.813, loss: 0.875 (data_loss: 0.620, reg_loss: 0.255), lr: 0.049925137256683606\n",
            "epoch: 3100, acc: 0.813, loss: 0.876 (data_loss: 0.621, reg_loss: 0.255), lr: 0.04992264486178666\n",
            "epoch: 3200, acc: 0.810, loss: 0.879 (data_loss: 0.624, reg_loss: 0.256), lr: 0.04992015271573119\n",
            "epoch: 3300, acc: 0.813, loss: 0.876 (data_loss: 0.621, reg_loss: 0.256), lr: 0.04991766081847992\n",
            "epoch: 3400, acc: 0.813, loss: 0.874 (data_loss: 0.619, reg_loss: 0.255), lr: 0.049915169169995596\n",
            "epoch: 3500, acc: 0.837, loss: 0.875 (data_loss: 0.620, reg_loss: 0.255), lr: 0.049912677770240964\n",
            "epoch: 3600, acc: 0.813, loss: 0.875 (data_loss: 0.619, reg_loss: 0.256), lr: 0.049910186619178794\n",
            "epoch: 3700, acc: 0.797, loss: 0.882 (data_loss: 0.626, reg_loss: 0.256), lr: 0.04990769571677183\n",
            "epoch: 3800, acc: 0.827, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04990520506298287\n",
            "epoch: 3900, acc: 0.843, loss: 0.877 (data_loss: 0.621, reg_loss: 0.256), lr: 0.04990271465777467\n",
            "epoch: 4000, acc: 0.813, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.049900224501110035\n",
            "epoch: 4100, acc: 0.827, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04989773459295174\n",
            "epoch: 4200, acc: 0.820, loss: 0.876 (data_loss: 0.619, reg_loss: 0.256), lr: 0.04989524493326262\n",
            "epoch: 4300, acc: 0.823, loss: 0.877 (data_loss: 0.621, reg_loss: 0.256), lr: 0.04989275552200545\n",
            "epoch: 4400, acc: 0.817, loss: 0.875 (data_loss: 0.619, reg_loss: 0.256), lr: 0.04989026635914307\n",
            "epoch: 4500, acc: 0.830, loss: 0.875 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04988777744463829\n",
            "epoch: 4600, acc: 0.830, loss: 0.875 (data_loss: 0.619, reg_loss: 0.256), lr: 0.049885288778453954\n",
            "epoch: 4700, acc: 0.827, loss: 0.875 (data_loss: 0.618, reg_loss: 0.257), lr: 0.049882800360552884\n",
            "epoch: 4800, acc: 0.820, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.04988031219089794\n",
            "epoch: 4900, acc: 0.820, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.049877824269451976\n",
            "epoch: 5000, acc: 0.823, loss: 0.878 (data_loss: 0.621, reg_loss: 0.258), lr: 0.04987533659617785\n",
            "epoch: 5100, acc: 0.833, loss: 0.875 (data_loss: 0.617, reg_loss: 0.258), lr: 0.04987284917103844\n",
            "epoch: 5200, acc: 0.837, loss: 0.874 (data_loss: 0.616, reg_loss: 0.257), lr: 0.04987036199399661\n",
            "epoch: 5300, acc: 0.820, loss: 0.878 (data_loss: 0.621, reg_loss: 0.258), lr: 0.04986787506501525\n",
            "epoch: 5400, acc: 0.843, loss: 0.876 (data_loss: 0.618, reg_loss: 0.257), lr: 0.04986538838405724\n",
            "epoch: 5500, acc: 0.837, loss: 0.875 (data_loss: 0.617, reg_loss: 0.257), lr: 0.049862901951085496\n",
            "epoch: 5600, acc: 0.787, loss: 0.884 (data_loss: 0.627, reg_loss: 0.257), lr: 0.049860415766062906\n",
            "epoch: 5700, acc: 0.837, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.0498579298289524\n",
            "epoch: 5800, acc: 0.837, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.04985544413971689\n",
            "epoch: 5900, acc: 0.820, loss: 0.874 (data_loss: 0.616, reg_loss: 0.257), lr: 0.049852958698319315\n",
            "epoch: 6000, acc: 0.830, loss: 0.876 (data_loss: 0.619, reg_loss: 0.257), lr: 0.04985047350472258\n",
            "epoch: 6100, acc: 0.830, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.04984798855888967\n",
            "epoch: 6200, acc: 0.840, loss: 0.875 (data_loss: 0.618, reg_loss: 0.257), lr: 0.049845503860783506\n",
            "epoch: 6300, acc: 0.827, loss: 0.884 (data_loss: 0.627, reg_loss: 0.257), lr: 0.049843019410367055\n",
            "epoch: 6400, acc: 0.837, loss: 0.876 (data_loss: 0.620, reg_loss: 0.257), lr: 0.04984053520760327\n",
            "epoch: 6500, acc: 0.823, loss: 0.876 (data_loss: 0.619, reg_loss: 0.257), lr: 0.049838051252455155\n",
            "epoch: 6600, acc: 0.843, loss: 0.878 (data_loss: 0.621, reg_loss: 0.257), lr: 0.049835567544885655\n",
            "epoch: 6700, acc: 0.820, loss: 0.875 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04983308408485778\n",
            "epoch: 6800, acc: 0.823, loss: 0.875 (data_loss: 0.618, reg_loss: 0.256), lr: 0.0498306008723345\n",
            "epoch: 6900, acc: 0.823, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.04982811790727884\n",
            "epoch: 7000, acc: 0.817, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.04982563518965381\n",
            "epoch: 7100, acc: 0.823, loss: 0.875 (data_loss: 0.619, reg_loss: 0.257), lr: 0.049823152719422406\n",
            "epoch: 7200, acc: 0.837, loss: 0.879 (data_loss: 0.622, reg_loss: 0.256), lr: 0.049820670496547675\n",
            "epoch: 7300, acc: 0.840, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04981818852099264\n",
            "epoch: 7400, acc: 0.803, loss: 0.876 (data_loss: 0.619, reg_loss: 0.257), lr: 0.049815706792720335\n",
            "epoch: 7500, acc: 0.827, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.0498132253116938\n",
            "epoch: 7600, acc: 0.833, loss: 0.874 (data_loss: 0.617, reg_loss: 0.256), lr: 0.04981074407787611\n",
            "epoch: 7700, acc: 0.813, loss: 0.876 (data_loss: 0.620, reg_loss: 0.257), lr: 0.049808263091230306\n",
            "epoch: 7800, acc: 0.827, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04980578235171948\n",
            "epoch: 7900, acc: 0.813, loss: 0.876 (data_loss: 0.620, reg_loss: 0.256), lr: 0.04980330185930667\n",
            "epoch: 8000, acc: 0.827, loss: 0.875 (data_loss: 0.618, reg_loss: 0.257), lr: 0.04980082161395499\n",
            "epoch: 8100, acc: 0.827, loss: 0.873 (data_loss: 0.617, reg_loss: 0.256), lr: 0.04979834161562752\n",
            "epoch: 8200, acc: 0.840, loss: 0.875 (data_loss: 0.619, reg_loss: 0.257), lr: 0.04979586186428736\n",
            "epoch: 8300, acc: 0.830, loss: 0.874 (data_loss: 0.617, reg_loss: 0.256), lr: 0.04979338235989761\n",
            "epoch: 8400, acc: 0.807, loss: 0.875 (data_loss: 0.618, reg_loss: 0.257), lr: 0.04979090310242139\n",
            "epoch: 8500, acc: 0.837, loss: 0.875 (data_loss: 0.619, reg_loss: 0.256), lr: 0.049788424091821805\n",
            "epoch: 8600, acc: 0.827, loss: 0.875 (data_loss: 0.618, reg_loss: 0.256), lr: 0.049785945328062006\n",
            "epoch: 8700, acc: 0.830, loss: 0.874 (data_loss: 0.617, reg_loss: 0.257), lr: 0.0497834668111051\n",
            "epoch: 8800, acc: 0.810, loss: 0.876 (data_loss: 0.619, reg_loss: 0.257), lr: 0.049780988540914256\n",
            "epoch: 8900, acc: 0.817, loss: 0.874 (data_loss: 0.617, reg_loss: 0.256), lr: 0.0497785105174526\n",
            "epoch: 9000, acc: 0.837, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04977603274068329\n",
            "epoch: 9100, acc: 0.817, loss: 0.875 (data_loss: 0.619, reg_loss: 0.257), lr: 0.04977355521056952\n",
            "epoch: 9200, acc: 0.823, loss: 0.874 (data_loss: 0.619, reg_loss: 0.256), lr: 0.049771077927074414\n",
            "epoch: 9300, acc: 0.823, loss: 0.873 (data_loss: 0.617, reg_loss: 0.256), lr: 0.0497686008901612\n",
            "epoch: 9400, acc: 0.830, loss: 0.873 (data_loss: 0.617, reg_loss: 0.256), lr: 0.04976612409979302\n",
            "epoch: 9500, acc: 0.820, loss: 0.874 (data_loss: 0.617, reg_loss: 0.256), lr: 0.0497636475559331\n",
            "epoch: 9600, acc: 0.840, loss: 0.876 (data_loss: 0.619, reg_loss: 0.257), lr: 0.049761171258544616\n",
            "epoch: 9700, acc: 0.823, loss: 0.875 (data_loss: 0.618, reg_loss: 0.256), lr: 0.0497586952075908\n",
            "epoch: 9800, acc: 0.833, loss: 0.874 (data_loss: 0.617, reg_loss: 0.256), lr: 0.04975621940303483\n",
            "epoch: 9900, acc: 0.837, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.049753743844839965\n",
            "epoch: 10000, acc: 0.817, loss: 0.874 (data_loss: 0.618, reg_loss: 0.256), lr: 0.04975126853296942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Hyperparameter tuning</b></span></h1>"
      ],
      "metadata": {
        "id": "5TdFc9mhA2xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Dataset ---\n",
        "X_full, y_full = spiral_data(samples=1000, classes=3)\n",
        "\n",
        "# --- Step 1: Split into Train / Val / Test ---\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
        ")  # 60% / 20% / 20%\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# --- Helper: training function ---\n",
        "def train_model(X, y, X_val, y_val,\n",
        "                learning_rate=0.01,\n",
        "                l2_strength=0.0,\n",
        "                hidden_neurons=64,\n",
        "                epochs=1000):\n",
        "\n",
        "    # Build model\n",
        "    dense1 = Layer_Dense(2, hidden_neurons,\n",
        "                         weight_regularizer_l2=l2_strength,\n",
        "                         bias_regularizer_l2=l2_strength)\n",
        "    activation1 = Activation_ReLU()\n",
        "    dense2 = Layer_Dense(hidden_neurons, 3,\n",
        "                         weight_regularizer_l2=l2_strength,\n",
        "                         bias_regularizer_l2=l2_strength)\n",
        "    loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "    optimizer = Optimizer_Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        dense1.forward(X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss_activation.backward(loss_activation.output, y)\n",
        "        dense2.backward(loss_activation.dinputs)\n",
        "        activation1.backward(dense2.dinputs)\n",
        "        dense1.backward(activation1.dinputs)\n",
        "\n",
        "        # Update\n",
        "        optimizer.pre_update_params()\n",
        "        optimizer.update_params(dense1)\n",
        "        optimizer.update_params(dense2)\n",
        "        optimizer.post_update_params()\n",
        "\n",
        "    # Validation\n",
        "    dense1.forward(X_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    val_loss = loss_activation.forward(dense2.output, y_val)\n",
        "\n",
        "    val_preds = np.argmax(loss_activation.output, axis=1)\n",
        "    y_val_eval = np.argmax(y_val, axis=1) if len(y_val.shape) == 2 else y_val\n",
        "    val_acc = np.mean(val_preds == y_val_eval)\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "\n",
        "# --- Step 2: Try multiple hyperparameters ---\n",
        "param_grid = [\n",
        "    {\"learning_rate\": 0.01, \"l2_strength\": 1e-4, \"hidden_neurons\": 32},\n",
        "    {\"learning_rate\": 0.05, \"l2_strength\": 5e-4, \"hidden_neurons\": 64},\n",
        "    {\"learning_rate\": 0.1, \"l2_strength\": 1e-3, \"hidden_neurons\": 128},\n",
        "]\n",
        "\n",
        "results = []\n",
        "for params in param_grid:\n",
        "    print(f\"Testing {params}\")\n",
        "    val_loss, val_acc = train_model(X_train, y_train, X_val, y_val,\n",
        "                                    learning_rate=params[\"learning_rate\"],\n",
        "                                    l2_strength=params[\"l2_strength\"],\n",
        "                                    hidden_neurons=params[\"hidden_neurons\"],\n",
        "                                    epochs=1000)\n",
        "    results.append((params, val_loss, val_acc))\n",
        "    print(f\"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\\n\")\n",
        "\n",
        "\n",
        "# --- Step 3: Pick best hyperparameters ---\n",
        "best_params, best_val_loss, best_val_acc = max(results, key=lambda x: x[2])\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# --- Step 4: Retrain best model on TRAIN+VAL ---\n",
        "print(\"\\nRetraining best model on TRAIN+VAL...\")\n",
        "train_model(X_train_val, y_train_val, X_val, y_val,\n",
        "            learning_rate=best_params[\"learning_rate\"],\n",
        "            l2_strength=best_params[\"l2_strength\"],\n",
        "            hidden_neurons=best_params[\"hidden_neurons\"],\n",
        "            epochs=3000)\n",
        "\n",
        "# Final evaluation on Test\n",
        "dense1 = Layer_Dense(2, best_params[\"hidden_neurons\"],\n",
        "                     weight_regularizer_l2=best_params[\"l2_strength\"],\n",
        "                     bias_regularizer_l2=best_params[\"l2_strength\"])\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(best_params[\"hidden_neurons\"], 3,\n",
        "                     weight_regularizer_l2=best_params[\"l2_strength\"],\n",
        "                     bias_regularizer_l2=best_params[\"l2_strength\"])\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "optimizer = Optimizer_Adam(learning_rate=best_params[\"learning_rate\"])\n",
        "\n",
        "# Train final model on train+val\n",
        "for epoch in range(3000):\n",
        "    dense1.forward(X_train_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    loss = loss_activation.forward(dense2.output, y_train_val)\n",
        "\n",
        "    loss_activation.backward(loss_activation.output, y_train_val)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n",
        "\n",
        "# Test evaluation\n",
        "dense1.forward(X_test)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "test_loss = loss_activation.forward(dense2.output, y_test)\n",
        "\n",
        "test_preds = np.argmax(loss_activation.output, axis=1)\n",
        "y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test\n",
        "test_acc = np.mean(test_preds == y_test_eval)\n",
        "\n",
        "print(\"\\n--- Final Test Evaluation ---\")\n",
        "print(f\"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onzMAA94A2oB",
        "outputId": "e0f102a9-c6be-40e6-8f84-53430d91e743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (1800, 2), Val: (600, 2), Test: (600, 2)\n",
            "Testing {'learning_rate': 0.01, 'l2_strength': 0.0001, 'hidden_neurons': 32}\n",
            "Val Loss: 0.613, Val Acc: 0.733\n",
            "\n",
            "Testing {'learning_rate': 0.05, 'l2_strength': 0.0005, 'hidden_neurons': 64}\n",
            "Val Loss: 0.705, Val Acc: 0.765\n",
            "\n",
            "Testing {'learning_rate': 0.1, 'l2_strength': 0.001, 'hidden_neurons': 128}\n",
            "Val Loss: 0.941, Val Acc: 0.585\n",
            "\n",
            "Best Hyperparameters: {'learning_rate': 0.05, 'l2_strength': 0.0005, 'hidden_neurons': 64}\n",
            "\n",
            "Retraining best model on TRAIN+VAL...\n",
            "\n",
            "--- Final Test Evaluation ---\n",
            "Test Loss: 0.717, Test Accuracy: 0.760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Dataset ---\n",
        "X_full, y_full = spiral_data(samples=1000, classes=3)\n",
        "\n",
        "# --- Train/Val/Test Split ---\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Full dataset shape: {X_full.shape}\")\n",
        "print(\"---\")\n",
        "print(f\"Training set shape:   {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}\")\n",
        "print(f\"Test set shape:       {X_test.shape}\")\n",
        "\n",
        "# --- Hyperparameters to tune ---\n",
        "learning_rates = [0.01, 0.05, 0.1]\n",
        "l2_strengths = [0.0, 1e-4, 1e-3, 1e-2]\n",
        "\n",
        "results = []\n",
        "\n",
        "# --- Hyperparameter Search ---\n",
        "for lr in learning_rates:\n",
        "    for l2 in l2_strengths:\n",
        "        # Model\n",
        "        dense1 = Layer_Dense(2, 64, weight_regularizer_l2=l2, bias_regularizer_l2=l2)\n",
        "        activation1 = Activation_ReLU()\n",
        "        dense2 = Layer_Dense(64, 3, weight_regularizer_l2=l2, bias_regularizer_l2=l2)\n",
        "        loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "        optimizer = Optimizer_Adam(learning_rate=lr)\n",
        "\n",
        "        # Training loop (shorter for validation runs)\n",
        "        for epoch in range(1001):\n",
        "            # Forward\n",
        "            dense1.forward(X_train)\n",
        "            activation1.forward(dense1.output)\n",
        "            dense2.forward(activation1.output)\n",
        "            data_loss = loss_activation.forward(dense2.output, y_train)\n",
        "            reg_loss = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "                       loss_activation.loss.regularization_loss(dense2)\n",
        "            loss = data_loss + reg_loss\n",
        "\n",
        "            # Backward\n",
        "            loss_activation.backward(loss_activation.output, y_train)\n",
        "            dense2.backward(loss_activation.dinputs)\n",
        "            activation1.backward(dense2.dinputs)\n",
        "            dense1.backward(activation1.dinputs)\n",
        "\n",
        "            optimizer.pre_update_params()\n",
        "            optimizer.update_params(dense1)\n",
        "            optimizer.update_params(dense2)\n",
        "            optimizer.post_update_params()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        dense1.forward(X_val)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        val_data_loss = loss_activation.forward(dense2.output, y_val)\n",
        "        val_reg_loss = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "                       loss_activation.loss.regularization_loss(dense2)\n",
        "        val_loss = val_data_loss + val_reg_loss\n",
        "\n",
        "        val_predictions = np.argmax(loss_activation.output, axis=1)\n",
        "        y_val_eval = np.argmax(y_val, axis=1) if len(y_val.shape) == 2 else y_val\n",
        "        val_acc = np.mean(val_predictions == y_val_eval)\n",
        "\n",
        "        results.append((lr, l2, val_loss, val_acc))\n",
        "        print(f\"LR={lr}, L2={l2} → Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Pick best (by validation acc) ---\n",
        "best_lr, best_l2, best_val_loss, best_val_acc = max(results, key=lambda x: x[3])\n",
        "print(\"\\nBest hyperparameters → LR =\", best_lr, \"L2 =\", best_l2)\n",
        "\n",
        "# --- Step 3: Retrain best model on TRAIN+VAL ---\n",
        "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(64, 3, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "optimizer = Optimizer_Adam(learning_rate=best_lr)\n",
        "\n",
        "for epoch in range(3001):\n",
        "    dense1.forward(X_train_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    data_loss = loss_activation.forward(dense2.output, y_train_val)\n",
        "    reg_loss = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "               loss_activation.loss.regularization_loss(dense2)\n",
        "    loss = data_loss + reg_loss\n",
        "\n",
        "    loss_activation.backward(loss_activation.output, y_train_val)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n",
        "\n",
        "    if not epoch % 500:\n",
        "        preds = np.argmax(loss_activation.output, axis=1)\n",
        "        y_eval = np.argmax(y_train_val, axis=1) if len(y_train_val.shape) == 2 else y_train_val\n",
        "        acc = np.mean(preds == y_eval)\n",
        "        print(f\"Epoch {epoch} → Loss: {loss:.3f}, Acc: {acc:.3f}\")\n",
        "\n",
        "\n",
        "# --- Step 4: Final Test Evaluation ---\n",
        "dense1.forward(X_test)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "test_data_loss = loss_activation.forward(dense2.output, y_test)\n",
        "test_reg_loss = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "                loss_activation.loss.regularization_loss(dense2)\n",
        "test_loss = test_data_loss + test_reg_loss\n",
        "\n",
        "test_preds = np.argmax(loss_activation.output, axis=1)\n",
        "y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test\n",
        "test_acc = np.mean(test_preds == y_test_eval)\n",
        "\n",
        "print(\"\\n--- Final Test Evaluation ---\")\n",
        "print(f\"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFu6_aefBeZP",
        "outputId": "0d658034-1929-483a-a15f-f854402e075e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset shape: (3000, 2)\n",
            "---\n",
            "Training set shape:   (1800, 2)\n",
            "Validation set shape: (600, 2)\n",
            "Test set shape:       (600, 2)\n",
            "LR=0.01, L2=0.0 → Val Loss: 0.454, Val Acc: 0.817\n",
            "LR=0.01, L2=0.0001 → Val Loss: 0.615, Val Acc: 0.812\n",
            "LR=0.01, L2=0.001 → Val Loss: 1.041, Val Acc: 0.583\n",
            "LR=0.01, L2=0.01 → Val Loss: 1.097, Val Acc: 0.372\n",
            "LR=0.05, L2=0.0 → Val Loss: 0.518, Val Acc: 0.773\n",
            "LR=0.05, L2=0.0001 → Val Loss: 0.607, Val Acc: 0.825\n",
            "LR=0.05, L2=0.001 → Val Loss: 1.041, Val Acc: 0.595\n",
            "LR=0.05, L2=0.01 → Val Loss: 1.098, Val Acc: 0.373\n",
            "LR=0.1, L2=0.0 → Val Loss: 0.340, Val Acc: 0.857\n",
            "LR=0.1, L2=0.0001 → Val Loss: 0.593, Val Acc: 0.847\n",
            "LR=0.1, L2=0.001 → Val Loss: 1.041, Val Acc: 0.583\n",
            "LR=0.1, L2=0.01 → Val Loss: 1.098, Val Acc: 0.370\n",
            "\n",
            "Best hyperparameters → LR = 0.1 L2 = 0.0\n",
            "Epoch 0 → Loss: 1.099, Acc: 0.361\n",
            "Epoch 500 → Loss: 0.316, Acc: 0.870\n",
            "Epoch 1000 → Loss: 0.260, Acc: 0.888\n",
            "Epoch 1500 → Loss: 0.254, Acc: 0.895\n",
            "Epoch 2000 → Loss: 0.248, Acc: 0.897\n",
            "Epoch 2500 → Loss: 0.250, Acc: 0.895\n",
            "Epoch 3000 → Loss: 0.242, Acc: 0.900\n",
            "\n",
            "--- Final Test Evaluation ---\n",
            "Test Loss: 0.247, Test Accuracy: 0.892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g30rNpP2D2VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer_Dropout:\n",
        "    # Initialize with the dropout rate\n",
        "    def __init__(self, rate):\n",
        "        # 'rate' is the dropout rate. We store the 'success rate'.\n",
        "        self.rate = 1 - rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # --- The Core Dropout Logic ---\n",
        "        # 1. Create a binary mask using a binomial distribution.\n",
        "        #    This creates an array of 0s and 1s with the same shape as the inputs.\n",
        "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape)\n",
        "\n",
        "        # 2. Apply the mask to the inputs (zeroing out some neurons)\n",
        "        self.output = inputs * self.binary_mask\n",
        "\n",
        "        # 3. Scale the output (Inverted Dropout)\n",
        "        self.output /= self.rate\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # The gradient is also masked.\n",
        "        # Only the neurons that were active during the forward pass\n",
        "        # contribute to the gradient passed backward.\n",
        "        self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "        # The gradient is also scaled by the same factor.\n",
        "        self.dinputs /= self.rate"
      ],
      "metadata": {
        "id": "TVfbrYgpD1-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Training Script ---\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# --- Create Model ---\n",
        "# NEW: Add L2 regularization strength to the dense layers\n",
        "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
        "dropout1 = Layer_Dropout(0.1)  # NEW: Create a Dropout layer with a 10% dropout rate\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(64, 3, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
        "dropout2 = Layer_Dropout(0.1)  # NEW: Create a Dropout layer with a 10% dropout rate\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "    # --- Forward Pass ---\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dropout1.forward(activation1.output)\n",
        "    dense2.forward(dropout1.output)\n",
        "    dropout2.forward(dense2.output)\n",
        "\n",
        "    # NEW: Calculate the data loss\n",
        "    loss_activation.forward(dropout2.output, y)\n",
        "\n",
        "    # NEW: Calculate the regularization loss\n",
        "    reg_loss = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "               loss_activation.loss.regularization_loss(dense2)\n",
        "\n",
        "    # NEW: Combine data and regularization loss\n",
        "    loss = data_loss + reg_loss\n",
        "\n",
        "    # --- Calculate Accuracy ---\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    # Print accuracy, loss, and learning rate every 100 epochs\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f} (' + # NEW: Print total loss\n",
        "              f'data_loss: {data_loss:.3f}, ' + # NEW: Print data loss\n",
        "              f'reg_loss: {reg_loss:.3f}), ' + # NEW: Print regularization loss\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # --- Backward Pass ---\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dropout2.backward(loss_activation.dinputs)\n",
        "    dense2.backward(dropout2.dinputs)\n",
        "    # The gradient from dense2 goes to dropout1 (which came before it)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    # The gradient from dropout1 goes to activation1\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    # The gradient from activation1 goes to dense1\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # --- Update Parameters ---\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3TiG8MPvD7ZP",
        "outputId": "4a2cefa4-49ed-4d74-e8eb-690eb4b67758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.283, loss: 1.008 (data_loss: 1.008, reg_loss: 0.000), lr: 0.05\n",
            "epoch: 100, acc: 0.567, loss: 1.079 (data_loss: 1.008, reg_loss: 0.071), lr: 0.04999752512250644\n",
            "epoch: 200, acc: 0.563, loss: 1.106 (data_loss: 1.008, reg_loss: 0.098), lr: 0.04999502549496326\n",
            "epoch: 300, acc: 0.590, loss: 1.119 (data_loss: 1.008, reg_loss: 0.111), lr: 0.049992526117345455\n",
            "epoch: 400, acc: 0.577, loss: 1.122 (data_loss: 1.008, reg_loss: 0.114), lr: 0.04999002698961558\n",
            "epoch: 500, acc: 0.590, loss: 1.126 (data_loss: 1.008, reg_loss: 0.118), lr: 0.049987528111736124\n",
            "epoch: 600, acc: 0.600, loss: 1.125 (data_loss: 1.008, reg_loss: 0.117), lr: 0.049985029483669646\n",
            "epoch: 700, acc: 0.580, loss: 1.126 (data_loss: 1.008, reg_loss: 0.118), lr: 0.049982531105378675\n",
            "epoch: 800, acc: 0.633, loss: 1.126 (data_loss: 1.008, reg_loss: 0.118), lr: 0.04998003297682575\n",
            "epoch: 900, acc: 0.597, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.049977535097973466\n",
            "epoch: 1000, acc: 0.590, loss: 1.129 (data_loss: 1.008, reg_loss: 0.121), lr: 0.049975037468784345\n",
            "epoch: 1100, acc: 0.627, loss: 1.128 (data_loss: 1.008, reg_loss: 0.120), lr: 0.049972540089220974\n",
            "epoch: 1200, acc: 0.587, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.04997004295924593\n",
            "epoch: 1300, acc: 0.603, loss: 1.129 (data_loss: 1.008, reg_loss: 0.121), lr: 0.04996754607882181\n",
            "epoch: 1400, acc: 0.627, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.049965049447911185\n",
            "epoch: 1500, acc: 0.613, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.04996255306647668\n",
            "epoch: 1600, acc: 0.607, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.049960056934480884\n",
            "epoch: 1700, acc: 0.563, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.04995756105188642\n",
            "epoch: 1800, acc: 0.613, loss: 1.134 (data_loss: 1.008, reg_loss: 0.125), lr: 0.049955065418655915\n",
            "epoch: 1900, acc: 0.613, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.04995257003475201\n",
            "epoch: 2000, acc: 0.553, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.04995007490013731\n",
            "epoch: 2100, acc: 0.603, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.0499475800147745\n",
            "epoch: 2200, acc: 0.590, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.0499450853786262\n",
            "epoch: 2300, acc: 0.590, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.0499425909916551\n",
            "epoch: 2400, acc: 0.607, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.04994009685382384\n",
            "epoch: 2500, acc: 0.573, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.04993760296509512\n",
            "epoch: 2600, acc: 0.590, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.049935109325431604\n",
            "epoch: 2700, acc: 0.593, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.049932615934796004\n",
            "epoch: 2800, acc: 0.580, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04993012279315098\n",
            "epoch: 2900, acc: 0.650, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.049927629900459285\n",
            "epoch: 3000, acc: 0.573, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.049925137256683606\n",
            "epoch: 3100, acc: 0.593, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.04992264486178666\n",
            "epoch: 3200, acc: 0.623, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.04992015271573119\n",
            "epoch: 3300, acc: 0.590, loss: 1.129 (data_loss: 1.008, reg_loss: 0.121), lr: 0.04991766081847992\n",
            "epoch: 3400, acc: 0.593, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.049915169169995596\n",
            "epoch: 3500, acc: 0.597, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.049912677770240964\n",
            "epoch: 3600, acc: 0.633, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.049910186619178794\n",
            "epoch: 3700, acc: 0.600, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.04990769571677183\n",
            "epoch: 3800, acc: 0.620, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.04990520506298287\n",
            "epoch: 3900, acc: 0.593, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04990271465777467\n",
            "epoch: 4000, acc: 0.593, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.049900224501110035\n",
            "epoch: 4100, acc: 0.617, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.04989773459295174\n",
            "epoch: 4200, acc: 0.580, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04989524493326262\n",
            "epoch: 4300, acc: 0.613, loss: 1.138 (data_loss: 1.008, reg_loss: 0.130), lr: 0.04989275552200545\n",
            "epoch: 4400, acc: 0.630, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.04989026635914307\n",
            "epoch: 4500, acc: 0.623, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04988777744463829\n",
            "epoch: 4600, acc: 0.613, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.049885288778453954\n",
            "epoch: 4700, acc: 0.583, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.049882800360552884\n",
            "epoch: 4800, acc: 0.597, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04988031219089794\n",
            "epoch: 4900, acc: 0.570, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.049877824269451976\n",
            "epoch: 5000, acc: 0.650, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.04987533659617785\n",
            "epoch: 5100, acc: 0.600, loss: 1.133 (data_loss: 1.008, reg_loss: 0.124), lr: 0.04987284917103844\n",
            "epoch: 5200, acc: 0.593, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.04987036199399661\n",
            "epoch: 5300, acc: 0.633, loss: 1.132 (data_loss: 1.008, reg_loss: 0.124), lr: 0.04986787506501525\n",
            "epoch: 5400, acc: 0.607, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.04986538838405724\n",
            "epoch: 5500, acc: 0.613, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.049862901951085496\n",
            "epoch: 5600, acc: 0.620, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.049860415766062906\n",
            "epoch: 5700, acc: 0.647, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.0498579298289524\n",
            "epoch: 5800, acc: 0.597, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04985544413971689\n",
            "epoch: 5900, acc: 0.603, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.049852958698319315\n",
            "epoch: 6000, acc: 0.617, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04985047350472258\n",
            "epoch: 6100, acc: 0.647, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.04984798855888967\n",
            "epoch: 6200, acc: 0.590, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.049845503860783506\n",
            "epoch: 6300, acc: 0.543, loss: 1.131 (data_loss: 1.008, reg_loss: 0.123), lr: 0.049843019410367055\n",
            "epoch: 6400, acc: 0.580, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04984053520760327\n",
            "epoch: 6500, acc: 0.633, loss: 1.130 (data_loss: 1.008, reg_loss: 0.122), lr: 0.049838051252455155\n",
            "epoch: 6600, acc: 0.613, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.049835567544885655\n",
            "epoch: 6700, acc: 0.640, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04983308408485778\n",
            "epoch: 6800, acc: 0.580, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.0498306008723345\n",
            "epoch: 6900, acc: 0.597, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04982811790727884\n",
            "epoch: 7000, acc: 0.593, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04982563518965381\n",
            "epoch: 7100, acc: 0.550, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.049823152719422406\n",
            "epoch: 7200, acc: 0.627, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.049820670496547675\n",
            "epoch: 7300, acc: 0.620, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04981818852099264\n",
            "epoch: 7400, acc: 0.593, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.049815706792720335\n",
            "epoch: 7500, acc: 0.597, loss: 1.136 (data_loss: 1.008, reg_loss: 0.128), lr: 0.0498132253116938\n",
            "epoch: 7600, acc: 0.580, loss: 1.134 (data_loss: 1.008, reg_loss: 0.125), lr: 0.04981074407787611\n",
            "epoch: 7700, acc: 0.600, loss: 1.136 (data_loss: 1.008, reg_loss: 0.128), lr: 0.049808263091230306\n",
            "epoch: 7800, acc: 0.637, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04980578235171948\n",
            "epoch: 7900, acc: 0.580, loss: 1.136 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04980330185930667\n",
            "epoch: 8000, acc: 0.633, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04980082161395499\n",
            "epoch: 8100, acc: 0.600, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04979834161562752\n",
            "epoch: 8200, acc: 0.587, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04979586186428736\n",
            "epoch: 8300, acc: 0.633, loss: 1.136 (data_loss: 1.008, reg_loss: 0.128), lr: 0.04979338235989761\n",
            "epoch: 8400, acc: 0.603, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04979090310242139\n",
            "epoch: 8500, acc: 0.660, loss: 1.136 (data_loss: 1.008, reg_loss: 0.128), lr: 0.049788424091821805\n",
            "epoch: 8600, acc: 0.603, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.049785945328062006\n",
            "epoch: 8700, acc: 0.623, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.0497834668111051\n",
            "epoch: 8800, acc: 0.557, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.049780988540914256\n",
            "epoch: 8900, acc: 0.627, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.0497785105174526\n",
            "epoch: 9000, acc: 0.573, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04977603274068329\n",
            "epoch: 9100, acc: 0.573, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04977355521056952\n",
            "epoch: 9200, acc: 0.563, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.049771077927074414\n",
            "epoch: 9300, acc: 0.600, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.0497686008901612\n",
            "epoch: 9400, acc: 0.580, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04976612409979302\n",
            "epoch: 9500, acc: 0.587, loss: 1.135 (data_loss: 1.008, reg_loss: 0.126), lr: 0.0497636475559331\n",
            "epoch: 9600, acc: 0.637, loss: 1.133 (data_loss: 1.008, reg_loss: 0.125), lr: 0.049761171258544616\n",
            "epoch: 9700, acc: 0.627, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.0497586952075908\n",
            "epoch: 9800, acc: 0.607, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.04975621940303483\n",
            "epoch: 9900, acc: 0.650, loss: 1.135 (data_loss: 1.008, reg_loss: 0.127), lr: 0.049753743844839965\n",
            "epoch: 10000, acc: 0.617, loss: 1.134 (data_loss: 1.008, reg_loss: 0.126), lr: 0.04975126853296942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gp-dZmz9FpLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Dataset ---\n",
        "X_full, y_full = spiral_data(samples=1000, classes=3)\n",
        "\n",
        "# --- Train/Val/Test Split (60/20/20) ---\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Full: {X_full.shape} | Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
        "\n",
        "# --- Hyperparameters to tune ---\n",
        "learning_rates = [0.01, 0.05, 0.1]\n",
        "l2_strengths   = [0.0, 1e-4, 1e-3, 1e-2]\n",
        "dropout_rate   = 0.1  # fixed as in your model\n",
        "search_epochs  = 1000\n",
        "final_epochs   = 3000\n",
        "\n",
        "results = []\n",
        "\n",
        "# --- Hyperparameter Search ---\n",
        "for lr in learning_rates:\n",
        "    for l2 in l2_strengths:\n",
        "        # Build model\n",
        "        dense1 = Layer_Dense(2, 64, weight_regularizer_l2=l2, bias_regularizer_l2=l2)\n",
        "        activation1 = Activation_ReLU()\n",
        "        dropout1 = Layer_Dropout(dropout_rate)\n",
        "\n",
        "        dense2 = Layer_Dense(64, 3, weight_regularizer_l2=l2, bias_regularizer_l2=l2)\n",
        "        dropout2 = Layer_Dropout(dropout_rate)\n",
        "\n",
        "        loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "        optimizer = Optimizer_Adam(learning_rate=lr)\n",
        "\n",
        "        # --- Train (with dropout) ---\n",
        "        for epoch in range(search_epochs):\n",
        "            dense1.forward(X_train)\n",
        "            activation1.forward(dense1.output)\n",
        "            dropout1.forward(activation1.output)         # dropout ON (training)\n",
        "            dense2.forward(dropout1.output)\n",
        "            dropout2.forward(dense2.output)              # dropout ON (training)\n",
        "\n",
        "            data_loss = loss_activation.forward(dropout2.output, y_train)\n",
        "            reg_loss  = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "                        loss_activation.loss.regularization_loss(dense2)\n",
        "            loss = data_loss + reg_loss\n",
        "\n",
        "            # Backward\n",
        "            loss_activation.backward(loss_activation.output, y_train)\n",
        "            dropout2.backward(loss_activation.dinputs)\n",
        "            dense2.backward(dropout2.dinputs)\n",
        "            dropout1.backward(dense2.dinputs)\n",
        "            activation1.backward(dropout1.dinputs)\n",
        "            dense1.backward(activation1.dinputs)\n",
        "\n",
        "            optimizer.pre_update_params()\n",
        "            optimizer.update_params(dense1)\n",
        "            optimizer.update_params(dense2)\n",
        "            optimizer.post_update_params()\n",
        "\n",
        "        # --- Validate (NO dropout) ---\n",
        "        dense1.forward(X_val)\n",
        "        activation1.forward(dense1.output)\n",
        "        # skip dropout1\n",
        "        dense2.forward(activation1.output)\n",
        "        # skip dropout2\n",
        "\n",
        "        val_data_loss = loss_activation.forward(dense2.output, y_val)\n",
        "        val_reg_loss  = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "                        loss_activation.loss.regularization_loss(dense2)\n",
        "        val_loss = val_data_loss + val_reg_loss\n",
        "\n",
        "        val_preds   = np.argmax(loss_activation.output, axis=1)\n",
        "        y_val_eval  = np.argmax(y_val, axis=1) if len(y_val.shape) == 2 else y_val\n",
        "        val_acc     = np.mean(val_preds == y_val_eval)\n",
        "\n",
        "        results.append((lr, l2, val_loss, val_acc))\n",
        "        print(f\"LR={lr:>.3g}, L2={l2:>.1e} → Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "# --- Select best by validation accuracy ---\n",
        "best_lr, best_l2, best_val_loss, best_val_acc = max(results, key=lambda x: x[3])\n",
        "print(f\"\\nBest hyperparams → LR={best_lr}, L2={best_l2} | Val Acc={best_val_acc:.3f}\")\n",
        "\n",
        "# --- Retrain best model on TRAIN+VAL (with dropout during training) ---\n",
        "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)\n",
        "activation1 = Activation_ReLU()\n",
        "dropout1 = Layer_Dropout(dropout_rate)\n",
        "\n",
        "dense2 = Layer_Dense(64, 3, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)\n",
        "dropout2 = Layer_Dropout(dropout_rate)\n",
        "\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "optimizer = Optimizer_Adam(learning_rate=best_lr)\n",
        "\n",
        "for epoch in range(final_epochs + 1):\n",
        "    # Train (dropout ON)\n",
        "    dense1.forward(X_train_val)\n",
        "    activation1.forward(dense1.output)\n",
        "    dropout1.forward(activation1.output)\n",
        "    dense2.forward(dropout1.output)\n",
        "    dropout2.forward(dense2.output)\n",
        "\n",
        "    data_loss = loss_activation.forward(dropout2.output, y_train_val)\n",
        "    reg_loss  = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "                loss_activation.loss.regularization_loss(dense2)\n",
        "    loss = data_loss + reg_loss\n",
        "\n",
        "    loss_activation.backward(loss_activation.output, y_train_val)\n",
        "    dropout2.backward(loss_activation.dinputs)\n",
        "    dense2.backward(dropout2.dinputs)\n",
        "    dropout1.backward(dense2.dinputs)\n",
        "    activation1.backward(dropout1.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        preds = np.argmax(loss_activation.output, axis=1)\n",
        "        y_eval = np.argmax(y_train_val, axis=1) if len(y_train_val.shape) == 2 else y_train_val\n",
        "        acc = np.mean(preds == y_eval)\n",
        "        print(f\"Final-train Epoch {epoch} → Loss: {loss:.3f}, Acc: {acc:.3f}\")\n",
        "\n",
        "# --- Final Test (NO dropout) ---\n",
        "dense1.forward(X_test)\n",
        "activation1.forward(dense1.output)\n",
        "# skip dropout1\n",
        "dense2.forward(activation1.output)\n",
        "# skip dropout2\n",
        "\n",
        "test_data_loss = loss_activation.forward(dense2.output, y_test)\n",
        "test_reg_loss  = loss_activation.loss.regularization_loss(dense1) + \\\n",
        "                 loss_activation.loss.regularization_loss(dense2)\n",
        "test_loss = test_data_loss + test_reg_loss\n",
        "\n",
        "test_preds  = np.argmax(loss_activation.output, axis=1)\n",
        "y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test\n",
        "test_acc    = np.mean(test_preds == y_test_eval)\n",
        "\n",
        "print(\"\\n--- Final Test Evaluation ---\")\n",
        "print(f\"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZzaxvD8FpBv",
        "outputId": "2be79360-8802-4ef0-d7c7-d043763ad8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full: (3000, 2) | Train: (1800, 2) | Val: (600, 2) | Test: (600, 2)\n",
            "LR=0.01, L2=0.0e+00 → Val Loss: 0.886, Val Acc: 0.567\n",
            "LR=0.01, L2=1.0e-04 → Val Loss: 0.850, Val Acc: 0.665\n",
            "LR=0.01, L2=1.0e-03 → Val Loss: 1.058, Val Acc: 0.457\n",
            "LR=0.01, L2=1.0e-02 → Val Loss: 1.105, Val Acc: 0.368\n",
            "LR=0.05, L2=0.0e+00 → Val Loss: 0.879, Val Acc: 0.610\n",
            "LR=0.05, L2=1.0e-04 → Val Loss: 0.767, Val Acc: 0.707\n",
            "LR=0.05, L2=1.0e-03 → Val Loss: 1.056, Val Acc: 0.510\n",
            "LR=0.05, L2=1.0e-02 → Val Loss: 1.104, Val Acc: 0.367\n",
            "LR=0.1, L2=0.0e+00 → Val Loss: 0.610, Val Acc: 0.732\n",
            "LR=0.1, L2=1.0e-04 → Val Loss: 0.746, Val Acc: 0.758\n",
            "LR=0.1, L2=1.0e-03 → Val Loss: 1.060, Val Acc: 0.482\n",
            "LR=0.1, L2=1.0e-02 → Val Loss: 1.103, Val Acc: 0.363\n",
            "\n",
            "Best hyperparams → LR=0.1, L2=0.0001 | Val Acc=0.758\n",
            "Final-train Epoch 0 → Loss: 1.099, Acc: 0.364\n",
            "Final-train Epoch 500 → Loss: 0.832, Acc: 0.657\n",
            "Final-train Epoch 1000 → Loss: 0.841, Acc: 0.654\n",
            "Final-train Epoch 1500 → Loss: 0.828, Acc: 0.672\n",
            "Final-train Epoch 2000 → Loss: 0.813, Acc: 0.680\n",
            "Final-train Epoch 2500 → Loss: 0.819, Acc: 0.667\n",
            "Final-train Epoch 3000 → Loss: 0.803, Acc: 0.696\n",
            "\n",
            "--- Final Test Evaluation ---\n",
            "Test Loss: 0.731, Test Accuracy: 0.723\n"
          ]
        }
      ]
    }
  ]
}