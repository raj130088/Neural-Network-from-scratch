# -*- coding: utf-8 -*-
"""nn from scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d_ijHtPey3QN6BWnyx1BrB9nxG0CFyKX

<h1><b>Coding a single neuron</b></span></h1>
"""

# A neuron with four inputs
inputs = [1.0, 2.0, 3.0, 2.5]
weights = [0.2, 0.8, -0.5, 1.0]
bias = 2.0

# Calculate the neuron's output
output = (inputs[0] * weights[0] +
          inputs[1] * weights[1] +
          inputs[2] * weights[2] +
          inputs[3] * weights[3] + bias)

print(output)
# Expected output: 4.8

"""<h1><b>A layer of neuron manually</b></span></h1>

"""

# A layer of 3 neurons, each with 4 inputs
inputs = [1.0, 2.0, 3.0, 2.5]

# Weights for each of the 3 neurons
weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

# Biases for each of the 3 neurons
biases = [2.0, 3.0, 0.5]

# Manually calculating the output for each neuron
output = [
    # Neuron 1
    (inputs[0] * weights[0][0] +
     inputs[1] * weights[0][1] +
     inputs[2] * weights[0][2] +
     inputs[3] * weights[0][3] + biases[0]),

    # Neuron 2
    (inputs[0] * weights[1][0] +
     inputs[1] * weights[1][1] +
     inputs[2] * weights[1][2] +
     inputs[3] * weights[1][3] + biases[1]),

    # Neuron 3
    (inputs[0] * weights[2][0] +
     inputs[1] * weights[2][1] +
     inputs[2] * weights[2][2] +
     inputs[3] * weights[2][3] + biases[2])
]

print(output)
# Expected output: [4.8, 1.21, 2.385]

"""<h1><b>A layer of neuron using Loop</b></span></h1>

"""

# A layer of 3 neurons, each with 4 inputs (scalable version)
inputs = [1.0, 2.0, 3.0, 2.5]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2.0, 3.0, 0.5]

# Final list of outputs from the layer
layer_outputs = []

# Iterate through each neuron's weights and bias
for neuron_weights, neuron_bias in zip(weights, biases):
    # Start with the neuron's bias
    neuron_output = 0
    # Iterate through the inputs and the neuron's weights
    for n_input, weight in zip(inputs, neuron_weights):
        # Calculate the weighted sum of inputs
        neuron_output += n_input * weight
    # Add the bias to the weighted sum
    neuron_output += neuron_bias
    # Add the neuron's final output to the layer's output list
    layer_outputs.append(neuron_output)

print(layer_outputs)
# Expected output: [4.8, 1.21, 2.385]

"""<h1><b>A layer of neuron using NumPy</b></span></h1>

"""

import numpy as np

# A batch of 3 input samples
# inputs: batch size * number of input features
inputs = [[1.0, 2.0, 3.0, 2.5],
          [2.0, 5.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

# Weights for 3 neurons
# weights: number of neurons * number of input features
weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

# biases: 1 * bias for each neuron
biases = [2.0, 3.0, 0.5]

# Convert inputs and weights to NumPy arrays
inputs_array = np.array(inputs)
weights_array = np.array(weights)

# The inputs are multiplied by the TRANSPOSE of the weights
# Note: np.array() is used to enable the .T (transpose) method
layer_outputs = np.dot(inputs_array, weights_array.T) + biases
# when we add biases to np.dot(inputs, np.array(weights).T)
# broadcasting occurs and biases is added to each row

print(layer_outputs)
# Expected output:
# [[ 4.8    1.21   2.385]
#  [ 8.9   -1.81   0.285]
#  [ 1.485  2.065  1.425]]

"""<h1><b>Coding a two layer network</b></span></h1>

"""

import numpy as np

# A batch of 3 input samples (same as before)
inputs = np.array([[1.0, 2.0, 3.0, 2.5],
                   [2.0, 5.0, -1.0, 2.0],
                   [-1.5, 2.7, 3.3, -0.8]])

# --- Layer 1 ---
# 3 neurons, 4 weights each
weights1 = np.array([[0.2, 0.8, -0.5, 1.0],
                     [0.5, -0.91, 0.26, -0.5],
                     [-0.26, -0.27, 0.17, 0.87]])
biases1 = np.array([2.0, 3.0, 0.5])

# --- Layer 2 ---
# 3 neurons, but now with 3 weights each (because Layer 1 has 3 outputs)
weights2 = np.array([[0.1, -0.14, 0.5],
                     [-0.5, 0.12, -0.33],
                     [-0.44, 0.73, -0.13]])
biases2 = np.array([-1.0, 2.0, -0.5])

# Step 1: Calculate the output of Layer 1
layer1_outputs = np.dot(inputs, weights1.T) + biases1

# Step 2: Use Layer 1's output as the input for Layer 2
layer2_outputs = np.dot(layer1_outputs, weights2.T) + biases2

# Print the final output from the second layer
print(layer2_outputs)

"""<h1><b>Using a class for dense layer</b></span></h1>

"""

import numpy as np

# This is the blueprint for any dense layer we want to create.
class Layer_Dense:

    # The __init__ method is called automatically when we create a new layer.
    def __init__(self, n_inputs, n_neurons):
        # Create a weights matrix of shape (inputs x neurons) with small random values.
        # This shape avoids the need for transposing later.
        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)
        # Create a biases vector of zeros.
        self.biases = np.zeros((1, n_neurons))

    # The forward method performs the calculation for the layer.
    def forward(self, inputs):
        # Calculate the dot product and add biases.
        self.output = np.dot(inputs, self.weights) + self.biases

# Assume X is our spiral data with shape (100, 2)
# (100 samples, 2 features each)
!pip install nnfs
from nnfs.datasets import spiral_data
import numpy as np
import nnfs
nnfs.init()  # Sets NumPy seed and default dtype for reproducibility

import matplotlib.pyplot as plt

# Generate data
X, y = spiral_data(samples=100, classes=3)

# Plot
# plt.scatter(X[:, 0], X[:, 1])
# plt.show()

# 1. Create a layer instance from our class blueprint.
# This layer will take 2 inputs (from our data) and have 3 neurons.
dense1 = Layer_Dense(2, 3)

# 2. Perform the forward pass by calling the layer's forward method.
dense1.forward(X)

# The results are now stored inside the object itself.
print(dense1.output[:5]) # Print the output for the first 5 samples

"""<h1><b>Activation function RELU</b></span></h1>

"""

import numpy as np

class Activation_ReLU:
    # A forward pass takes the output from a dense layer
    def forward(self, inputs):
        # Apply the ReLU function: max(0, input)
        self.output = np.maximum(0, inputs)

"""<h1><b>Activation function Softmax</b></span></h1>

"""

import numpy as np

class Activation_Softmax:
    def forward(self, inputs):
        # Step 1: Subtract the max value from each row for numerical stability.
        # This prevents the exponentiated values from becoming infinitely large.
        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))

        # Step 2: Normalize the values to get probabilities.
        # Divide each value by the sum of its row.
        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)

        self.output = probabilities

"""<h1><b>First forward pass</b></span></h1>

"""

# Assume X is our spiral data

# Create the first dense layer (2 inputs, 3 neurons)
dense1 = Layer_Dense(2, 3)
# Create the ReLU activation for the first layer
activation1 = Activation_ReLU()

# Create the second dense layer (3 inputs from layer1, 3 neurons for 3 classes)
dense2 = Layer_Dense(3, 3)
# Create the Softmax activation for the final output
activation2 = Activation_Softmax()

# --- Perform the full forward pass ---

# Pass data through the first dense layer
dense1.forward(X)

# Pass the output of the dense layer through the ReLU activation
activation1.forward(dense1.output)

# Pass the output of the ReLU activation through the second dense layer
dense2.forward(activation1.output)

# Pass the output of the second dense layer through the Softmax activation
activation2.forward(dense2.output)

# The final probabilities are now in activation2.output
print(activation2.output[:5]) # Print probabilities for the first 5 samples

"""<h1><b>Loss function</b></span></h1>

"""

import numpy as np

# A parent class for any loss function
class Loss:
    def calculate(self, output, y):
        sample_losses = self.forward(output, y)
        data_loss = np.mean(sample_losses)
        return data_loss

# The specific class for Cross-Entropy Loss
class Loss_CategoricalCrossentropy(Loss):
    def forward(self, y_pred, y_true):
        # Clip data to prevent division by 0
        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)

        # Handle integer labels
        if len(y_true.shape) == 1:
            correct_confidences = y_pred_clipped[range(len(y_pred_clipped)), y_true]
        # Handle one-hot encoded labels
        elif len(y_true.shape) == 2:
            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)

        # Calculate losses for each sample
        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods

"""Gradient: The gradient of the loss function is a vector that points in the direction of the steepest ascent (where the loss increases the fastest). By moving in the opposite direction of the gradient, we can descend the loss landscape as efficiently as possible. This is the core idea behind gradient descent.

<h1><b>Chain rule decomposition</b></span></h1>
∂Loss / ∂w₀ = (∂Loss / ∂ReLU) * (∂ReLU / ∂Sum) * (∂Sum / ∂(x₀w₀)) * (∂(x₀w₀) / ∂w₀)

X (Inputs): (batch_size, num_input_features)

W (Weights): (num_input_features, num_neurons)

Z (Output): The result of the dot product is (batch_size, num_neurons).

Since the output Z has the shape (batch_size, num_neurons),
its gradient ∂L/∂Z must also have the shape (batch_size, num_neurons).

**Weight Gradients: ∂L/∂W = Xᵀ ⋅ (∂L/∂Z)**

**Bias Gradients: ∂L/∂b = sum(∂L/∂Z, axis=0)**

**Input Gradients (to pass backwards): ∂L/∂X = (∂L/∂Z) ⋅ Wᵀ**

<h1><b>Coding the backpropagation building blocks</b></span></h1>
"""

import numpy as np

class Layer_Dense:
    # The __init__ and forward methods are the same as before
    def __init__(self, n_inputs, n_neurons):
        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))

    def forward(self, inputs):
        # Store inputs for use in the backward pass
        self.inputs = inputs
        self.output = np.dot(inputs, self.weights) + self.biases

    # --- NEW BACKWARD METHOD ---
    def backward(self, dvalues):
        # dvalues is the upstream gradient, ∂L/∂Z

        # 1. Gradient on weights (∂L/∂W = Xᵀ ⋅ ∂L/∂Z)
        self.dweights = np.dot(self.inputs.T, dvalues)

        # 2. Gradient on biases (∂L/∂b = sum(∂L/∂Z))
        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)

        # 3. Gradient on inputs (∂L/∂X = (∂L/∂Z) ⋅ Wᵀ)
        self.dinputs = np.dot(dvalues, self.weights.T)

"""
<h1><b>Backpropagation through activation function</b></span></h1>"""

import numpy as np

class Activation_ReLU:
    def forward(self, inputs):
        # Store the input values for use in the backward pass
        self.inputs = inputs
        self.output = np.maximum(0, inputs)

    # --- NEW BACKWARD METHOD ---
    def backward(self, dvalues):
        # dvalues is the upstream gradient, ∂L/∂A

        # 1. Start by making a copy of the upstream gradient.
        self.dinputs = dvalues.copy()

        # 2. Zero out the gradient where the original inputs were negative.
        # This is the implementation of the chain rule: ∂L/∂A * (1 or 0).
        self.dinputs[self.inputs <= 0] = 0

"""
<h1><b>Categorical cross-entropy loss</b></span></h1>
Loss = -log(predicted_probability_of_the_correct_class)"""

# Cross-entropy loss
class Loss_CategoricalCrossentropy(Loss):
    # Forward pass
    def forward(self, y_pred, y_true):
        # Number of samples in a batch
        samples = len(y_pred)

        # Clip data to prevent division by 0
        # Clip both sides to not drag mean towards any value
        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)

        # Probabilities for target values –
        # only if categorical labels
        if len(y_true.shape) == 1:
            correct_confidences = y_pred_clipped[
                range(samples),
                y_true
            ]
        # Mask values – only for one-hot encoded labels
        elif len(y_true.shape) == 2:
            correct_confidences = np.sum(
                y_pred_clipped * y_true,
                axis=1
            )

        # Losses
        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods

        # --- NEW BACKWARD METHOD ---
    def backward(self, dvalues, y_true):
        # dvalues are the predicted probabilities from Softmax
        # y_true are the true labels

        # Number of samples and labels
        samples = len(dvalues)
        labels = len(dvalues[0])

        # If labels are sparse (integers), turn them into a one-hot vector
        if len(y_true.shape) == 1:
            y_true = np.eye(labels)[y_true]

        # --- Calculate the gradient using the formula ---
        # gradient = -y_true / y_pred
        self.dinputs = -y_true / dvalues

        # --- Normalize the gradient ---
        self.dinputs = self.dinputs / samples

"""The main purpose of normalizing the gradient by the number of samples is to make the learning rate independent of the batch size. This stabilizes the training process and makes hyperparameter tuning more consistent.

The Problem Without Normalization
During backpropagation, the total gradient for a batch is the sum of the individual gradients from each sample. This means that if you use a larger batch, the magnitude of the total gradient will naturally be larger.
"""

# How np.eye()[] converts sparse labels into one-hot vector
# np.eye(labels) creates an identity matrix of size [labels, labels]

# np.eye(3)
# array([[1., 0., 0.],
#        [0., 1., 0.],
#        [0., 0., 1.]])

# [] chooses a different row of the identity matrix, depending on the integer label.
#[[0. 0. 1.]   # from row 2
#[1. 0. 0.]   # from row 0
#[0. 1. 0.]]  # from row 1

"""Calculating the derivative for the Softmax activation function is mathematically complicated and involves Jacobian matrices. So we can't find the gradient.

Since the Softmax activation is always immediately followed by the Categorical Cross-Entropy loss function, we can mathematically combine them. The derivative of this combined Softmax and Loss function turns out to be an incredibly simple and elegant formula: Predicted Probabilities - True Labels
"""

# Softmax classifier – combined Softmax activation
# and cross-entropy loss for faster backward step
class Activation_Softmax_Loss_CategoricalCrossentropy:
    # Creates activation and loss function objects
    def __init__(self):
        self.activation = Activation_Softmax()
        self.loss = Loss_CategoricalCrossentropy()

    # Forward pass
    def forward(self, inputs, y_true):
        # Output layer's activation function
        self.activation.forward(inputs)
        # Set the output
        self.output = self.activation.output
        # Calculate and return loss value
        return self.loss.calculate(self.output, y_true)

    # Backward pass
    def backward(self, dvalues, y_true):
        # dvalues are the predicted probabilities from Softmax
        # Number of samples
        samples = len(dvalues)

        # If labels are one-hot encoded,
        # turn them into discrete values
        if len(y_true.shape) == 2:
            y_true = np.argmax(y_true, axis=1)

        # Copy so we can safely modify
        self.dinputs = dvalues.copy()

        # Calculate gradient
        self.dinputs[range(samples), y_true] -= 1

        # Normalize gradient
        self.dinputs = self.dinputs / samples

"""<h1><b>Assembling the whole Neural Network</b></span></h1>"""

# Assume X is the spiral data and y are the true labels

# --- Create the network components ---

# Layer 1: 2 input features, 3 neurons
dense1 = Layer_Dense(2, 3)
# ReLU activation for Layer 1
activation1 = Activation_ReLU()

# Layer 2: 3 input features (from layer 1), 3 output neurons (for 3 classes)
dense2 = Layer_Dense(3, 3)

# Combined Softmax and Cross-Entropy Loss for the output layer
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# --- Perform the full forward pass ---

# Layer 1
dense1.forward(X)
activation1.forward(dense1.output)

# Layer 2
dense2.forward(activation1.output)

# Calculate the loss from the output of the final dense layer
loss = loss_activation.forward(dense2.output, y)

# Print initial loss and accuracy
print(f'Loss: {loss}')
# ... (code to calculate and print accuracy)
# Calculate accuracy from output of activation2 and targets
# calculate values along first axis
predictions = np.argmax(loss_activation.output, axis=1)

if len(y.shape) == 2:
    y = np.argmax(y, axis=1)

accuracy = np.mean(predictions == y)

# Print accuracy
print('acc:', accuracy)

# --- Perform the full backward pass ---

# Start the backward pass from the combined loss and activation function
loss_activation.backward(loss_activation.output, y)

# Pass the gradient back through Dense Layer 2
dense2.backward(loss_activation.dinputs)

# Pass the gradient back through the ReLU Activation
activation1.backward(dense2.dinputs)

# Pass the gradient back through Dense Layer 1
dense1.backward(activation1.dinputs)

# --- Gradients are now ready ---
# The optimizer can now access these gradients to update the weights
print("Gradients for Layer 1 weights:\n", dense1.dweights)
print("Gradients for Layer 1 biases:\n", dense1.dbiases)
print("Gradients for Layer 2 weights:\n", dense2.dweights)
print("Gradients for Layer 2 biases:\n", dense2.dbiases)

"""<h1><b>Optimizer SGD</b></span></h1>"""

class Optimizer_SGD:
    # Initialize the optimizer with a learning rate
    def __init__(self, learning_rate=1.0):
        self.learning_rate = learning_rate

    # Update a layer's parameters
    def update_params(self, layer):
        # Update weights using the gradient descent rule
        layer.weights += -self.learning_rate * layer.dweights
        # Update biases using the gradient descent rule
        layer.biases += -self.learning_rate * layer.dbiases

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (output of previous layer)
# and 3 output values (for 3 classes)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_SGD(learning_rate=0.9) # Using a high learning rate, the loss won't decrease

# --- Training Loop ---
for epoch in range(30001):
    # 1. Forward Pass
    dense1.forward(X)
    activation1.forward(dense1.output)
    dense2.forward(activation1.output)
    loss = loss_activation.forward(dense2.output, y)

    # Predictions (class indices from Softmax outputs)
    predictions = np.argmax(loss_activation.output, axis=1)

    # If labels are one-hot encoded, convert them to integers
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)

    # Calculate accuracy
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)


    # 2. Backward Pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # --- 3. Update Parameters ---
    # Use the optimizer to update the weights and biases of each layer
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)

    # Print progress every 100 epochs
    if not epoch % 100:
        print(f'Epoch: {epoch}, Loss: {loss:.3f}, Accuracy: {accuracy:.3f}')

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (output of previous layer)
# and 3 output values (for 3 classes)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_SGD()

# Train in loop
for epoch in range(30001):
    # Forward pass through first dense layer
    dense1.forward(X)

    # Forward pass through ReLU activation
    activation1.forward(dense1.output)

    # Forward pass through second dense layer
    dense2.forward(activation1.output)

    # Forward pass through activation/loss function
    loss = loss_activation.forward(dense2.output, y)

    # Calculate accuracy
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy and loss every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f}')

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # Update weights and biases
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)

"""<h1><b>Adding decay to Optimizer SGD</b></span></h1>"""

class Optimizer_SGD:
    # The init method now accepts a decay rate
    def __init__(self, learning_rate=1.0, decay=0.0):
        self.initial_learning_rate = learning_rate
        self.current_learning_rate = learning_rate
        self.decay = decay
        self.iterations = 0

    # This method is called BEFORE updating the parameters
    def pre_update_params(self):
        # If decay is used, update the current learning rate
        if self.decay:
            self.current_learning_rate = self.initial_learning_rate * \
                (1. / (1. + self.decay * self.iterations))

    # The update method now uses the 'current_learning_rate'
    def update_params(self, layer):
        layer.weights += -self.current_learning_rate * layer.dweights
        layer.biases += -self.current_learning_rate * layer.dbiases

    # This method is called AFTER updating the parameters
    def post_update_params(self):
        # Increment the iteration counter for the next decay calculation
        self.iterations += 1

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (output of previous layer)
# and 3 output values (for 3 classes)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_SGD(decay=1e-3)

# Train in loop
for epoch in range(30001):
    # Forward pass through first dense layer
    dense1.forward(X)

    # Forward pass through ReLU activation
    activation1.forward(dense1.output)

    # Forward pass through second dense layer
    dense2.forward(activation1.output)

    # Forward pass through activation/loss function
    loss = loss_activation.forward(dense2.output, y)

    # Calculate accuracy
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy and loss every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f}, ' +
              f'lr: {optimizer.current_learning_rate}')

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # 3. Update Parameters
    optimizer.pre_update_params() # First, update the learning rate
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params() # Then, increment the iteration counter

"""<h1><b>Adding momentum to Optimizer SGD</b></span></h1>"""

class Optimizer_SGD:
    # Init now includes a momentum parameter
    def __init__(self, learning_rate=1.0, decay=1e-3, momentum=0.0):
        self.initial_learning_rate = learning_rate
        self.current_learning_rate = learning_rate
        self.decay = decay
        self.iterations = 0
        self.momentum = momentum

    # ... (pre_update_params method is the same) ...
    # This method is called BEFORE updating the parameters
    def pre_update_params(self):
        # If decay is used, update the current learning rate
        if self.decay:
            self.current_learning_rate = self.initial_learning_rate * \
                (1. / (1. + self.decay * self.iterations))

    def update_params(self, layer):
        # --- NEW MOMENTUM LOGIC ---
        # If momentum is used, the update is modified
        if self.momentum:
            # Create momentum arrays if they don't exist yet
            if not hasattr(layer, 'weight_momentums'):
                layer.weight_momentums = np.zeros_like(layer.weights)
                layer.bias_momentums = np.zeros_like(layer.biases)

            # Calculate the weight update with momentum
            weight_updates = (self.momentum * layer.weight_momentums) - \
                             (self.current_learning_rate * layer.dweights)
            layer.weight_momentums = weight_updates # Save for next iteration

            # Calculate the bias update with momentum
            bias_updates = (self.momentum * layer.bias_momentums) - \
                           (self.current_learning_rate * layer.dbiases)
            layer.bias_momentums = bias_updates # Save for next iteration

        # Standard SGD update (if no momentum)
        else:
            weight_updates = -self.current_learning_rate * layer.dweights
            bias_updates = -self.current_learning_rate * layer.dbiases

        # Apply the final update to the layer's parameters
        layer.weights += weight_updates
        layer.biases += bias_updates

    # ... (post_update_params method is the same) ...
    # This method is called AFTER updating the parameters
    def post_update_params(self):
        # Increment the iteration counter for the next decay calculation
        self.iterations += 1

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (output of previous layer)
# and 3 output values (for 3 classes)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)

# Train in loop
for epoch in range(30001):
    # Forward pass through first dense layer
    dense1.forward(X)

    # Forward pass through ReLU activation
    activation1.forward(dense1.output)

    # Forward pass through second dense layer
    dense2.forward(activation1.output)

    # Forward pass through activation/loss function
    loss = loss_activation.forward(dense2.output, y)

    # Calculate accuracy
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy and loss every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f}, ' +
              f'lr: {optimizer.current_learning_rate}')

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # 3. Update Parameters
    optimizer.pre_update_params() # First, update the learning rate
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params() # Then, increment the iteration counter

"""<h1><b>Optimizer AdaGrad</b></span></h1>"""

class Optimizer_AdaGrad:
    def __init__(self, learning_rate=1.0, decay=0., epsilon=1e-7):
        self.initial_learning_rate = learning_rate
        self.current_learning_rate = learning_rate
        self.decay = decay
        self.iterations = 0
        self.epsilon = epsilon

    # ... (pre_update_params method is the same) ...
    # This method is called BEFORE updating the parameters
    def pre_update_params(self):
        # If decay is used, update the current learning rate
        if self.decay:
            self.current_learning_rate = self.initial_learning_rate * \
                (1. / (1. + self.decay * self.iterations))

    def update_params(self, layer):
        # Create cache arrays if they don't exist yet
        if not hasattr(layer, 'weight_cache'):
            layer.weight_cache = np.zeros_like(layer.weights)
            layer.bias_cache = np.zeros_like(layer.biases)

        # --- Update the cache with the square of the current gradients ---
        layer.weight_cache += layer.dweights**2
        layer.bias_cache += layer.dbiases**2

        # --- Perform the AdaGrad update ---
        # Update weights
        layer.weights += -self.current_learning_rate * layer.dweights / \
                         (np.sqrt(layer.weight_cache) + self.epsilon)
        # Update biases
        layer.biases += -self.current_learning_rate * layer.dbiases / \
                        (np.sqrt(layer.bias_cache) + self.epsilon)

    # ... (post_update_params method is the same) ...
    # This method is called AFTER updating the parameters
    def post_update_params(self):
        # Increment the iteration counter for the next decay calculation
        self.iterations += 1

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (output of previous layer)
# and 3 output values (for 3 classes)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_AdaGrad(learning_rate=0.1, decay=1e-3)

# Train in loop
for epoch in range(30001):
    # Forward pass through first dense layer
    dense1.forward(X)

    # Forward pass through ReLU activation
    activation1.forward(dense1.output)

    # Forward pass through second dense layer
    dense2.forward(activation1.output)

    # Forward pass through activation/loss function
    loss = loss_activation.forward(dense2.output, y)

    # Calculate accuracy
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy and loss every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f}, ' +
              f'lr: {optimizer.current_learning_rate}')

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # 3. Update Parameters
    optimizer.pre_update_params() # First, update the learning rate
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params() # Then, increment the iteration counter

"""<h1><b>Optimizer RMSProp</b></span></h1>"""

class Optimizer_RMSProp:
    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):
        self.initial_learning_rate = learning_rate
        self.current_learning_rate = learning_rate
        self.decay = decay
        self.iterations = 0
        self.epsilon = epsilon
        self.rho = rho  # decay rate for cache averaging

    def pre_update_params(self):
        if self.decay:
            self.current_learning_rate = self.initial_learning_rate * \
                (1. / (1. + self.decay * self.iterations))

    def update_params(self, layer):
        # Create cache arrays if they don’t exist
        if not hasattr(layer, 'weight_cache'):
            layer.weight_cache = np.zeros_like(layer.weights)
            layer.bias_cache = np.zeros_like(layer.biases)

        # Update cache with moving average of squared gradients
        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2
        layer.bias_cache   = self.rho * layer.bias_cache  + (1 - self.rho) * layer.dbiases**2

        # Parameter update
        layer.weights += -self.current_learning_rate * layer.dweights / \
                         (np.sqrt(layer.weight_cache) + self.epsilon)
        layer.biases  += -self.current_learning_rate * layer.dbiases / \
                         (np.sqrt(layer.bias_cache) + self.epsilon)

    def post_update_params(self):
        self.iterations += 1

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (output of previous layer)
# and 3 output values (for 3 classes)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_RMSProp(learning_rate=0.1, decay=1e-3, rho=0.99)

# Train in loop
for epoch in range(30001):
    # Forward pass through first dense layer
    dense1.forward(X)

    # Forward pass through ReLU activation
    activation1.forward(dense1.output)

    # Forward pass through second dense layer
    dense2.forward(activation1.output)

    # Forward pass through activation/loss function
    loss = loss_activation.forward(dense2.output, y)

    # Calculate accuracy
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy and loss every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f}, ' +
              f'lr: {optimizer.current_learning_rate}')

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # 3. Update Parameters
    optimizer.pre_update_params() # First, update the learning rate
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params() # Then, increment the iteration counter

"""<h1><b>Optimizer Adam</b></span></h1>"""

class Optimizer_Adam:
    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,
                 beta_1=0.9, beta_2=0.999):
        self.initial_learning_rate = learning_rate
        self.current_learning_rate = learning_rate
        self.decay = decay
        self.iterations = 0
        self.epsilon = epsilon
        self.beta_1 = beta_1
        self.beta_2 = beta_2

    def pre_update_params(self):
        if self.decay:
            self.current_learning_rate = self.initial_learning_rate * \
                (1. / (1. + self.decay * self.iterations))

    def update_params(self, layer):
        # Initialize moment and cache arrays if not already
        if not hasattr(layer, 'weight_momentums'):
            layer.weight_momentums = np.zeros_like(layer.weights)
            layer.bias_momentums   = np.zeros_like(layer.biases)
            layer.weight_cache     = np.zeros_like(layer.weights)
            layer.bias_cache       = np.zeros_like(layer.biases)

        # --- Update momentums (m) ---
        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \
                                 (1 - self.beta_1) * layer.dweights
        layer.bias_momentums   = self.beta_1 * layer.bias_momentums + \
                                 (1 - self.beta_1) * layer.dbiases

        # --- Update cache (v) ---
        layer.weight_cache = self.beta_2 * layer.weight_cache + \
                             (1 - self.beta_2) * layer.dweights**2
        layer.bias_cache   = self.beta_2 * layer.bias_cache + \
                             (1 - self.beta_2) * layer.dbiases**2

        # --- Bias correction ---
        weight_momentums_corrected = layer.weight_momentums / \
            (1 - self.beta_1 ** (self.iterations + 1))
        bias_momentums_corrected = layer.bias_momentums / \
            (1 - self.beta_1 ** (self.iterations + 1))

        weight_cache_corrected = layer.weight_cache / \
            (1 - self.beta_2 ** (self.iterations + 1))
        bias_cache_corrected = layer.bias_cache / \
            (1 - self.beta_2 ** (self.iterations + 1))

        # --- Parameter update ---
        layer.weights += -self.current_learning_rate * \
            weight_momentums_corrected / \
            (np.sqrt(weight_cache_corrected) + self.epsilon)

        layer.biases += -self.current_learning_rate * \
            bias_momentums_corrected / \
            (np.sqrt(bias_cache_corrected) + self.epsilon)

    def post_update_params(self):
        self.iterations += 1

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create Dense layer with 2 input features and 64 output values
dense1 = Layer_Dense(2, 64)

# Create ReLU activation (to be used with Dense layer)
activation1 = Activation_ReLU()

# Create second Dense layer with 64 input features (output of previous layer)
# and 3 output values (for 3 classes)
dense2 = Layer_Dense(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_Adam(learning_rate=0.1, decay=1e-3, beta_1=0.9, beta_2=0.999)

# Train in loop
for epoch in range(10001):
    # Forward pass through first dense layer
    dense1.forward(X)

    # Forward pass through ReLU activation
    activation1.forward(dense1.output)

    # Forward pass through second dense layer
    dense2.forward(activation1.output)

    # Forward pass through activation/loss function
    loss = loss_activation.forward(dense2.output, y)

    # Calculate accuracy
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy and loss every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f}, ' +
              f'lr: {optimizer.current_learning_rate}')

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # 3. Update Parameters
    optimizer.pre_update_params() # First, update the learning rate
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params() # Then, increment the iteration counter

"""<h1><b>Evaluating model's performance on new data</b></span></h1>"""

# --- Create a new test dataset ---
X_test, y_test = spiral_data(samples=100, classes=3)

# --- Perform a forward pass on the test data ---
# We use the existing, trained layers (dense1, activation1, etc.)
dense1.forward(X_test)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
loss = loss_activation.forward(dense2.output, y_test)

# --- Calculate accuracy on the test data ---
predictions = np.argmax(loss_activation.output, axis=1)
if len(y_test.shape) == 2:
    y_test = np.argmax(y_test, axis=1)
accuracy = np.mean(predictions == y_test)

print(f'Validation Accuracy: {accuracy:.3f}, Loss: {loss:.3f}')

"""<h1><b>Hyperparameter tuning (Learning rate)</b></span></h1>"""

import numpy as np
from sklearn.model_selection import train_test_split

# Assume 'spiral_data' is the function from your course that creates the dataset
# Let's create a large dataset for this example
X_full, y_full = spiral_data(samples=1000, classes=3)

# --- Step 1: Split the full dataset to create a Test Set (20%) ---
# The test set is now locked away and will not be touched until the very end.
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_full, y_full, test_size=0.2, random_state=42
)

# --- Step 2: Split the remaining data to create a Validation Set (25% of the 80%) ---
# This creates a 60% train, 20% validation, 20% test split of the original data.
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42
)


print(f"Full dataset shape: {X_full.shape}")
print("---")
print(f"Training set shape:   {X_train.shape}")
print(f"Validation set shape: {X_val.shape}")
print(f"Test set shape:       {X_test.shape}")

# Now you would:
# 1. Train multiple models with different hyperparameters using (X_train, y_train).
# 2. Evaluate each one on (X_val, y_val) to pick the best model.
# 3. Train your single best model on the combined (X_train_val, y_train_val).
# 4. Perform a final, one-time evaluation on (X_test, y_test).

# Example: Try different learning rates
learning_rates = [0.01, 0.05, 0.1]
results = []

for lr in learning_rates:
    # Reinitialize model
    dense1 = Layer_Dense(2, 64)
    activation1 = Activation_ReLU()
    dense2 = Layer_Dense(64, 3)
    loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

    optimizer = Optimizer_Adam(learning_rate=lr)

    # Training loop (shorter for validation runs)
    for epoch in range(1001):
        # Forward pass
        dense1.forward(X_train)
        activation1.forward(dense1.output)
        dense2.forward(activation1.output)
        loss = loss_activation.forward(dense2.output, y_train)

        # Accuracy
        predictions = np.argmax(loss_activation.output, axis=1)
        if len(y_train.shape) == 2:
            y_train_eval = np.argmax(y_train, axis=1)
        else:
            y_train_eval = y_train
        acc = np.mean(predictions == y_train_eval)

        # Backward pass
        loss_activation.backward(loss_activation.output, y_train)
        dense2.backward(loss_activation.dinputs)
        activation1.backward(dense2.dinputs)
        dense1.backward(activation1.dinputs)

        # Update params
        optimizer.pre_update_params()
        optimizer.update_params(dense1)
        optimizer.update_params(dense2)
        optimizer.post_update_params()

    # After training, evaluate on validation set
    dense1.forward(X_val)
    activation1.forward(dense1.output)
    dense2.forward(activation1.output)
    val_loss = loss_activation.forward(dense2.output, y_val)

    val_predictions = np.argmax(loss_activation.output, axis=1)
    if len(y_val.shape) == 2:
        y_val_eval = np.argmax(y_val, axis=1)
    else:
        y_val_eval = y_val
    val_acc = np.mean(val_predictions == y_val_eval)

    results.append((lr, val_loss, val_acc))
    print(f"LR={lr} → Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}")


# --- Step 2: Pick the best model (based on validation accuracy) ---
best_lr, best_val_loss, best_val_acc = max(results, key=lambda x: x[2])
print("\nBest hyperparameter: LR =", best_lr)


# --- Step 3: Retrain the best model on TRAIN+VAL (X_train_val, y_train_val) ---
dense1 = Layer_Dense(2, 64)
activation1 = Activation_ReLU()
dense2 = Layer_Dense(64, 3)
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()
optimizer = Optimizer_Adam(learning_rate=best_lr)

for epoch in range(3001):  # longer training since this is the final model
    dense1.forward(X_train_val)
    activation1.forward(dense1.output)
    dense2.forward(activation1.output)
    loss = loss_activation.forward(dense2.output, y_train_val)

    # Backward pass
    loss_activation.backward(loss_activation.output, y_train_val)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    optimizer.pre_update_params()
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params()

    if not epoch % 500:
        preds = np.argmax(loss_activation.output, axis=1)
        y_eval = np.argmax(y_train_val, axis=1) if len(y_train_val.shape) == 2 else y_train_val
        acc = np.mean(preds == y_eval)
        print(f"Epoch {epoch} → Loss: {loss:.3f}, Acc: {acc:.3f}")


# --- Step 4: Final one-time evaluation on TEST set ---
dense1.forward(X_test)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
test_loss = loss_activation.forward(dense2.output, y_test)

test_preds = np.argmax(loss_activation.output, axis=1)
y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test
test_acc = np.mean(test_preds == y_test_eval)

print("\n--- Final Test Evaluation ---")
print(f"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}")

"""<h1><b>L1/L2 Regularization</b></span></h1>"""

import numpy as np

class Layer_Dense:
    """
    A dense (fully-connected) layer with L1 and L2 regularization.
    """

    # Layer initialization
    def __init__(self, n_inputs, n_neurons,
                 weight_regularizer_l1=0, weight_regularizer_l2=0,
                 bias_regularizer_l1=0, bias_regularizer_l2=0):

        # Initialize weights and biases
        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))

        # Set regularization strength for L1 and L2 penalties
        self.weight_regularizer_l1 = weight_regularizer_l1
        self.weight_regularizer_l2 = weight_regularizer_l2
        self.bias_regularizer_l1 = bias_regularizer_l1
        self.bias_regularizer_l2 = bias_regularizer_l2

    # Forward pass
    def forward(self, inputs):
        # Store inputs for use in the backward pass
        self.inputs = inputs
        # Calculate the output of the layer
        self.output = np.dot(inputs, self.weights) + self.biases

    # Backward pass
    def backward(self, dvalues):
        # Gradients on parameters
        self.dweights = np.dot(self.inputs.T, dvalues)
        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)

        # --- Add regularization gradients ---

        # L1 regularization on weights
        if self.weight_regularizer_l1 > 0:
            # Create a matrix of 1s with the same shape as weights
            dL1 = np.ones_like(self.weights)
            # Set -1 for weights that are negative
            dL1[self.weights < 0] = -1
            # Add the L1 gradient to the original weight gradients
            self.dweights += self.weight_regularizer_l1 * dL1

        # L2 regularization on weights
        if self.weight_regularizer_l2 > 0:
            # Add the L2 gradient (2 * lambda * w) to the original weight gradients
            self.dweights += 2 * self.weight_regularizer_l2 * self.weights

        # L1 regularization on biases
        if self.bias_regularizer_l1 > 0:
            dL1 = np.ones_like(self.biases)
            dL1[self.biases < 0] = -1
            self.dbiases += self.bias_regularizer_l1 * dL1

        # L2 regularization on biases
        if self.bias_regularizer_l2 > 0:
            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases

        # Gradient on inputs (to be passed to the previous layer)
        self.dinputs = np.dot(dvalues, self.weights.T)

import numpy as np

# Base Loss class
class Loss:
    # Calculates the regularization loss from all trainable layers
    def regularization_loss(self, layer):
        regularization_loss = 0
        # L1 regularization - weights
        if layer.weight_regularizer_l1 > 0:
            regularization_loss += layer.weight_regularizer_l1 * \
                                   np.sum(np.abs(layer.weights))
        # L2 regularization - weights
        if layer.weight_regularizer_l2 > 0:
            regularization_loss += layer.weight_regularizer_l2 * \
                                   np.sum(layer.weights * layer.weights)
        # L1 regularization - biases
        if layer.bias_regularizer_l1 > 0:
            regularization_loss += layer.bias_regularizer_l1 * \
                                   np.sum(np.abs(layer.biases))
        # L2 regularization - biases
        if layer.bias_regularizer_l2 > 0:
            regularization_loss += layer.bias_regularizer_l2 * \
                                   np.sum(layer.biases * layer.biases)
        return regularization_loss

    # Calculates the data and regularization losses
    # given model output and ground truth values
    def calculate(self, output, y):
        # Calculate sample losses
        sample_losses = self.forward(output, y)
        # Calculate mean loss
        data_loss = np.mean(sample_losses)
        # Return loss
        return data_loss

# Cross-entropy loss
class Loss_CategoricalCrossentropy(Loss):
    # Forward pass
    def forward(self, y_pred, y_true):
        # Number of samples in a batch
        samples = len(y_pred)

        # Clip data to prevent division by 0
        # Clip both sides to not drag mean towards any value
        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)

        # Probabilities for target values –
        # only if categorical labels
        if len(y_true.shape) == 1:
            correct_confidences = y_pred_clipped[
                range(samples),
                y_true
            ]
        # Mask values – only for one-hot encoded labels
        elif len(y_true.shape) == 2:
            correct_confidences = np.sum(
                y_pred_clipped * y_true,
                axis=1
            )

        # Losses
        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods

        # --- NEW BACKWARD METHOD ---
    def backward(self, dvalues, y_true):
        # dvalues are the predicted probabilities from Softmax
        # Number of samples
        samples = len(dvalues)
        labels = len(dvalues[0])

        # If labels are sparse (integers), turn them into a one-hot vector
        if len(y_true.shape) == 1:
            y_true = np.eye(labels)[y_true]

        # Copy so we can safely modify
        self.dinputs = dvalues.copy()

        # Calculate gradient
        self.dinputs[range(samples), y_true] -= 1

        # Normalize gradient
        self.dinputs = self.dinputs / samples

# --- Main Training Script ---

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# --- Create Model ---
# NEW: Add L2 regularization strength to the dense layers
dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)
activation1 = Activation_ReLU()
dense2 = Layer_Dense(64, 3, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7, beta_1=0.9, beta_2=0.999)

# Train in loop
for epoch in range(10001):
    # --- Forward Pass ---
    dense1.forward(X)
    activation1.forward(dense1.output)
    dense2.forward(activation1.output)

    # NEW: Calculate the data loss
    data_loss = loss_activation.forward(dense2.output, y)

    # NEW: Calculate the regularization loss
    reg_loss = loss_activation.loss.regularization_loss(dense1) + \
               loss_activation.loss.regularization_loss(dense2)

    # NEW: Combine data and regularization loss
    loss = data_loss + reg_loss

    # --- Calculate Accuracy ---
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy, loss, and learning rate every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f} (' + # NEW: Print total loss
              f'data_loss: {data_loss:.3f}, ' + # NEW: Print data loss
              f'reg_loss: {reg_loss:.3f}), ' + # NEW: Print regularization loss
              f'lr: {optimizer.current_learning_rate}')

    # --- Backward Pass ---
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    # --- Update Parameters ---
    optimizer.pre_update_params()
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params()

"""<h1><b>Hyperparameter tuning</b></span></h1>"""

import numpy as np
from sklearn.model_selection import train_test_split

# --- Dataset ---
X_full, y_full = spiral_data(samples=1000, classes=3)

# --- Step 1: Split into Train / Val / Test ---
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_full, y_full, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42
)  # 60% / 20% / 20%

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

# --- Helper: training function ---
def train_model(X, y, X_val, y_val,
                learning_rate=0.01,
                l2_strength=0.0,
                hidden_neurons=64,
                epochs=1000):

    # Build model
    dense1 = Layer_Dense(2, hidden_neurons,
                         weight_regularizer_l2=l2_strength,
                         bias_regularizer_l2=l2_strength)
    activation1 = Activation_ReLU()
    dense2 = Layer_Dense(hidden_neurons, 3,
                         weight_regularizer_l2=l2_strength,
                         bias_regularizer_l2=l2_strength)
    loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()
    optimizer = Optimizer_Adam(learning_rate=learning_rate)

    # Training loop
    for epoch in range(epochs):
        # Forward pass
        dense1.forward(X)
        activation1.forward(dense1.output)
        dense2.forward(activation1.output)
        loss = loss_activation.forward(dense2.output, y)

        # Backward pass
        loss_activation.backward(loss_activation.output, y)
        dense2.backward(loss_activation.dinputs)
        activation1.backward(dense2.dinputs)
        dense1.backward(activation1.dinputs)

        # Update
        optimizer.pre_update_params()
        optimizer.update_params(dense1)
        optimizer.update_params(dense2)
        optimizer.post_update_params()

    # Validation
    dense1.forward(X_val)
    activation1.forward(dense1.output)
    dense2.forward(activation1.output)
    val_loss = loss_activation.forward(dense2.output, y_val)

    val_preds = np.argmax(loss_activation.output, axis=1)
    y_val_eval = np.argmax(y_val, axis=1) if len(y_val.shape) == 2 else y_val
    val_acc = np.mean(val_preds == y_val_eval)

    return val_loss, val_acc


# --- Step 2: Try multiple hyperparameters ---
param_grid = [
    {"learning_rate": 0.01, "l2_strength": 1e-4, "hidden_neurons": 32},
    {"learning_rate": 0.05, "l2_strength": 5e-4, "hidden_neurons": 64},
    {"learning_rate": 0.1, "l2_strength": 1e-3, "hidden_neurons": 128},
]

results = []
for params in param_grid:
    print(f"Testing {params}")
    val_loss, val_acc = train_model(X_train, y_train, X_val, y_val,
                                    learning_rate=params["learning_rate"],
                                    l2_strength=params["l2_strength"],
                                    hidden_neurons=params["hidden_neurons"],
                                    epochs=1000)
    results.append((params, val_loss, val_acc))
    print(f"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\n")


# --- Step 3: Pick best hyperparameters ---
best_params, best_val_loss, best_val_acc = max(results, key=lambda x: x[2])
print("Best Hyperparameters:", best_params)

# --- Step 4: Retrain best model on TRAIN+VAL ---
print("\nRetraining best model on TRAIN+VAL...")
train_model(X_train_val, y_train_val, X_val, y_val,
            learning_rate=best_params["learning_rate"],
            l2_strength=best_params["l2_strength"],
            hidden_neurons=best_params["hidden_neurons"],
            epochs=3000)

# Final evaluation on Test
dense1 = Layer_Dense(2, best_params["hidden_neurons"],
                     weight_regularizer_l2=best_params["l2_strength"],
                     bias_regularizer_l2=best_params["l2_strength"])
activation1 = Activation_ReLU()
dense2 = Layer_Dense(best_params["hidden_neurons"], 3,
                     weight_regularizer_l2=best_params["l2_strength"],
                     bias_regularizer_l2=best_params["l2_strength"])
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()
optimizer = Optimizer_Adam(learning_rate=best_params["learning_rate"])

# Train final model on train+val
for epoch in range(3000):
    dense1.forward(X_train_val)
    activation1.forward(dense1.output)
    dense2.forward(activation1.output)
    loss = loss_activation.forward(dense2.output, y_train_val)

    loss_activation.backward(loss_activation.output, y_train_val)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    optimizer.pre_update_params()
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params()

# Test evaluation
dense1.forward(X_test)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
test_loss = loss_activation.forward(dense2.output, y_test)

test_preds = np.argmax(loss_activation.output, axis=1)
y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test
test_acc = np.mean(test_preds == y_test_eval)

print("\n--- Final Test Evaluation ---")
print(f"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}")

import numpy as np
from sklearn.model_selection import train_test_split

# --- Dataset ---
X_full, y_full = spiral_data(samples=1000, classes=3)

# --- Train/Val/Test Split ---
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_full, y_full, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42
)

print(f"Full dataset shape: {X_full.shape}")
print("---")
print(f"Training set shape:   {X_train.shape}")
print(f"Validation set shape: {X_val.shape}")
print(f"Test set shape:       {X_test.shape}")

# --- Hyperparameters to tune ---
learning_rates = [0.01, 0.05, 0.1]
l2_strengths = [0.0, 1e-4, 1e-3, 1e-2]

results = []

# --- Hyperparameter Search ---
for lr in learning_rates:
    for l2 in l2_strengths:
        # Model
        dense1 = Layer_Dense(2, 64, weight_regularizer_l2=l2, bias_regularizer_l2=l2)
        activation1 = Activation_ReLU()
        dense2 = Layer_Dense(64, 3, weight_regularizer_l2=l2, bias_regularizer_l2=l2)
        loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

        optimizer = Optimizer_Adam(learning_rate=lr)

        # Training loop (shorter for validation runs)
        for epoch in range(1001):
            # Forward
            dense1.forward(X_train)
            activation1.forward(dense1.output)
            dense2.forward(activation1.output)
            data_loss = loss_activation.forward(dense2.output, y_train)
            reg_loss = loss_activation.loss.regularization_loss(dense1) + \
                       loss_activation.loss.regularization_loss(dense2)
            loss = data_loss + reg_loss

            # Backward
            loss_activation.backward(loss_activation.output, y_train)
            dense2.backward(loss_activation.dinputs)
            activation1.backward(dense2.dinputs)
            dense1.backward(activation1.dinputs)

            optimizer.pre_update_params()
            optimizer.update_params(dense1)
            optimizer.update_params(dense2)
            optimizer.post_update_params()

        # Evaluate on validation set
        dense1.forward(X_val)
        activation1.forward(dense1.output)
        dense2.forward(activation1.output)
        val_data_loss = loss_activation.forward(dense2.output, y_val)
        val_reg_loss = loss_activation.loss.regularization_loss(dense1) + \
                       loss_activation.loss.regularization_loss(dense2)
        val_loss = val_data_loss + val_reg_loss

        val_predictions = np.argmax(loss_activation.output, axis=1)
        y_val_eval = np.argmax(y_val, axis=1) if len(y_val.shape) == 2 else y_val
        val_acc = np.mean(val_predictions == y_val_eval)

        results.append((lr, l2, val_loss, val_acc))
        print(f"LR={lr}, L2={l2} → Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}")


# --- Step 2: Pick best (by validation acc) ---
best_lr, best_l2, best_val_loss, best_val_acc = max(results, key=lambda x: x[3])
print("\nBest hyperparameters → LR =", best_lr, "L2 =", best_l2)

# --- Step 3: Retrain best model on TRAIN+VAL ---
dense1 = Layer_Dense(2, 64, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)
activation1 = Activation_ReLU()
dense2 = Layer_Dense(64, 3, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()
optimizer = Optimizer_Adam(learning_rate=best_lr)

for epoch in range(3001):
    dense1.forward(X_train_val)
    activation1.forward(dense1.output)
    dense2.forward(activation1.output)
    data_loss = loss_activation.forward(dense2.output, y_train_val)
    reg_loss = loss_activation.loss.regularization_loss(dense1) + \
               loss_activation.loss.regularization_loss(dense2)
    loss = data_loss + reg_loss

    loss_activation.backward(loss_activation.output, y_train_val)
    dense2.backward(loss_activation.dinputs)
    activation1.backward(dense2.dinputs)
    dense1.backward(activation1.dinputs)

    optimizer.pre_update_params()
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params()

    if not epoch % 500:
        preds = np.argmax(loss_activation.output, axis=1)
        y_eval = np.argmax(y_train_val, axis=1) if len(y_train_val.shape) == 2 else y_train_val
        acc = np.mean(preds == y_eval)
        print(f"Epoch {epoch} → Loss: {loss:.3f}, Acc: {acc:.3f}")


# --- Step 4: Final Test Evaluation ---
dense1.forward(X_test)
activation1.forward(dense1.output)
dense2.forward(activation1.output)
test_data_loss = loss_activation.forward(dense2.output, y_test)
test_reg_loss = loss_activation.loss.regularization_loss(dense1) + \
                loss_activation.loss.regularization_loss(dense2)
test_loss = test_data_loss + test_reg_loss

test_preds = np.argmax(loss_activation.output, axis=1)
y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test
test_acc = np.mean(test_preds == y_test_eval)

print("\n--- Final Test Evaluation ---")
print(f"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}")

import numpy as np

class Layer_Dropout:
    # Initialize with the dropout rate
    def __init__(self, rate):
        # 'rate' is the dropout rate. We store the 'success rate'.
        self.rate = 1 - rate

    # Forward pass
    def forward(self, inputs):
        self.inputs = inputs

        # --- The Core Dropout Logic ---
        # 1. Create a binary mask using a binomial distribution.
        #    This creates an array of 0s and 1s with the same shape as the inputs.
        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape)

        # 2. Apply the mask to the inputs (zeroing out some neurons)
        self.output = inputs * self.binary_mask

        # 3. Scale the output (Inverted Dropout)
        self.output /= self.rate

    # Backward pass
    def backward(self, dvalues):
        # The gradient is also masked.
        # Only the neurons that were active during the forward pass
        # contribute to the gradient passed backward.
        self.dinputs = dvalues * self.binary_mask

        # The gradient is also scaled by the same factor.
        self.dinputs /= self.rate

# --- Main Training Script ---

# Create dataset
X, y = spiral_data(samples=100, classes=3)

# --- Create Model ---
# NEW: Add L2 regularization strength to the dense layers
dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)
dropout1 = Layer_Dropout(0.1)  # NEW: Create a Dropout layer with a 10% dropout rate
activation1 = Activation_ReLU()

dense2 = Layer_Dense(64, 3, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)
dropout2 = Layer_Dropout(0.1)  # NEW: Create a Dropout layer with a 10% dropout rate
loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()

# Create optimizer
optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7, beta_1=0.9, beta_2=0.999)

# Train in loop
for epoch in range(10001):
    # --- Forward Pass ---
    dense1.forward(X)
    activation1.forward(dense1.output)
    dropout1.forward(activation1.output)
    dense2.forward(dropout1.output)
    dropout2.forward(dense2.output)

    # NEW: Calculate the data loss
    loss_activation.forward(dropout2.output, y)

    # NEW: Calculate the regularization loss
    reg_loss = loss_activation.loss.regularization_loss(dense1) + \
               loss_activation.loss.regularization_loss(dense2)

    # NEW: Combine data and regularization loss
    loss = data_loss + reg_loss

    # --- Calculate Accuracy ---
    predictions = np.argmax(loss_activation.output, axis=1)
    if len(y.shape) == 2:
        y = np.argmax(y, axis=1)
    accuracy = np.mean(predictions == y)

    # Print accuracy, loss, and learning rate every 100 epochs
    if not epoch % 100:
        print(f'epoch: {epoch}, ' +
              f'acc: {accuracy:.3f}, ' +
              f'loss: {loss:.3f} (' + # NEW: Print total loss
              f'data_loss: {data_loss:.3f}, ' + # NEW: Print data loss
              f'reg_loss: {reg_loss:.3f}), ' + # NEW: Print regularization loss
              f'lr: {optimizer.current_learning_rate}')

    # --- Backward Pass ---
    loss_activation.backward(loss_activation.output, y)
    dropout2.backward(loss_activation.dinputs)
    dense2.backward(dropout2.dinputs)
    # The gradient from dense2 goes to dropout1 (which came before it)
    dropout1.backward(dense2.dinputs)
    # The gradient from dropout1 goes to activation1
    activation1.backward(dropout1.dinputs)
    # The gradient from activation1 goes to dense1
    dense1.backward(activation1.dinputs)

    # --- Update Parameters ---
    optimizer.pre_update_params()
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params()

import numpy as np
from sklearn.model_selection import train_test_split

# --- Dataset ---
X_full, y_full = spiral_data(samples=1000, classes=3)

# --- Train/Val/Test Split (60/20/20) ---
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_full, y_full, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42
)

print(f"Full: {X_full.shape} | Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}")

# --- Hyperparameters to tune ---
learning_rates = [0.01, 0.05, 0.1]
l2_strengths   = [0.0, 1e-4, 1e-3, 1e-2]
dropout_rate   = 0.1  # fixed as in your model
search_epochs  = 1000
final_epochs   = 3000

results = []

# --- Hyperparameter Search ---
for lr in learning_rates:
    for l2 in l2_strengths:
        # Build model
        dense1 = Layer_Dense(2, 64, weight_regularizer_l2=l2, bias_regularizer_l2=l2)
        activation1 = Activation_ReLU()
        dropout1 = Layer_Dropout(dropout_rate)

        dense2 = Layer_Dense(64, 3, weight_regularizer_l2=l2, bias_regularizer_l2=l2)
        dropout2 = Layer_Dropout(dropout_rate)

        loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()
        optimizer = Optimizer_Adam(learning_rate=lr)

        # --- Train (with dropout) ---
        for epoch in range(search_epochs):
            dense1.forward(X_train)
            activation1.forward(dense1.output)
            dropout1.forward(activation1.output)         # dropout ON (training)
            dense2.forward(dropout1.output)
            dropout2.forward(dense2.output)              # dropout ON (training)

            data_loss = loss_activation.forward(dropout2.output, y_train)
            reg_loss  = loss_activation.loss.regularization_loss(dense1) + \
                        loss_activation.loss.regularization_loss(dense2)
            loss = data_loss + reg_loss

            # Backward
            loss_activation.backward(loss_activation.output, y_train)
            dropout2.backward(loss_activation.dinputs)
            dense2.backward(dropout2.dinputs)
            dropout1.backward(dense2.dinputs)
            activation1.backward(dropout1.dinputs)
            dense1.backward(activation1.dinputs)

            optimizer.pre_update_params()
            optimizer.update_params(dense1)
            optimizer.update_params(dense2)
            optimizer.post_update_params()

        # --- Validate (NO dropout) ---
        dense1.forward(X_val)
        activation1.forward(dense1.output)
        # skip dropout1
        dense2.forward(activation1.output)
        # skip dropout2

        val_data_loss = loss_activation.forward(dense2.output, y_val)
        val_reg_loss  = loss_activation.loss.regularization_loss(dense1) + \
                        loss_activation.loss.regularization_loss(dense2)
        val_loss = val_data_loss + val_reg_loss

        val_preds   = np.argmax(loss_activation.output, axis=1)
        y_val_eval  = np.argmax(y_val, axis=1) if len(y_val.shape) == 2 else y_val
        val_acc     = np.mean(val_preds == y_val_eval)

        results.append((lr, l2, val_loss, val_acc))
        print(f"LR={lr:>.3g}, L2={l2:>.1e} → Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}")

# --- Select best by validation accuracy ---
best_lr, best_l2, best_val_loss, best_val_acc = max(results, key=lambda x: x[3])
print(f"\nBest hyperparams → LR={best_lr}, L2={best_l2} | Val Acc={best_val_acc:.3f}")

# --- Retrain best model on TRAIN+VAL (with dropout during training) ---
dense1 = Layer_Dense(2, 64, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)
activation1 = Activation_ReLU()
dropout1 = Layer_Dropout(dropout_rate)

dense2 = Layer_Dense(64, 3, weight_regularizer_l2=best_l2, bias_regularizer_l2=best_l2)
dropout2 = Layer_Dropout(dropout_rate)

loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()
optimizer = Optimizer_Adam(learning_rate=best_lr)

for epoch in range(final_epochs + 1):
    # Train (dropout ON)
    dense1.forward(X_train_val)
    activation1.forward(dense1.output)
    dropout1.forward(activation1.output)
    dense2.forward(dropout1.output)
    dropout2.forward(dense2.output)

    data_loss = loss_activation.forward(dropout2.output, y_train_val)
    reg_loss  = loss_activation.loss.regularization_loss(dense1) + \
                loss_activation.loss.regularization_loss(dense2)
    loss = data_loss + reg_loss

    loss_activation.backward(loss_activation.output, y_train_val)
    dropout2.backward(loss_activation.dinputs)
    dense2.backward(dropout2.dinputs)
    dropout1.backward(dense2.dinputs)
    activation1.backward(dropout1.dinputs)
    dense1.backward(activation1.dinputs)

    optimizer.pre_update_params()
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)
    optimizer.post_update_params()

    if epoch % 500 == 0:
        preds = np.argmax(loss_activation.output, axis=1)
        y_eval = np.argmax(y_train_val, axis=1) if len(y_train_val.shape) == 2 else y_train_val
        acc = np.mean(preds == y_eval)
        print(f"Final-train Epoch {epoch} → Loss: {loss:.3f}, Acc: {acc:.3f}")

# --- Final Test (NO dropout) ---
dense1.forward(X_test)
activation1.forward(dense1.output)
# skip dropout1
dense2.forward(activation1.output)
# skip dropout2

test_data_loss = loss_activation.forward(dense2.output, y_test)
test_reg_loss  = loss_activation.loss.regularization_loss(dense1) + \
                 loss_activation.loss.regularization_loss(dense2)
test_loss = test_data_loss + test_reg_loss

test_preds  = np.argmax(loss_activation.output, axis=1)
y_test_eval = np.argmax(y_test, axis=1) if len(y_test.shape) == 2 else y_test
test_acc    = np.mean(test_preds == y_test_eval)

print("\n--- Final Test Evaluation ---")
print(f"Test Loss: {test_loss:.3f}, Test Accuracy: {test_acc:.3f}")